[["index.html", "Statistics for Soil Survey - Part 1 Pre-course Assignment 0.1 Create Workspace 0.2 Configure RStudio 0.3 Essentials 0.4 Personalization 0.5 Install Required Packages 0.6 Dealing With Errors 0.7 Packages not on CRAN 0.8 Install NASIS 0.9 Connect Local NASIS 0.10 Proof 0.11 Additional Soil Data and R References", " Statistics for Soil Survey - Part 1 Soil Survey Staff 2025-01-27 Pre-course Assignment 0.1 Create Workspace Make a local folder C:\\workspace2 to use as a working directory for this course. Use all lower case letters please. 0.2 Configure RStudio Open RStudio, and edit the “Global Options” (Main menu: Tools → Global Options). 0.3 Essentials These options are important for pleasant, reproducible and efficient use of the RStudio environment: Change the default working directory to C:\\workspace2 (R General Tab) Uncheck “Restore .Rdata into workspace at startup” (R General Tab) VERY IMPORTANT Figure 1: Example of RStudio General settings. RStudio detects the available R installations on your computer. Individual versions are certified for the Software Center as they become available, and sometimes there is a more recent version available for download. It is worth taking the time before installing packages to get the latest version of R available to you. This is to minimize compatibility issues which arise over time. 0.4 Personalization Figure 2: Example of RStudio Code/Editing settings. Optional: Check “Soft-wrap R source files” (Code/Editing Tab) Optional: Show help tooltips, control auto-completion and diagnostics (Code/Completion and Diagnostics Tabs) Optional: Update code font size, colors and theme (Appearance) Optional: Use RStudio Projects (top-right corner) to manage working directories 0.5 Install Required Packages Packages can be installed by name from the Comprehensive R Archive Network (CRAN) using the base R function install.packages(). There are a lot of packages out there–many more than you will download here, and many of which are useful for Soil Survey work. The first time you install packages, R may ask you if you want to create a local package library. You need to do this because we cannot write to system folders as non-administrator users on CCE machines. The default location for R package library on Windows is: C:\\Users\\&lt;User.Name&gt;\\AppData\\Local\\R\\win-library\\&lt;X.X&gt; where &lt;User.Name&gt; is the current Windows user name and &lt;X.X&gt; is the version of R packages are being installed for. If you have an existing R package library (for same minor version of R), you can copy that library into the AppData\\Local\\R folder as needed. For example, to download and install the remotes package from CRAN: install.packages(&quot;remotes&quot;) To install the R packages used in this class copy all of the code from the box below and paste into the R console window. Paste after the command prompt (&gt;) and press enter. Downloading and configuring the packages will take a while if you are installing or upgrading all of the packages in the list below. On particularly slow network connections, i.e. over VPN or USDA network in general, it may be necessary to increase the “timeout” option to ensure the downloads have sufficient time to complete. # increase default timeout from 1 minute to 5 minutes (for current session only) options(timeout = 300) ## character vector of package names packages &lt;- c( # soil &quot;aqp&quot;, &quot;soilDB&quot;, &quot;sharpshootR&quot;, &quot;soiltexture&quot;, # gis &quot;raster&quot;, &quot;sp&quot;, &quot;sf&quot;, &quot;terra&quot;, &quot;gdalUtilities&quot;, &quot;rgrass&quot;, &quot;RSAGA&quot;, &quot;exactextractr&quot;, &quot;fasterize&quot;, # data management &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;devtools&quot;, &quot;roxygen2&quot;, &quot;Hmisc&quot;, &quot;circular&quot;, &quot;DT&quot;, &quot;remotes&quot;, &quot;DescTools&quot;, # databases &quot;DBI&quot;, &quot;odbc&quot;, &quot;RSQLite&quot;, # graphics &quot;ggplot2&quot;, &quot;latticeExtra&quot;, &quot;maps&quot;, &quot;spData&quot;, &quot;tmap&quot;, &quot;kableExtra&quot;, &quot;corrplot&quot;, &quot;farver&quot;, &quot;mapview&quot;, &quot;ggmap&quot;, &quot;plotrix&quot;, &quot;rpart.plot&quot;, &quot;visreg&quot;, &quot;diagram&quot;, &quot;GGally&quot;, &quot;wesanderson&quot;, &quot;viridisLite&quot;, &quot;prettymapr&quot;, # modeling &quot;car&quot;, &quot;rms&quot;, &quot;randomForest&quot;, &quot;ranger&quot;, &quot;party&quot;, &quot;caret&quot;, &quot;vegan&quot;, &quot;ape&quot;, &quot;shape&quot;, &quot;modEvA&quot;, &quot;gower&quot;, &quot;MBESS&quot;, &quot;yardstick&quot;, # sampling &quot;clhs&quot;, &quot;spcosa&quot;, &quot;sgsR&quot; ) # ipkCRAN: a helper fuction for installing required packages from CRAN source(&quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/ipkCRAN.R&quot;) ## - p: vector of package names ## - up: logical - upgrade installed packages? Default: TRUE ## up = TRUE to download all packages ## up = FALSE to download only packages you don&#39;t already have installed ipkCRAN(p = packages, up = TRUE) The ipkCRAN function will let you know if any of the above packages fail to install. Whenever you run some code always check the console output for warnings and errors before continuing. It may be easiest to send commands individually to learn about and inspect their output, rather than running the entire file and wondering where an error occurred. 0.6 Dealing With Errors If a lot of output is produced by a command you should scroll up and sift through it as best you can. Copy and paste parts of the error message to use in internet searches, and try to find cases where folks have encountered problems. 0.6.1 No output is produced after pasting into console If you do not have a new command prompt (&gt;) and a blinking cursor on the left hand side of your console, but instead see a + after you run a command, R may think you are still in the middle of submitting input to the “read-eval-print-loop” (REPL). If this is not expected you are possibly missing closing quotes, braces, brackets or parentheses. R needs to know you were done with your expression, so you may need to supply some input to get the command to be complete. Pasting code line-by-line is useful but prone to input errors with multiline expressions. Alternately, you can run commands or an entire file using the GUI or keyboard shortcuts such as Ctrl+Enter. You have a chance to try this in the example at the end. 0.6.2 ‘SOMEPACKAGE’ is not available (for R version X.Y.Z) This means either: A package named ‘SOMEPACKAGE’ exists but it is not available for your version of R CRAN does not have a package with that name You can try again, but first check for spelling and case-sensitivity. When in doubt search the package name on Google or CRAN to make sure you have it right. Note that not all R packages are available on CRAN: there are many other ways that you can deliver packages (including GitHub described below). 0.7 Packages not on CRAN Some R packages rely on compiled code. Windows users are limited to installing “binary” versions of such packages from CRAN unless they have “Rtools” installed. The Rtools software is available from the Software Center, and is specific to the version of R you have. One way to get the latest binary builds of R packages that use compiled code is by using https://r-universe.dev. This website provides custom repositories that can be used in addition to the defaults in install.packages() For example, you can install raster-related “rspatial” packages from r-universe.dev. This may not be “required” but it is good to know how to specify an alternate package repository source using the repos= argument. Check with your mentor to see if there are known issues with current CRAN packages. install.packages(c(&#39;terra&#39;, &#39;raster&#39;), repos=&#39;https://rspatial.r-universe.dev&#39;) To install the latest version of packages from the Algorithms for Quantitative Pedology (AQP) suite off GitHub we use the remotes package. The AQP packages are updated much more frequently on GitHub than they are on CRAN. Generally, the CRAN versions (installed above) are the “stable” releases whereas the GitHub repositories have new features and bug fixes. remotes::install_github(&quot;ncss-tech/aqp&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/soilDB&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/sharpshootR&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/soilReports&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) 0.8 Install NASIS If you do not have NASIS installed you will not be able to complete the next steps. For USDA staff NASIS can be installed from the Software Center. Search \"NASIS\" and you should find two applications. First you must install “Microsoft SQL Server Express for the NASIS Client”, then restart your computer, and then install the “NASIS Client”. If you do not have access to the Software Center, or cannot find NASIS, it can be installed by OCIO staff by submitting an IT support ticket for “Install / Uninstall Software”. If you are not a USDA employee you can find additional information on NASIS installation here: https://new.cloudvault.usda.gov/index.php/s/xFTJabHiT45WDom More information on installing and initializing your NASIS client for the first time can be found in the training materials for NASIS 1010 (Basic NASIS) course: https://ncss-tech.github.io/nasis_training/content/pre-course.html 0.9 Connect Local NASIS Establish an ODBC connection to NASIS by following the directions at the following hyperlink (ODBC Connection to NASIS). Once you’ve successfully established a ODBC connection, prove it by loading your NASIS selected set with the site and pedon tables for any pedons from your local area. You only need a few pedons at a minimum for this demo – too many (say, &gt;20) will make the example profile plot cluttered. Paste the below code at the command prompt (&gt;) and press enter, as you did above. Or create a new R script (Main menu: File → New File → R Script) and paste code into the “Source” pane (script editor window). Then, click the Run button in the top-right corner of the Script Editor or use Ctrl+Enter to run code at the cursor location / any selected code. This will execute the code in the Console. Submit the resulting plot to your mentor (from “Plot” pane (bottom-right): Export → Save as PDF…) # load packages into the current session library(aqp) # provides &quot;SoilProfileCollection&quot; object &amp; more library(soilDB) # provides database access methods # get pedons from NASIS selected set test &lt;- fetchNASIS(from = &#39;pedons&#39;) # inspect the result str(test, max.level = 2) # make a profile plot # set margins smaller than default par(mar=c(1,1,1,1)) # make profile plot of selected set, with userpedonid as label plot(test, label=&#39;upedonid&#39;) 0.10 Proof Follow the one line example below, copy the output, and submit the results to your mentor. This will help us to verify that all of the required packages have been installed. # dump list of packages that are loaded into the current session sessionInfo() 0.11 Additional Soil Data and R References 0.11.1 Soil Data Videos NCSS Partners Seminar: Soils Data 101 Paul Finnell’s NASIS webinar Stats for Soil Survey Webinar Soil Data Aggregation using R Webinar 0.11.2 R Books and Manuals R-Intro R for Data Science Geocomputation with R Spatial Data Science with R and “terra” Geographic Data Science with R book Impatient R The R Inferno AQP Website and Tutorials "],["intro.html", "Chapter 1 Introduction to R 1.1 Outline 1.2 Course Overview 1.3 What is R? 1.4 RStudio: An Integrated Development Environment (IDE) for R 1.5 R basics 1.6 Managing Packages 1.7 Getting Help 1.8 Documenting your work 1.9 Organizing your work 1.10 Saving your work 1.11 Exercise 1: R packages and Functions 1.12 Loading Data 1.13 Data manipulation 1.14 Exercise 2: Data Manipulation 1.15 Review 1.16 Additional Reading (Introduction)", " Chapter 1 Introduction to R 1.1 Outline Course Overview Review Course Objectives Why is this training needed? Why is course organized this way? What is R? Why should I use R? What can R do? How do I get started? RStudio interface What are packages? How to navigate the Help tab How to save files Manipulating data Loading &amp; viewing data Filtering, transforming, merging, aggregating and reshaping data Exporting data 1.2 Course Overview 1.2.1 Course Objectives Develop solutions to investigate soil survey correlation problems and update activities. Evaluate investigations for interpretive results and determine how to proceed. Summarize data for population in NASIS. Analyze spatial data to investigate soil-landscape relationships Help to pursue the question “why” 1.2.2 Why is this training needed? Long standing goal of the Soil Science Division to have a course in statistics (Mausbach 2003) Opportunities to learn these techniques are limited, especially at the undergraduate level (Hennemann and Rossiter 2004) Consistent methodology (data analysis, data population, sampling design, etc.) There is continually a greater need to use these techniques: Mapping of lands at high production rates ((MacMillan, Moon, and Coupé 2007); (Kempen et al. 2012); (Brevik et al. 2016)) Ecological Sites (Maynard et al. 2019) Soil survey refinement (disaggregation) ((Chaney et al. 2016);(Ramcharan et al. 2018)) 1.2.3 Why is course organized this way? The web content is a long-term investment and serves as a reference Our best judgment for assembling into 24 hours what could be 6 University level courses Mixture of slides and script-enabled web pages is “new” for NRCS Feel free to provide feedback for improving the class for future offerings. 1.3 What is R? R is a free, open-source software and programming language developed in 1995 at the University of Auckland as an environment for statistical computing and graphics (Ihaka and Gentleman 1996). Since then R has become one of the dominant software environments for data analysis and is used by a variety of scientific disiplines, including soil science, ecology, and geoinformatics (Envirometrics CRAN Task View; Spatial CRAN Task View). R is particularly popular for its graphical capabilities, but it is also prized for it’s GIS capabilities which make it relatively easy to generate raster-based models. More recently, R has also gained several packages designed specifically for analyzing soil data. A software environment: statistics graphics programming calculator GIS A language to explore, summarize, and model data functions = verbs objects = nouns 1.3.1 Why Should I Learn R? While the vast majority of people use Microsoft Excel for data analysis, R offers numerous advantages, such as: Cost. R is free! (“Free as in free speech, not free beer.”) Reproducible Research (self-documenting, repeatable) repeatable: code + output in a single document (‘I want the right answer, not a quick answer’ - Paul Finnell) easier the next time (humorous example) numerous Excel horror stories of scientific studies gone wrong exist (TED Talk) scalable: applicable to small or large problems R in a Community Numerous Discipline Specific R Groups Numerous Local R User Groups (including R-Ladies Groups) Stack Overflow Learning Resources (quantity and quality) R books (Free Online) R Books “If we don’t accept these challenges, others who are less qualified will; and soil scientists will be displaced by apathy.” (Arnold and Wilding 1991) While some people find the use of a command line environment daunting, it is becoming a necessary skill for scientists as the volume and variety of data has grown. Thus scripting or programming has become a third language for many scientists, in addition to their native language and discipline specific terminology. Other popular programming languages include: SQL (i.e. NASIS), Python (i.e. ArcGIS), and JavaScript. ODBC and GDAL link R to nearly all possible formats/interfaces 1.3.2 What can R do? 1.3.3 Packages Base R (functionality is extended through packages) basic summaries of quantitative or qualitative data data exploration via graphics GIS data processing and analysis Soil Science R Packages aqp - visualization, aggregation, classification soilDB - access to commonly used soil databases soilReports - handful of report templates soiltexture - textural triangles Ecology R packages vegan - ordination, diversity analysis, etc. dismo - species distribution modeling 1.3.3.1 Soil Science Applications 1.3.3.1.1 Create Maps 1.3.3.1.2 Draw Soil Profiles 1.3.3.1.3 Draw Depth Plots 1.3.3.1.4 Estimate the Range in Characteristics (RIC) variable genhz pct10 median pct90 clay A 13 16 22 clay BAt 16 19 25 clay Bt1 18 24 32 clay Bt2 22 30 44 clay Cr 15 15 15 phfield A 6 6 7 phfield BAt 5 6 6 phfield Bt1 5 6 7 1.4 RStudio: An Integrated Development Environment (IDE) for R RStudio is an integrated development environment (IDE) that allows you to interact with R more readily. RStudio is similar to the standard RGui, but is considerably more user friendly. It has more drop-down menus, windows with multiple tabs, and many customization options. The first time you open RStudio, you will see three windows. A forth window is hidden by default, but can be opened by clicking the File drop-down menu, then New File, and then R Script. Detailed information on using RStudio can be found at at RStudio’s Website. RStudio Windows / Tabs Location Description Console Window lower-left location were commands are entered and the output is printed Source Tabs upper-left built-in text editor Environment Tab upper-right interactive list of loaded R objects History Tab upper-right list of key strokes entered into the Console Files Tab lower-right file explorer to navigate C drive folders Plots Tab lower-right output location for plots Packages Tab lower-right list of installed packages Help Tab lower-right output location for help commands and help search window Viewer Tab lower-right advanced tab for local web content 1.5 R basics R is command-line driven. It requires you to type or copy-and-paste commands after a command prompt (&gt;) that appears when you open R. This is called the “Read-Eval-Print-Loop” or REPL. After typing a command in the R console and pressing Enter on your keyboard, the command will run. If your command is not complete, R issues a continuation prompt (signified by a plus sign: +). R is case sensitive. Make sure your spelling and capitalization are correct. Commands in R are also called functions. The basic format of a function in R is: object &lt;- function.name(argument_1 = data, argument_2 = TRUE). The up arrow (^) on your keyboard can be used to bring up previous commands that you’ve typed in the R console. Comments in R code need to start with the # symbol (a.k.a. hash-tag, comment, pound, or number symbol). R ignores the remainder of the script line following #. # Math 1 + 1 10 * 10 log10(100) # combine values c(1, 2, 3) # Create sequence of values 1:10 # Implicit looping 1:10 * 5 1:10 * 1:10 # Assignment and data types ## numeric clay &lt;- c(10, 12, 15, 26, 30) ## character subgroup &lt;- c(&quot;typic haplocryepts&quot;,&quot;andic haplocryepts&quot;,&quot;typic dystrocryepts&quot;) ## logical andic &lt;- c(FALSE, TRUE ,FALSE) # Print print(clay) subgroup 1.6 Managing Packages Packages are collections of additional functions that can be loaded on demand. They commonly include example data that can be used to demonstrate those functions. Although R comes with many common statistical functions and models, most of our work requires additional packages. 1.6.1 Installing Packages To use a package, you must first install it and then load it. These steps can be done at the command line or using the Packages Tab. Examples of both approaches are provided below. R packages only need to be installed once (until R is upgraded or re-installed). Every time you start a new R session, however, you need to load every package that you intend to use in that session. Within the Packages tab you will see a list of all the packages currently installed on your computer, and 2 buttons labeled either “Install” or “Update”. To install a new package simply select the Install button. You can enter install one or more than one packages at a time by simply separating them with a comma. To find out what packages are installed on your computer, use the following commands: library() # or installed.packages() One useful package for soil scientists is the soiltexture package. It allows you to plot soil textural triangles. The following command shows how to install this package if you do not currently have it downloaded: # CRAN (static version) install.packages(c(&quot;soiltexture&quot;)) # GitHub (development version) remotes::install_github(&quot;julienmoeys/soiltexture/pkg/soiltexture&quot;, dependencies = FALSE, upgrade = FALSE, build = FALSE) 1.6.2 Loading Packages Once a package is installed, it must be loaded into the R session to be used. This can be done by using library(). The package name does not need to be quoted. library(soilDB) You can also load packages using the Packages Tab, by checking the box next to the package name. For example, documentation for the soilDB package is available from the help() function. help(package = &quot;soilDB&quot;) 1.7 Getting Help R has extensive documentation, numerous mailing lists, and countless books (many of which are free and listed at end of each chapter for this course). To learn more about the function you are using and the options and arguments available, learn to help yourself by taking advantage of some of the following help functions in RStudio: Use the Help tab in the lower-right Window to search commands (such as hist) or topics (such as histogram). Type help(read.csv) or ?read.csv in the Console window to bring up a help page. Results will appear in the Help tab in the lower right-hand window. Certain functions may require quotations, such as help(\"+\"). # Help file for a function help(read.csv) # or ?read.csv # Help files for a package help(package = &quot;soiltexture&quot;) 1.8 Documenting your work RStudio’s Source Tabs serve as a built-in text editor. Prior to executing R functions at the Console, commands are typically written down (or scripted). Scripting is essentially showing your work. The sequence of functions necessary to complete a task are scripted in order to document or automate a task. While scripting may seems cumbersome at first, it ultimately saves time in the long run, particularly for repetitive tasks (humorous YouTube Video on Scripting). Benefits include: allows others to reproduce your work, which is the foundation of science serves as instruction/reminder on how to perform a task allows rapid iteration, which saves time and allows the evaluation of incremental changes reduces the chance of human error 1.8.1 Basic Tips for Scripting To write a script, simply open a new R script file by clicking File&gt;New File&gt;R Script. Within the text editor type out a sequence of functions. Place each function (e.g. read.csv()) on a separate line. If a function has a long list of arguments, place each argument on a separate line. A command can be excuted from the text editor by placing the cursor on a line and typing Crtl + Enter, or by clicking the Run button. An entire R script file can be excuted by clicking the Source button. 1.8.2 Comments It is a good idea to include comments in your code, so that in the future both yourself and others can understand what you were doing. Each line with a comment starts with #. In RStudio, you can use # comments to create an “outline” for your source documents. Multiple # signs increase the depth of the hierarchy. Ending a comment line with four hyphens (----) indicates that text should be included in the outline. The source file outline using comments in regular .R source files is analogous to the Markdown syntax used in R Markdown and Quarto for headers. For example, the following code block creates two outline sections, each with a nested subsection. To show the outline view, click the “outline” button in the top-right hand corner of the source window. Paste it in a fresh R document to try it out. # one ---- print(&quot;Section 1&quot;) ## one two ---- # this is an ordinary comment (does not show in outline) print(&quot;Subsection 1.2&quot;) # two ---- print(&quot;Section 2&quot;) ## two one ---- print(&quot;Subsection 2.1) 1.9 Organizing your work When you first begin a project you should create a new folder and place within it all the data and code associated with the project. This simplifies the process of accessing your files from R. Using a project folder is also a good habit because it makes it easier to pickup where you left off and find your data if you need to come back to it later. Within R, your project folder is also known as your working directory. This directory will be the default location your plots and other R output are saved. You want to have inputs for your code in the working directory so that you can refer to them using relative file paths. Relative file paths make it easier if you move the folder containing your script(s) around. Or, if you share it with someone else, they will have little issue getting your code to work on their own file system. 1.9.1 Setting the Working Directory Before you begin working in R, you should set your working directory to your project folder; for example, setwd(\"C:\\\\workspace2\\\\projectx...\"). You can use RStudio to manage your projects and folders. NOTE: Beware when specifying any file paths that R uses forward slashes / instead of back slashes \\. Back slashes are reserved for use as an escape character, so you must use two of them to get one in result character string. To change the working directory in RStudio, select main menu Session &gt;&gt; Set Working Directory &gt;&gt; …. Or, from the “Files” tab click More &gt;&gt; Set As Working Directory to use the current location of the “Files” tab as your working directory. Setting the working directory can also be done via the Console with the setwd() command: setwd(&quot;C:/workspace2&quot;) To check the file path of the current working directory (which should now be \"C:\\\\workspace2\"), type: getwd() 1.9.2 RStudio Projects (.Rproj files) You can also manage your working directory using RStudio Projects. An RStudio Project file (.Rproj) is analogous to, for example, a .mxd file for ArcMap. It contains information about the specific settings you may have set for a “project”. You open or create projects using the drop down menu in the top right-hand corner of the RStudio window (shown below) RStudio Project Menu Here is what a typical Project drop-down menu looks like: RStudio Project Menu (expanded) You can create new projects from existing or new directories with “New Project…”. When you click “Open Project…”, your working directory is automatically set to the .Rproj file’s location – this is extremely handy Any projects you have created/used recently will show up in the “Project List” 1.10 Saving your work In R, you can save several types of files to keep track of the work you do. The file types include: workspace, script, history, and graphics. It is important to save often because R, like any other software, may crash periodically. Such problems are especially likely when working with large files. You can save your workspace in R via the command line or the File menu. 1.10.0.1 R script (.R) An R script is simply a text file of R commands that you’ve typed. You may want to save your scripts (whether they were written in R Editor or another program such as Notepad) so that you can reference them in the future, edit them as needed, and keep track of what you’ve done. To save R scripts in RStudio, simply click the save button from your R script tab. Save scripts with the .R extension. R assumes that script files are saved with only that extension. If you are using another text editor, you won’t need to worry about saving your scripts in R. You can open text files in the RStudio text editor, but beware copying and pasting from Word files as discussed below. To open an R script, click the file icon. 1.10.0.2 Microsoft Word Files Using Microsoft Word to write or save R scripts is generally a bad idea. Certain keyboard characters, such as quotations ““, are not stored the same in Word (e.g. they are”left” and “right” handed). The difference is hard to distinguish, but will not run in R. Also, pasting your R code or output into Word documents manually is not reproducible, so while it may work in a pinch, it ultimately costs you time. You can use the word_document Rmarkdown template to automatically “Knit” .docx files from R code using a template, which is very handy for quickly getting a nice looking document! 1.10.0.3 R Markdown (.Rmd) R Markdown (.Rmd) documents contain information for the reproducible combination of narrative text and code to produce elegantly formatted output. You can use multiple languages in .Rmd documents including R, Python, and SQL. You can easily “knit” visually appealing and high-quality documents into rich HTML, PDF or Word documents from the RStudio interface. The Statistics for Soil Survey webpage is made in bookdown, which is a variant of rmarkdown used for book templates with multiple chapters. You can make blogs and websites for your R packages with blogdown and pkgdown. These are all tools based off of the powerful “pandoc” engine and the tools in the R Markdown ecosystem. 1.10.0.4 R history (.Rhistory) An R history file is a copy of all your key strokes. You can think of it as brute force way of saving your work. It can be useful if you didn’t document all your steps in an R script file. Like an R file, an Rhistory file is simply a text file that lists all of the commands that you’ve executed. It does not keep a record of the results. To load or save your R history from the History Tab click the Open File or Save button. If you load an Rhistory file, your previous commands will again become available with the up-arrow and down-arrow keys. You can also use the command line to load or save your history. savehistory(file = &quot;sand.Rhistory&quot;) loadhistory(file = &quot;sand.Rhistory&quot;) history(max.show=Inf) #displays all previous commands 1.10.0.5 R Graphics Graphic outputs can be saved in various formats. Format Function pdf pdf(“graphic.pdf”) window metafile win.metafile(“graphic.wmf”) png png(“graph.png”) jpeg jpeg(“graph.jpg”) bmp bmp(“graph.bmp”) postscript postscript(“graph.ps”) To save a graphic: (1) Click the Plots Tab window, (2) click the Export button, (3) Choose your desired format, (3) Modify the export settings as you desire, and (4) click Save. The R command for saving a graphic is: png(file = &quot;npk_yield.png&quot;) plot(npk$yield) dev.off() The first line of this command creates a blank file named \"npk_yield.png\". The second line plots the data object that you want to create a graphic of (here it is conveniently the same name as the PNG file we are creating). The third line closes the graphics device. 1.11 Exercise 1: R packages and Functions Using the examples discussed thus far as a guide, demonstrate your mastery of the material by performing the following tasks. Create a new R script file with all commands you use. Demonstrate usage of 3 R functions and comment (#) your code with answers to the questions. Install the FedData R package from CRAN and GitHub. Load the FedData R package and read the help file for the get_ssurgo() function within the FedData package. What is the 1st input/argument? Save your R script and forward to your mentor. 1.12 Loading Data R can load a variety of data formats, however tabular data is by far the most common, and what we will spend of the majority of our time working with. Typically tabular data is stored in spreadsheets (e.g. .txt, .csv, .xlsx), databases (e.g. NASIS), or webpages (.html). Within R tabular data is stored as a data.frame. 1.12.0.1 Text files Text files are a preferable format for storing and transferring small datasets. One basic command for importing text files into R is read.csv(). The command is followed by the file name or URL and then some optional instructions for how to read the file. These files can either be imported into R by clicking the Import Dataset &gt;&gt; From Text buttons from the Environment tab, or by typing the following command into the R console: # from working directory sand &lt;- read.csv(&quot;C:/workspace2/sand_example.csv&quot;) # from URL sand &lt;- read.csv(&quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/sand_example.csv&quot;) 1.12.0.2 Excel files R can import Excel files, but generally speaking it is a bad idea to use Excel. Excel has a dangerous default which automatically converts data with common notations to their standard format without warning or notice. For example, the character “11-JUN” entered into a cell automatically becomes the date 6/11/2021, even though the data is still displayed as 11-JUN. The only way to avoid this default behavior is to manually import your data into Excel via the Data Tab&gt;Get External Data Ribbon, and manually set the data type of all your columns to text. Failure to do so has resulted in numerous retracted research articles (Washington Post Article). Warnings aside, Excel files are a very common and are a format most people are familiar with. Therefore we will illustrate how to bring them into R. Download the sand Excel dataset from GitHub at https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/Pre-course/R_sand/sand_example.xlsx Excel datasets can either be imported into R by clicking the Import Dataset &gt;&gt; From Excel buttons from the Environment tab, or by typing the following command into the R console: library(readxl) sand_example &lt;- read_excel(&quot;sand_example.xlsx&quot;) 1.12.0.3 NASIS Web Reports NASIS provides many reports that can be read into R for analysis. The soilDB R package provides a series of functions to read data from NASIS either using a local database connection or via HTML web reports. Similar functions also exist for accessing tabular data from Soil Data Access. More details on soilDB will be provided in the next chapter, but now we’ll illustrate how to access some example datasets for manipulating tabular data. library(soilDB) # get projects prj &lt;- get_project_from_NASISWebReport(mlrassoarea = &quot;NE-IND&quot;, fiscalyear = 2020) l# get legends leg &lt;- get_legend_from_NASISWebReport(mlraoffice = &quot;Indi%&quot;, areasymbol = &quot;%&quot;) # get map units mu &lt;- get_mapunit_from_NASISWebReport(areasymbol = c(&quot;IN001&quot;, &quot;IN11%&quot;)) Since these functions in soilDB query web reports, it is not necessary to be a NASIS user or set up your local database and selected set. We can simply pass parameters to the web report URL and get tabular results back. 1.13 Data manipulation Before we can do any analysis our data usually needs to be manipulated one way or another. Estimates vary, but an analyst typically spend 80% of their time manipulating data and only 20% actually analyzing or modeling. Tasks generally involve filtering, transforming, merging, aggregating, and reshaping data. R has many functions and packages for manipulating data frames, but within the past several years a family of packages, known as the tidyverse, have been developed to simplify interacting with data frames (or tibbles). Within the tidyverse the most commonly used packages are dplyr and tidyr. Many of the tidyverse function names are patterned after SQL functions. These packages and are built around the “tidy data” philosophy. This simply means that rows are observations and each column is a variable. We will review the most common functions you need to know in order to accomplish the majority of data manipulation tasks. 1.13.1 Viewing and Removing Data Once a file is imported, it is imperative that you check that R correctly imported your data. Make sure numeric data are correctly imported as numeric, that your column headings are preserved, etc. To view the data you can click on the mu dataset listed in the RStudio Environment tab. This will open up a separate window that displays a spreadsheet like view. This is equivalent to the function View(mu). Additionally you can use the following functions to view your data in R. Function Description print() prints the entire object (avoid with large tables) head() prints the first 6 lines of your data str() shows the data structure of an R object names() lists the column names (i.e., headers) of your data ls() lists all the R objects in your workspace directory dput() print code to re-create an R object View() show a spreadsheet-style data viewer for an R object Try entering the following commands to view the mu dataset in R: str(mu) names(mu) head(mu) ls() dput(mu[1, ]) View(mu) A data object is anything you’ve created or imported and assigned a name to in R. The Environment tab allows you to see what data objects are in your R session and expand their structure. If you wanted to delete all data objects from your R session, you could click the broom icon from the Environments tab. Otherwise you could type: # Remove all R objects rm(list = ls(all = TRUE)) # Remove individual objects by name rm(mu, leg, sand) 1.13.2 Filtering or Subsetting Data When analyzing data in NASIS, filtering is typically accomplished by loading your selected set with only the records you’re interested in. However, it is often useful or necessary to subset your data after it’s loaded. This can allow you to isolate interesting records within large datasets. For these reasons R has numerous options/functions for filtering data. Data frames can be filtered by both columns and rows, using either names, position (e.g. column 1, row 5), or logical indices (e.g. TRUE/FALSE). Another particularly useful feature is the use of pattern matching which uses regular expressions to select data, which is similar to the LIKE statement from SQL. **Filtering with names and numerical indices # Filtering with names (character values) mu$areasymbol # select column names using $ mu[, c(&quot;areasymbol&quot;, &quot;musym&quot;)] # select column names using [] mu[c(&quot;1&quot;, &quot;2&quot;), ] # select row names using [] mu[c(&quot;1&quot;, &quot;2&quot;), c(&quot;areasymbol&quot;, &quot;musym&quot;)] # select column and row names using [] # Filtering by position (integer values) mu[1, ] # select first row mu[, 1] # select first column mu[2, 2] # select second row and second column mu[c(1, 2, 3), ] # select multiple rows mu[c(-1, -2), ] # drop multiple rows Logical Operators == Equal To (NOTE: R uses a double equal sign) != Not Equal To &lt;, &gt;, &lt;=, &gt;= Less than, greater than, less than or equal to, and greater than or equal &amp; Equivalent to AND must match both conditions | Equivalent to OR must match at least one condition %in% equivalent to IN () in SQL (e.g. mu$areasymbol %in% c(\"IN001\", \"IN111\") grepl() similar to LIKE in SQL (e.g. grepl(\"IN%\", mu$areasymbol)) Filtering with logicals # Standard evaluation with base R [] # Filtering with logicals mu[mu$areasymbol == &quot;IN001&quot;, ] # select rows that equal IN001 mu[mu$areasymbol != &quot;IN001&quot;, ] # select rows that do not equal IN001 mu[, names(mu) == &quot;areasymbol&quot;] # select columns that equal areasymbol mu[, names(mu) %in% c(&quot;areasymbol&quot;, &quot;musym&quot;)] # select columns that match areasymbol and musym mu[grepl(&quot;Miami&quot;, mu$muname), ] # select rows that contain Miami # Non-standard evaluation with tidyverse library(dplyr) # Filtering rows filter(mu, areasymbol == &quot;IN001&quot;) filter(mu, areasymbol != &quot;IN001&quot;) filter(mu, areasymbol %in% c(&quot;IN001&quot;, &quot;IN111&quot;)) filter(mu, grepl(&quot;Miami&quot;, muname)) filter(mu, muacres &gt; 0) # Select columns select(mu, areasymbol, musym) # Slice rows slice(mu, 1:5) 1.13.3 Transforming Data This allows you to create new columns by convert, compute, or combine data within existing columns. mu &lt;- mutate( mu, # convert to hectares muhectares = muacres * 0.4047, # convert muname to TRUE or FALSE if Miami is present using pattern matching miami = grepl(&quot;Miami&quot;, muname), # compute % minor component n_minor = n_component - n_majcompflag, # combine columns key = paste(areasymbol, musym) ) 1.13.4 Sorting Data Sorting allows you to rearrange your data. Beware R has several similar functions (e.g. sort and order) for sorting data only work with specific datatypes. The tidyverse function arrange is designed to work with data frames. # sort ascending arrange(mu, areasymbol, muname) # sort descending arrange(mu, desc(areasymbol), desc(muname)) 1.13.5 Piping Data Another particularly useful feature provided by the magrittr package and used in the tidyverse is the use of pipe (%&gt;%). Base R also has a native pipe operator (|&gt;). Using the RStudio keyboard shortcut Ctrl + Shift + M inserts the pipe you have selected as default in Global Options &gt; Code. f(x,y) becomes x %&gt;% f(y) The “pipe” is something that occurs in many programming languages and computing contexts. It allows output from one expression to be passed as input to the first argument of the next function. This allows sequences of commands to be read from right to left b(or top to bottom) rather than from the inside out. # non-piping example 1 mu_sub &lt;- filter(mu, areasymbol == &quot;IN001&quot;) mu_sub &lt;- mutate(mu_sub, pct_100less = pct_component &lt; 100) # non-piping example 2 mu_sub &lt;- mutate(filter(mu, areasymbol == &quot;IN001&quot;), pct_100less = pct_component &lt; 100) # piping mu_sub &lt;- mu %&gt;% filter(areasymbol == &quot;IN001&quot;) %&gt;% mutate(pct_100less = pct_component &lt; 100) 1.13.6 Merging/Joining or Combining Data Joining When working with tabular data you often have 2 or more tables you need to join. There are several ways to join tables. Which direction to join and which columns to join will determine how you achieve the join. # inner join leg_mu &lt;- inner_join(leg, mu, by = c(&quot;liid&quot;, &quot;areasymbol&quot;)) # left join leg_mu &lt;- left_join(leg, mu, by = c(&quot;liid&quot;)) # right_join leg_mu &lt;- right_join(leg, mu, by = &quot;liid&quot;) Combining If your tables have the same structure (e.g. columns), or length and order you may simply combine them. For example, if you have two different mapunit tables. # combine rows rbind(mu, mu) rbind(mu, leg) # won&#39;t work # combine columns cbind(mu, mu) # beware combine tables with duplicate column names cbind(mu, areasymbol_2 = mu$areasymbol) cbind(mu, leg) # won&#39;t work 1.13.7 Aggregating or Grouping Data Because soil data has multiple dimensions (e.g. properties and depths) and levels of organization (e.g. many to one relationships), it is often necessary to aggregate it. For example, when we wish to make a map we often need to aggregate over components and then map units. Depending on the data type this aggregation may involve taking a weighted average or selecting the dominant condition. The group_by function defines the groups over which we wish to summarize the data. mu_agg &lt;- mu %&gt;% group_by(grpname, areasymbol) %&gt;% summarize(sum_muacres = sum(muacres), n_musym = length(musym)) 1.13.8 Reshaping Data Typically data is stored in what is known as a wide format, where each column contains a different variable (e.g. depth, clay, sand, rocks). However, sometimes it is necessary to reshape or pivot to a long format, where each variable/column is compressed into 2 new rows. One new column contains the old column names, while another new column contains the values from the old columns. This is particularly useful when combining multiple variables into a single plot. library(tidyr) # Simplify mu example dataset mu2 &lt;- mu %&gt;% select(grpname, areasymbol, musym, muacres, n_component, pct_hydric) %&gt;% slice(1:5) print(mu2) # Pivot long mu2_long &lt;- pivot_longer(mu2, cols = c(muacres, n_component, pct_hydric)) print(mu2_long) # Pivot wide mu2_wide &lt;- pivot_wider(mu2_long, names_from = name) print(mu2_wide) 1.13.9 Exporting Data To export data from R, use the command write.csv() or write.dbf() functions. Since we have already set our working directory, R automatically saves our file into the working directory. write.csv(mu_agg, file = &quot;mu_agg.csv&quot;) library(foreign) write.dbf(as.data.frame(mu_agg), file = &quot;mu_agg.dbf&quot;) 1.14 Exercise 2: Data Manipulation Save the code you use in an R script, add answers as comments, and send to your mentor. To get information from the NASIS legend table for the state of Wisconsin use the soilDB function get_legend_from_NASISWebReport() for mlraoffice = \"%\" and areasymbol = \"WI%\" Filter the legend table for rows where areaacres is less than 200,000. Inspect the result to find the areasymbol values. Load the mapunit table, using soilDB get_mapunit_from_NASISWebReport() using the area symbols you identified in step 3. Calculate the acreage of hydric soils for each mapunit by multiplying muacres by pct_hydric. Note: pct_hydric is a percentage, not a proportion. Aggregate the total acreage of hydric soils each soil survey area using dplyr functions group_by() and summarize(). Join the aggregated mapunit table from Step 6 to the legend table from Step 3 using dplyr left_join(). Calculate the proportion of the total soil survey area acres (areaacres) that are hydric soils. Answer the following questions: What Wisconsin soil survey areas are less than 200,000 acres? What proportion of those soil survey areas are hydric soils? Bonus: How does your joined result in Step 7 differ if you replace dplyr left_join() with inner_join()? Why? 1.15 Review Given what you now know about R, try to answer the following questions: Can you think of a situation where an existing hypothesis or conventional wisdom was not repeatable? What are packages? What is GitHub? Where can you get help? What is a data frame? What are 3 ways you can manipulate a data frame? 1.16 Additional Reading (Introduction) Introductory R Books R for Data Science Posit (RStudio) Cheatsheets Quick-R Advanced DSM R Books Predictive Soil Mapping with R Using R for Digital Soil Mapping (not free) Soil Spectral Inference with R (not free) Soil Science R Applications aqp and soilDB tutorials ISRIC World Soil Information Example Training Courses ISRIC World Soil Information YouTube Channel OpenGeoHub Trainings OpenGeoHub YouTube Channel David Rossiter’s Cornell Homepage Pierre Roudier Soil Sciences and Statistics Review Articles Arkley, R., 1976. Statistical Methods in Soil Classification Research. Advances in Agronomy 28:37-70. https://www.sciencedirect.com/science/article/pii/S0065211308605520 Mausbach, M., and L. Wilding, 1991. Spatial Variability of Soils and Landforms. Soil Science Society of America, Madison. https://dl.sciencesocieties.org/publications/books/tocs/sssaspecialpubl/spatialvariabil Wilding, L., Smeck, N., and G. Hall, 1983. Spatial Variability and Pedology. In : L. Widling, N. Smeck, and G. Hall (Eds). Pedogenesis and Soil Taxonomy I. Conceps and Interactions. Elseiver, Amsterdam, pp. 83-116. https://www.sciencedirect.com/science/article/pii/S0166248108705993 References "],["data.html", "Chapter 2 The Data We Use 2.1 Objectives (The Data We Use) 2.2 The Structure of Soil Data 2.3 Challenges with Pedon Data 2.4 The SoilProfileCollection 2.5 Exercise 1: Assemble a SoilProfileCollection from several CSV files 2.6 Using the soilDB Package 2.7 Basic Data Inspection 2.8 Exercise 2: O Horizon Thickness 2.9 Generalized Horizon Labels 2.10 fetchNASIS() data checks 2.11 Extended Data Functions 2.12 Exercise 3: Diagnostic Horizons in Your Own Data 2.13 Custom Queries to Local NASIS Database 2.14 Exercise 4: Generalized Horizons with Loafercreek", " Chapter 2 The Data We Use 2.1 Objectives (The Data We Use) Expand on basic R skills from Chapter 1 Inspect and work with different data types Perform operations on data such as filtering and aggregation Begin to explore regular expression (regex) patterns for text data Work with Soil Data Sources and Structures Use the soilDB package to load data into R Understand the SoilProfileCollection (SPC) object Learn about the data checks in the fetchNASIS() function 2.2 The Structure of Soil Data What if you could extract, organize, and visualize data from NASIS and many other commonly used soil database sources with a couple of lines of code? The aqp (Algorithms for Quantitative Pedology) and soilDB packages enable data to be fetched from various sources and cast into a SoilProfileCollection (SPC) object. Tabular and spatial data objects fetched via soilDB and processed using aqp methods can simplify the process of working with commonly used soil data. 2.2.1 Package References Package ‘aqp’ manual Package ‘soilDB’ manual Package ‘sharpshootR’ manual SoilProfileCollection Object Introduction Tutorials on the AQP website We load aqp and soilDB packages using the library() command. # load the libraries library(aqp) library(soilDB) The manual pages for soilDB and aqp are accessible online and from the Help tab in RStudio. 2.2.2 Importance of Pedon Data The importance of pedon data for present and future work cannot be overstated. These data represent decades of on-the-ground observations of the soil resource for a given area. As difficult as it may be to take the time to enter legacy pedon data, it is vitally important that we capture this resource and get these data into NASIS as an archive of point observations. 2.2.3 Some Issues With Pedon Data Making and documenting observations of soil requires hard work. Digging is difficult, and writing soil descriptions is time consuming! Our confidence in observations typically weakens with the depth of the material described. If we acknowledge this, which we must, then how do we deal with it in pedon data? Use a cutoff depth, for example 100 cm, can be used to truncate observations to a zone of greater confidence. Show the relative confidence of the data with depth. 2.3 Challenges with Pedon Data Consistency Missing data Confidence in the observations Uncertainty with depth Description style differences Depth described, horizonation usage styles Legacy data vintage Decadal span of data Taxonomy updates, horizon nomenclature changes Location confidence Origin of the location information Datum used for data collection Accuracy for GPS values at the time of data collection 2.4 The SoilProfileCollection The SoilProfileCollection class (SPC) provided by the aqp package is a specialized structure for soil data analysis. It simplifies the process of working with collections of data associated with soil profiles, e.g., site-level, horizon-level, spatial, diagnostic horizons, and other metadata. A SoilProfileCollection is similar to the NASIS Site/Pedon “object” in that it provides generalizations, specific routines and rules about data tables and their relationships. The SoilProfileCollection is an S4 R object. S4 objects have slots. Of primary importance, are the slots for site-level and horizon-level data. In many ways the SPC is more adaptable than the NASIS “Pedon” concept because it is more general. However, the SPC is not as expressive as the complex hierarchy of objects in NASIS, which are more aligned with data archival vs. analysis. 2.4.1 SoilProfileCollection methods Many “familiar” methods are defined for the SoilProfileCollection object. Some are unique, and others operate like more common functions of vector and data.frame objects, such as nrow() (“how many horizons?”) or length() (“how many sites/pedons?”). Perhaps most importantly, when you access the site data (with site(&lt;object&gt;)) or the horizon data (with horizons(&lt;object&gt;)) of a SoilProfileCollection, you get a data.frame object that you can use like any other you might use or make in R. 2.4.1.1 Promoting a data.frame to SoilProfileCollection The SoilProfileCollection object is a collection of 1-dimensional profile descriptions, of the type conventionally described on a Form 232, or of tabular data returned from laboratory. The object is “horizon data forward” in that you start with the layers, and can add or create site-level attributes by normalization, joins, and calculation. Most of the time if you are using your NASIS data, or an official database, there are defined ways of getting the data “into” an SPC. For example, fetchOSD returns a SoilProfileCollection that has been assembled from horizon and site level attributes gleaned from the OSDs text, Soil Classification database, and other sources. In the pre-course, we had you set up a process so you could connect to your local NASIS instance to “fetch” data and have methods like fetchNASIS put things together for you. This input to make a SoilProfileCollection can be represented most simply as a data.frame with unique site or profile ID and depth combinations for each horizon or layer–for example, a subset of the phorizon or chorizon table in NASIS. A simple demonstration of “tabular horizon data” is the sp4 data set bundled with aqp: some serpentine soil profiles stored in a data.frame in the aqp package (after McGahan et al., 2009). library(aqp) # Load sample serpentine soil data (McGahan et al., 2009) data(sp4, package = &quot;aqp&quot;) # this is a data.frame # same as if loaded from CSV file etc. class(sp4) ## [1] &quot;data.frame&quot; # inspect the first couple of rows head(sp4) ## id name top bottom K Mg Ca CEC_7 ex_Ca_to_Mg sand silt clay CF ## 1 colusa A 0 3 0.3 25.7 9.0 23.0 0.35 46 33 21 0.12 ## 2 colusa ABt 3 8 0.2 23.7 5.6 21.4 0.23 42 31 27 0.27 ## 3 colusa Bt1 8 30 0.1 23.2 1.9 23.7 0.08 40 28 32 0.27 ## 4 colusa Bt2 30 42 0.1 44.3 0.3 43.0 0.01 27 18 55 0.16 ## 5 glenn A 0 9 0.2 21.9 4.4 18.8 0.20 54 20 25 0.55 ## 6 glenn Bt 9 34 0.3 18.9 4.5 27.5 0.20 49 18 34 0.84 To convert this horizon data into a SoilProfileCollection, we need to identify three parameters: idname, top, and bottom. These parameters refer to the columns of unique profile IDs, top depths and bottom depths, respectively. There are a couple of important constraints and considerations: records (rows) represent horizons profiles are uniquely identified by a column (user pedon ID, pedon record ID, etc.) profiles IDs cannot contain missing values (NA) horizon top and bottom depths are identified by column names ideally there are no gaps, overlap, or missing top/bottom depths (more on that later) Use a formula to specify column names in the data.frame, in this case \"id\", \"top\" and \"bottom\". # profile ID ~ top depth + bottom depth depths(sp4) &lt;- id ~ top + bottom # note new class class(sp4) ## [1] &quot;SoilProfileCollection&quot; ## attr(,&quot;package&quot;) ## [1] &quot;aqp&quot; # compact summary sp4 ## SoilProfileCollection with 10 profiles and 30 horizons ## profile ID: id | horizon ID: hzID ## Depth range: 16 - 49 cm ## ## ----- Horizons (6 / 30 rows | 10 / 14 columns) ----- ## id hzID top bottom name K Mg Ca CEC_7 ex_Ca_to_Mg ## colusa 1 0 3 A 0.3 25.7 9.0 23.0 0.35 ## colusa 2 3 8 ABt 0.2 23.7 5.6 21.4 0.23 ## colusa 3 8 30 Bt1 0.1 23.2 1.9 23.7 0.08 ## colusa 4 30 42 Bt2 0.1 44.3 0.3 43.0 0.01 ## glenn 5 0 9 A 0.2 21.9 4.4 18.8 0.20 ## glenn 6 9 34 Bt 0.3 18.9 4.5 27.5 0.20 ## [... more horizons ...] ## ## ----- Sites (6 / 10 rows | 1 / 1 columns) ----- ## id ## colusa ## glenn ## kings ## mariposa ## mendocino ## napa ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] The formula expresses the idea that a profile id defined by set of top and bottom depths. NOTE: A character vector with same names has the same effect, and can be easier to “program” with than the formula-based syntax. depths(sp4) &lt;- c(&quot;id&quot;, &quot;top&quot;, &quot;bottom&quot;) 2.4.2 Promoting to “Spatial” SoilProfileCollection You can also use the SoilProfileCollection to manage the information about a profile’s position on the Earth. Chapter 4 will cover spatial data in greater detail, and the SoilProfileCollection Reference has a section on Spatial Data. For now know you can use the initSpatial&lt;- method to define “X” and “Y” coordinate columns and the coordinate reference system in one line: sp4$x &lt;- runif(10) sp4$y &lt;- runif(10) # dummy XY coordinates initSpatial(sp4, crs = &quot;OGC:CRS84&quot;) &lt;- ~ x + y This is new syntax introduced in aqp 2.0, the older syntax uses the coordinates&lt;- and proj4string&lt;- methods. 2.4.2.1 Extracting Site and Horizon Data You can extract values from the collection’s @site and @horizon slots using the site() and horizons() functions. These create data.frame objects that are separate from the SoilProfileCollection. # extract site data from SPC into new data.frame &#39;s&#39; # note that it only contains profile IDs s &lt;- site(sp4) str(s) ## &#39;data.frame&#39;: 10 obs. of 1 variable: ## $ id: chr &quot;colusa&quot; &quot;glenn&quot; &quot;kings&quot; &quot;mariposa&quot; ... # extract horizon data from SPC into new data.frame &#39;h&#39; h &lt;- horizons(sp4) str(h) ## &#39;data.frame&#39;: 30 obs. of 14 variables: ## $ id : chr &quot;colusa&quot; &quot;colusa&quot; &quot;colusa&quot; &quot;colusa&quot; ... ## $ name : chr &quot;A&quot; &quot;ABt&quot; &quot;Bt1&quot; &quot;Bt2&quot; ... ## $ top : int 0 3 8 30 0 9 0 4 13 0 ... ## $ bottom : int 3 8 30 42 9 34 4 13 40 3 ... ## $ K : num 0.3 0.2 0.1 0.1 0.2 0.3 0.2 0.6 0.8 0.6 ... ## $ Mg : num 25.7 23.7 23.2 44.3 21.9 18.9 12.1 12.1 17.7 28.3 ... ## $ Ca : num 9 5.6 1.9 0.3 4.4 4.5 1.4 7 4.4 5.8 ... ## $ CEC_7 : num 23 21.4 23.7 43 18.8 27.5 23.7 18 20 29.3 ... ## $ ex_Ca_to_Mg: num 0.35 0.23 0.08 0.01 0.2 0.2 0.58 0.51 0.25 0.2 ... ## $ sand : int 46 42 40 27 54 49 43 36 27 42 ... ## $ silt : int 33 31 28 18 20 18 55 49 45 26 ... ## $ clay : int 21 27 32 55 25 34 3 15 27 32 ... ## $ CF : num 0.12 0.27 0.27 0.16 0.55 0.84 0.5 0.75 0.67 0.25 ... ## $ hzID : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... 2.4.2.2 Methods like data.frame The base R functions for accessing and setting data.frame columns by name such as $ and [[ work for SoilProfileCollection objects, too. Review data.frame methods: [[ and $: single columns in data.frame, by name x[['variable']] x$variable [: combinations of rows and columns, by name or index x[i, ]: specified rows, all columns x[, j]: all rows, specified columns x[i, j]: specified rows, specified columns See Chapter 1 and the Chapter 2 Appendix for additional details and examples. 2.4.2.2.1 Column Access by Name: $ and [[ # sp4 is a SoilProfileCollection sp4$clay ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 sp4[[&#39;clay&#39;]] ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 # horizon data.frame h$clay ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 h[[&#39;clay&#39;]] ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 # use $&lt;- or [[&lt;- to set proportional clay content sp4$clay &lt;- sp4[[&#39;clay&#39;]] / 100 # undo what we did above; back to percentage sp4[[&#39;clay&#39;]] &lt;- sp4$clay * 100 # create new site variable (&quot;numberone&quot; recycled for all sites) site(sp4)$newvar1 &lt;- &quot;numberone&quot; # create new horizon variable (&quot;numbertwo&quot; recycled for all horizons) horizons(sp4)$newvar2 &lt;- &quot;numbertwo&quot; 2.4.2.2.2 Row Access: [ The SoilProfileCollection also has [ (“single bracket”), but with a different interpretation from the [i, j] indexing of data.frame objects. In a data.frame you have object[row, column, drop=TRUE]; the result is a data.frame (or a vector with default drop=TRUE). In a SoilProfileCollection you have object[site, horizon]; the result is a SoilProfileCollection. # i-index: first 2 profiles, all horizons sp4[1:2, ] ## SoilProfileCollection with 2 profiles and 6 horizons ## profile ID: id | horizon ID: hzID ## Depth range: 34 - 42 cm ## ## ----- Horizons (6 / 6 rows | 10 / 15 columns) ----- ## id hzID top bottom name K Mg Ca CEC_7 ex_Ca_to_Mg ## colusa 1 0 3 A 0.3 25.7 9.0 23.0 0.35 ## colusa 2 3 8 ABt 0.2 23.7 5.6 21.4 0.23 ## colusa 3 8 30 Bt1 0.1 23.2 1.9 23.7 0.08 ## colusa 4 30 42 Bt2 0.1 44.3 0.3 43.0 0.01 ## glenn 5 0 9 A 0.2 21.9 4.4 18.8 0.20 ## glenn 6 9 34 Bt 0.3 18.9 4.5 27.5 0.20 ## ## ----- Sites (2 / 2 rows | 2 / 2 columns) ----- ## id newvar1 ## colusa numberone ## glenn numberone ## ## Spatial Data: ## [EMPTY] # j-index: all profiles; first 2 horizons of each profile sp4[, 1:2] ## SoilProfileCollection with 10 profiles and 20 horizons ## profile ID: id | horizon ID: hzID ## Depth range: 5 - 40 cm ## ## ----- Horizons (6 / 20 rows | 10 / 15 columns) ----- ## id hzID top bottom name K Mg Ca CEC_7 ex_Ca_to_Mg ## colusa 1 0 3 A 0.3 25.7 9.0 23.0 0.35 ## colusa 2 3 8 ABt 0.2 23.7 5.6 21.4 0.23 ## glenn 5 0 9 A 0.2 21.9 4.4 18.8 0.20 ## glenn 6 9 34 Bt 0.3 18.9 4.5 27.5 0.20 ## kings 7 0 4 A 0.2 12.1 1.4 23.7 0.58 ## kings 8 4 13 Bt1 0.6 12.1 7.0 18.0 0.51 ## [... more horizons ...] ## ## ----- Sites (6 / 10 rows | 2 / 2 columns) ----- ## id newvar1 ## colusa numberone ## glenn numberone ## kings numberone ## mariposa numberone ## mendocino numberone ## napa numberone ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] When you use the [ function, everything in the SoilProfileCollection is subset simultaneously depending on the constraints specified by the indices. # First profile, first 2 horizons horizons(sp4[1, 1:2]) ## id name top bottom K Mg Ca CEC_7 ex_Ca_to_Mg sand silt clay CF hzID newvar2 ## 1 colusa A 0 3 0.3 25.7 9.0 23.0 0.35 46 33 21 0.12 1 numbertwo ## 2 colusa ABt 3 8 0.2 23.7 5.6 21.4 0.23 42 31 27 0.27 2 numbertwo All slots in the collection have a relationship to the site or i-index. When you remove sites (profiles), all associated records (e.g. spatial, diagnostics, horizons, etc.) in the object are removed. Similarly, when all horizons are removed (say, you request the 6th j-index from a profile that has only 5 layers), the site index and all associated data are removed from the collection. 2.5 Exercise 1: Assemble a SoilProfileCollection from several CSV files Save the code you use in an R script, add answers as comments, and send to your mentor. Link to exercise R code Questions: Run the code in the linked R file and answer these questions. How many profiles (sites) and horizons are in the granite SoilProfileCollection? How many in andesite? See length() and nrow() functions for SoilProfileCollection objects. Which profile (and which horizon in that profile) has the highest ratio of oxalate-extractable Fe to dithionite-citrate-extractable Fe (Fe_o_to_Fe_d)? 2.6 Using the soilDB Package The soilDB package for R provides functions for accessing data stored in NASIS, KSSL, SDA, SoilWeb, SoilGrids and other sources. These high-level ‘fetch’ functions bundle or wrap lower-level ‘get’ functions which access internal database interfaces to NASIS and other data sources. The ODBC connection to NASIS that you set up during the pre-course is an example of an internal database interface. Basic data checks are run within ‘fetch’ functions. These checks ensure the basic integrity of the data as it is queried and moved from its existing structure into an SPC. There are times when it is useful to use the lower-level get functions individually. They generally return single data.frame or list of data.frame. You can set up scripts to make custom queries against these or other sources on your own – there is an example at the end of this section. For now, we will start with the ‘fetch’ functions and others that will get you a large variety of data you can use for soil and ecological site analyses. 2.6.1 soilDB functions for tabular data soilDB functions are the quickest way to get up and running: fetchNASIS() Gets and re-packages data from a local NASIS database. soilDB Vignette Columns in fetchNASIS(from=\"pedons\") fetchVegdata() Gets Vegetation Plot and related/child tables into a list from a local NASIS database. fetchNASISLabData() Gets KSSL laboratory pedon/horizon layer data from a local NASIS database. fetchNASISWebReport() SDA_query() Can be used to access SSURGO, STATSGO (spatial and tabular), and Lab DataMart snapshots Submits queries to the Soil Data Access system. Soil Data Access Tutorial SDA and Spatial Data SDA and Interpretations fetchLDM() Gets KSSL data from the Lab Data Mart snapshot in Soil Data Access fetchSDA() Fetches legend/mapunit/component/horizon data from Soil Data Access. fetchKSSL() Gets KSSL data from the SoilWeb system via BBOX, MLRA, or series name query. KSSL Data Demo Water Retention Curve Development from KSSL Data fetchOSD() Fetches a limited subset of horizon- and site-level attributes for named soil series from the SoilWeb system. Querying Soil Series Data OSDquery() Full-text searching of OSD sections. Querying Soil Series Data fetchSCAN() Queries soil and climate data from USDA-NRCS SCAN Stations. A Unified Interface to SCAN/SNOTEL Data fetchHenry() Downloads data from the Henry Mount Soil Climate Database. Henry Mount Soil Climate Database Tutorial fetchPedonPC() Fetches commonly used site and horizon data from a PedonPC (MS Access) database. 2.6.2 Open Database Connectivity (ODBC) Connection to NASIS After setting up an ODBC connection, as you did as part of the pre-course, you can use R to access data from a selected set defined in your local NASIS database. How to Create an ODBC Connection to local NASIS database for R. Does NASIS need to be open and running to query data using soilDB? No, fetchNASIS() works whether the NASIS application is running or not. You just need to make sure that the data you want has been loaded into your selected set. 2.6.3 fetchNASIS() The fetchNASIS() convenience function extracts data from a NASIS selected set via Structured Query Language (SQL). Note that the import process in fetchNASIS(), and the other methods, is not comprehensive. It does not pull every column for every table related to pedon data out of NASIS. Instead, it pulls essential / commonly used pedon and horizon data. Higher level functions like fetchNASIS() bundle a series of lower-level queries to get specific parts of the Pedon or Component data structures. Much of the nested complexity of NASIS is simplified in the resulting object. Many-to-one relationships are “flattened” where possible by fetchNASIS(). This aggregates the data from various tables into one “site” record with related horizon records, per profile. You can see the child tables that are aggregated using the get_extended_data_from_NASIS() method, which returns a named list of child table sources that can be joined to the SoilProfileCollection made with fetchNASIS() using the internal record IDs. 2.6.3.1 fetchNASIS arguments fetchNASIS() has a number of different arguments: from = ‘pedons’ or ‘components’ or ‘pedon_report’ This option allows you to select which data you want to load from NASIS. Choosing either ‘pedons’ or ‘components’ will load data from your local database. If ‘pedon_report’ is specified then it will load data from the text file generated by the NASIS report ‘fetchNASIS’ (run offline). This is useful for loading more than 20,000 pedons at one time, such for an entire Soil Survey Region. url = string specifying the (temporary) URL for the NASIS pedon_report output generated by the fetchNASIS NASIS Report that can be run “Offline Against National Database” EXAMPLE OUTPUT (MT663) SS = TRUE/FALSE The Selected Set (SS) option allows you to choose whether you want the data to load from your current selected set in NASIS or from the local database tables. The default is set to TRUE so if unspecified fetchNASIS() will always load from the data in the selected set. stringAsFactors = NULL This option is no longer used. See soilDB::NASISDomainsAsFactor() rmHzErrors = TRUE/FALSE Setting this value to TRUE removes pedons that do not pass checks for horizon depth consistency. Default: FALSE nullFragsAreZero = TRUE/FALSE Setting this value to TRUE (the default) converts null entries for rock fragment volumes to 0. This is typically the right assumption because rock fragment data are typically populated only when observed. If you know that your data contain a combination of omitted information (e.g. no rock fragment volumes are populated) then consider setting this argument to FALSE. soilColorState = ‘moist’ or ‘dry’ Select dry or moist colors to be converted and placed into a horizon-level attribute called soil_color. The default is set to ‘moist’ unless specified. Moist and dry colors are also stored in moist_soil_color and dry_soil_color. lab = TRUE/FALSE This option allows for loading the data associated with horizons that may be in the phlabresults table. The default is set to FALSE, which will not load records from the phlabresults table. fill = TRUE/FALSE This option adds horizons to pedons or components that lack horizon data in the database, preserving their “position” within the output SoilProfileCollection object. Default is FALSE such that profiles without horizons are omitted from the result. dropAdditional = TRUE/FALSE Used only for from='components' with duplicates = TRUE. Prevent “duplication” of mustatus == \"additional\" mapunits? Default: TRUE dropNonRepresentative = TRUE/FALSE Used only for from='components' with duplicates = TRUE. Prevent “duplication” of non-representative data mapunits? Default: TRUE duplicates = FALSE Used only for from='components'. Duplicate components for all instances of use (i.e. one for each legend data mapunit is used on; optionally for additional mapunits, and/or non-representative data mapunits?). This will include columns from get_component_correlation_data_from_NASIS_db() that identify which legend(s) each component is used on. NOTE: This requires that parent tables above “component” (such as legend, mapunit, correlation and datamapunit) are loaded in your NASIS database/selected set. dsn = NULL Optional: custom path to an SQLite snapshot or DBIConnection object to database with NASIS schema. See soilDB::createStaticNASIS(). 2.6.4 The gopheridge Dataset The gopheridge sample data set is a sample R object returned from fetchNASIS() in a self-contained .rda file stored in soilDB. Open RStudio, and set up the environment by loading packages and the gopheridge sample dataset. library(aqp) library(soilDB) # load example dataset data(gopheridge, package = &quot;soilDB&quot;) # what kind of object is this? class(gopheridge) ## [1] &quot;SoilProfileCollection&quot; ## attr(,&quot;package&quot;) ## [1] &quot;aqp&quot; # what does the internal structure look like? str(gopheridge, 2) ## Formal class &#39;SoilProfileCollection&#39; [package &quot;aqp&quot;] with 8 slots ## ..@ idcol : chr &quot;peiid&quot; ## ..@ hzidcol : chr &quot;phiid&quot; ## ..@ depthcols : chr [1:2] &quot;hzdept&quot; &quot;hzdepb&quot; ## ..@ metadata :List of 6 ## ..@ horizons :&#39;data.frame&#39;: 317 obs. of 76 variables: ## ..@ site :&#39;data.frame&#39;: 52 obs. of 172 variables: ## ..@ diagnostic :&#39;data.frame&#39;: 164 obs. of 4 variables: ## ..@ restrictions:&#39;data.frame&#39;: 56 obs. of 8 variables: There are many columns in the data.frame objects within a fetchNASIS() SoilProfileCollection. The following guide is part of the soilDB documentation and describes the columns that are returned from a fetchNASIS(from=\"pedons\") call. In the future this guide will be extended to describe component data. fetchNASIS(from=“pedons”) column description guide With siteNames() and horizonNames() we can view the names for eachslot respectively in the gopheridge object. # the fields at the site and horizon levels within the SPC siteNames(gopheridge) ## [1] &quot;peiid&quot; &quot;siteiid&quot; ## [3] &quot;ecositeid&quot; &quot;ecositenm&quot; ## [5] &quot;ecositecorrdate&quot; &quot;es_classifier&quot; ## [7] &quot;siteecositehistory.classifier&quot; &quot;es_selection_method&quot; ## [9] &quot;upedonid&quot; &quot;siteobsiid&quot; ## [11] &quot;usiteid&quot; &quot;site_id&quot; ## [13] &quot;pedon_id&quot; &quot;obsdate&quot; ## [15] &quot;obs_date&quot; &quot;utmzone&quot; ## [17] &quot;utmeasting&quot; &quot;utmnorthing&quot; ## [19] &quot;horizdatnm&quot; &quot;longstddecimaldegrees&quot; ## [21] &quot;latstddecimaldegrees&quot; &quot;gpspositionalerror&quot; ## [23] &quot;describer&quot; &quot;descname&quot; ## [25] &quot;pedonpurpose&quot; &quot;pedontype&quot; ## [27] &quot;pedlabsampnum&quot; &quot;labdatadescflag&quot; ## [29] &quot;tsectstopnum&quot; &quot;tsectinterval&quot; ## [31] &quot;utransectid&quot; &quot;tsectkind&quot; ## [33] &quot;tsectselmeth&quot; &quot;erocl&quot; ## [35] &quot;elev_field&quot; &quot;slope_field&quot; ## [37] &quot;aspect_field&quot; &quot;elev&quot; ## [39] &quot;slope&quot; &quot;aspect&quot; ## [41] &quot;ecostatename&quot; &quot;ecostateid&quot; ## [43] &quot;commphasename&quot; &quot;commphaseid&quot; ## [45] &quot;plantassocnm&quot; &quot;earthcovkind1&quot; ## [47] &quot;earthcovkind2&quot; &quot;bedrckdepth&quot; ## [49] &quot;bedrckkind&quot; &quot;bedrckhardness&quot; ## [51] &quot;pmgroupname&quot; &quot;hillslopeprof&quot; ## [53] &quot;geomslopeseg&quot; &quot;shapeacross&quot; ## [55] &quot;shapedown&quot; &quot;slopecomplex&quot; ## [57] &quot;drainagecl&quot; &quot;geomposhill&quot; ## [59] &quot;geomposmntn&quot; &quot;geompostrce&quot; ## [61] &quot;geomposflats&quot; &quot;swaterdepth&quot; ## [63] &quot;flodfreqcl&quot; &quot;floddurcl&quot; ## [65] &quot;flodmonthbeg&quot; &quot;pondfreqcl&quot; ## [67] &quot;ponddurcl&quot; &quot;pondmonthbeg&quot; ## [69] &quot;climstaid&quot; &quot;climstanm&quot; ## [71] &quot;climstatype&quot; &quot;ffd&quot; ## [73] &quot;map&quot; &quot;reannualprecip&quot; ## [75] &quot;airtempa&quot; &quot;soiltempa&quot; ## [77] &quot;airtemps&quot; &quot;soiltemps&quot; ## [79] &quot;airtempw&quot; &quot;soiltempw&quot; ## [81] &quot;surface_fine_gravel&quot; &quot;surface_gravel&quot; ## [83] &quot;surface_cobbles&quot; &quot;surface_stones&quot; ## [85] &quot;surface_boulders&quot; &quot;surface_channers&quot; ## [87] &quot;surface_flagstones&quot; &quot;surface_parafine_gravel&quot; ## [89] &quot;surface_paragravel&quot; &quot;surface_paracobbles&quot; ## [91] &quot;surface_parastones&quot; &quot;surface_paraboulders&quot; ## [93] &quot;surface_parachanners&quot; &quot;surface_paraflagstones&quot; ## [95] &quot;surface_unspecified&quot; &quot;surface_total_frags_pct_nopf&quot; ## [97] &quot;surface_total_frags_pct&quot; &quot;othervegid&quot; ## [99] &quot;othervegclass&quot; &quot;site_state&quot; ## [101] &quot;site_county&quot; &quot;site_mlra&quot; ## [103] &quot;slope_shape&quot; &quot;surface_fgravel&quot; ## [105] &quot;classdate&quot; &quot;classifier&quot; ## [107] &quot;classtype&quot; &quot;taxonname&quot; ## [109] &quot;localphase&quot; &quot;taxonkind&quot; ## [111] &quot;seriesstatus&quot; &quot;taxclname&quot; ## [113] &quot;taxpartsize&quot; &quot;taxorder&quot; ## [115] &quot;taxsuborder&quot; &quot;taxgrtgroup&quot; ## [117] &quot;taxsubgrp&quot; &quot;soiltaxedition&quot; ## [119] &quot;osdtypelocflag&quot; &quot;taxmoistcl&quot; ## [121] &quot;taxtempregime&quot; &quot;taxfamother&quot; ## [123] &quot;taxreaction&quot; &quot;taxfamhahatmatcl&quot; ## [125] &quot;psctopdepth&quot; &quot;pscbotdepth&quot; ## [127] &quot;selection_method&quot; &quot;ochric.epipedon&quot; ## [129] &quot;argillic.horizon&quot; &quot;paralithic.materials&quot; ## [131] &quot;lithic.contact&quot; &quot;paralithic.contact&quot; ## [133] &quot;abrupt.textural.change&quot; &quot;duripan&quot; ## [135] &quot;cambic.horizon&quot; &quot;andic.soil.properties&quot; ## [137] &quot;umbric.epipedon&quot; &quot;aquic.conditions&quot; ## [139] &quot;slickensides&quot; &quot;mollic.epipedon&quot; ## [141] &quot;mottles.with.chroma.2.or.less&quot; &quot;calcic.horizon&quot; ## [143] &quot;anthropic.epipedon&quot; &quot;kandic.horizon&quot; ## [145] &quot;densic.contact&quot; &quot;densic.materials&quot; ## [147] &quot;lithologic.discontinuity&quot; &quot;redox.depletions.with.chroma.2.or.less&quot; ## [149] &quot;redox.concentrations&quot; &quot;reduced.matrix&quot; ## [151] &quot;histic.epipedon&quot; &quot;albic.horizon&quot; ## [153] &quot;spodic.horizon&quot; &quot;fibric.soil.materials&quot; ## [155] &quot;hemic.soil.materials&quot; &quot;sapric.soil.materials&quot; ## [157] &quot;volcanic.glass&quot; &quot;folistic.epipedon&quot; ## [159] &quot;strongly.contrasting.particle.size.class&quot; &quot;episaturation&quot; ## [161] &quot;endosaturation&quot; &quot;secondary.carbonates&quot; ## [163] &quot;human.transported.material&quot; &quot;glossic.horizon&quot; ## [165] &quot;spodic.materials&quot; &quot;albic.materials&quot; ## [167] &quot;landform_string&quot; &quot;landscape_string&quot; ## [169] &quot;microfeature_string&quot; &quot;geomicrorelief_string&quot; ## [171] &quot;pmkind&quot; &quot;pmorigin&quot; horizonNames(gopheridge) ## [1] &quot;phiid&quot; &quot;peiid&quot; &quot;hzname&quot; &quot;dspcomplayerid&quot; ## [5] &quot;hzdept&quot; &quot;hzdepb&quot; &quot;bounddistinct&quot; &quot;boundtopo&quot; ## [9] &quot;claytotest&quot; &quot;silttotest&quot; &quot;sandtotest&quot; &quot;clay&quot; ## [13] &quot;silt&quot; &quot;sand&quot; &quot;fragvoltot&quot; &quot;texture&quot; ## [17] &quot;texcl&quot; &quot;lieutex&quot; &quot;phfield&quot; &quot;effclass&quot; ## [21] &quot;labsampnum&quot; &quot;rupresblkdry&quot; &quot;rupresblkmst&quot; &quot;rupresblkcem&quot; ## [25] &quot;stickiness&quot; &quot;plasticity&quot; &quot;ksatpedon&quot; &quot;texture_class&quot; ## [29] &quot;hzID&quot; &quot;d_hue&quot; &quot;d_value&quot; &quot;d_chroma&quot; ## [33] &quot;dry_soil_color&quot; &quot;d_r&quot; &quot;d_g&quot; &quot;d_b&quot; ## [37] &quot;d_sigma&quot; &quot;m_hue&quot; &quot;m_value&quot; &quot;m_chroma&quot; ## [41] &quot;moist_soil_color&quot; &quot;m_r&quot; &quot;m_g&quot; &quot;m_b&quot; ## [45] &quot;m_sigma&quot; &quot;soil_color&quot; &quot;fine_gravel&quot; &quot;gravel&quot; ## [49] &quot;cobbles&quot; &quot;stones&quot; &quot;boulders&quot; &quot;channers&quot; ## [53] &quot;flagstones&quot; &quot;parafine_gravel&quot; &quot;paragravel&quot; &quot;paracobbles&quot; ## [57] &quot;parastones&quot; &quot;paraboulders&quot; &quot;parachanners&quot; &quot;paraflagstones&quot; ## [61] &quot;unspecified&quot; &quot;total_frags_pct_nopf&quot; &quot;total_frags_pct&quot; &quot;art_fgr&quot; ## [65] &quot;art_gr&quot; &quot;art_cb&quot; &quot;art_st&quot; &quot;art_by&quot; ## [69] &quot;art_ch&quot; &quot;art_fl&quot; &quot;art_unspecified&quot; &quot;total_art_pct&quot; ## [73] &quot;huartvol_cohesive&quot; &quot;huartvol_penetrable&quot; &quot;huartvol_innocuous&quot; &quot;huartvol_persistent&quot; 2.6.4.1 Make profile sketches The plotSPC() or plot() function generates sketches of a SoilProfileCollection object based on horizon depths, designations, and colors. The SoilProfileCollection Reference vignette contains many examples demonstrating way in which these sketches can be customized. The Soil Profile Sketches tutorial contains additional examples that demonstrate ways to customize soil profile sketches. See ?plotSPC for a detailed list of arguments and examples. The fetchNASIS() function automatically converts moist Munsell colors into R-style colors, available in the soil_color horizon level attribute. This color is used by default in plotSPC() par(mar = c(1, 1, 1, 1)) # omitting pedon IDs and horizon designations plotSPC(gopheridge, print.id = FALSE, name = NA, width = 0.3) title(&#39;Pedons from the `gopheridge` sample dataset&#39;, line = -0.5) Additional examples / documentation related to soil profile sketches: SoilProfileCollection Reference OSD Dendrogram tutorial Visualization of Horizon Boundaries tutorial Competing Series tutorial 2.6.4.2 Pedon Data Checks When you load pedons using the fetchNASIS() function, the following data checks are performed: Inconsistent horizon boundaries. Pedons with inconsistent horizon boundaries are not loaded when rmHzErrors = TRUE. In most cases, this occurs when the bottom depth of a horizon is not the same as the upper depth of the next lower horizon. ## hzname top bot ## 1 A 0 30 ## 2 Bt1 38 56 ## 3 Bt2 56 121 ## 4 Bk 121 135 ## 5 R 135 NA Note the issue above. The bottom depth of the A horizon and the upper depth of the Bt1 horizon should be the same: either 30 or 38 cm. The correct depth needs to be determined and fixed in the database. Missing lower horizon depths. Offending horizons are fixed by replacing the missing bottom depth with the top depth plus 1 cm. In the case of the profile shown above, a bottom depth of 137 cm would be inserted where the depth is missing. Sites missing pedon records. Data without corresponding horizons are not loaded. 2.6.4.3 Find Data with Errors If errors in the pedon data are detected when loading data using fetchNASIS(), the following “get” commands can trace them back to the corresponding records in NASIS. These access an option that is stored in a special object called an environment associated with the soilDB package – they generally contain vectors of IDs to help locating the problematic data. get('sites.missing.pedons', envir = soilDB.env) Returns user site ID for sites missing pedons get('dup.pedon.ids', envir = soilDB.env) Returns user pedon ID for sites with duplicate pedon ID get('bad.pedon.ids', envir = soilDB.env) Returns user pedon ID for pedons with inconsistent horizon depths get('bad.horizons', envir = soilDB.env) Returns a data.frame of horizon-level information for pedons with inconsistent horizon depths Here is a full list of ID vectors that can be created when errors are detected: # fetchNASIS(&quot;pedons&quot;) get(&#39;sites.missing.pedons&#39;, envir = get_soilDB_env()) get(&#39;dup.pedon.ids&#39;, envir = get_soilDB_env()) get(&#39;bad.pedon.ids&#39;, envir = get_soilDB_env()) get(&#39;missing.bottom.depths&#39;, envir=get_soilDB_env()) get(&#39;top.bottom.equal&#39;, envir=get_soilDB_env()) get(&#39;bad.horizons&#39;, envir = get_soilDB_env()) get(&#39;rock.fragment.volume.gt100.phiid&#39;, envir=get_soilDB_env()) get(&#39;artifact.volume.gt100.phiid&#39;, envir=get_soilDB_env()) get(&#39;multisiteobs.surface&#39;, envir=get_soilDB_env()) get(&#39;surface.fragment.cover.gt100.siteobsiid&#39;, envir=get_soilDB_env()) get(&#39;multiple.labsampnum.per.phiid&#39;, envir=get_soilDB_env()) # fetchNASISLabData() get(&#39;bad.labpedon.ids&#39;, envir=get_soilDB_env()) # fetchNASIS(&quot;components&quot;), fetchNASISWebReport() get(&#39;dupe.coiids&#39;, envir=get_soilDB_env()) get(&#39;dupe.muiids&#39;, envir=get_soilDB_env()) get(&#39;multiple.mu.per.dmu&#39;, envir=get_soilDB_env()) get(&quot;component.hz.problems&quot;, envir = get_soilDB_env()) get(&#39;multiple.ecosite.per.coiid&#39;, envir=get_soilDB_env()) get(&#39;multiple.otherveg.per.coiid&#39;, envir=get_soilDB_env()) # Soil Data Access get(&#39;dup.compmgrp.cokeyrvindictor&#39;, envir=get_soilDB_env()) get(&#39;component.hz.problems&#39;, envir=get_soilDB_env()) get(&#39;component.ecosite.problems&#39;, envir = get_soilDB_env()) These get() calls access variables stored in the package environment soilDB.env. The variables only exist if there are “problems” / values found by the data checks. If you fix the errors in the NASIS database and the checks don’t find any errors then these vectors of IDs will not be defined. 2.7 Basic Data Inspection Now that you’ve loaded some data, you can look at additional ways to summarize and interact with data elements. 2.7.1 table(): Tabulation and Cross Tabulation The base R table() function is very useful for quick summary operations. It returns a named vector with the amount of each unique level of the a given vector. The numeric vector of “counts” is commonly combined with other functions such as sort(), order(), prop.table(), is.na() to identify abundance, proportions, or missing data (NA). # load required packages library(aqp) library(soilDB) data(&quot;gopheridge&quot;, package = &quot;soilDB&quot;) # for these examples, we use the gopheridge object as our &quot;selected set&quot; pedons &lt;- gopheridge ## you can use fetchNASIS to load your own data, like this: # pedons &lt;- fetchNASIS() # summarize which soil taxa we have loaded table(pedons$taxonname) ## ## Gopheridge ## 52 # sort taxonomic names in descending order sort(table(pedons$taxonname), decreasing = TRUE) ## Gopheridge ## 52 # could do the same thing for taxonomic subgroups table(pedons$taxsubgrp) ## ## mollic haploxeralfs typic haploxerepts ultic haploxeralfs ultic haploxerolls ## 1 6 44 1 sort(table(pedons$taxsubgrp), decreasing = TRUE) ## ## ultic haploxeralfs typic haploxerepts mollic haploxeralfs ultic haploxerolls ## 44 6 1 1 We can convert counts in the table() result into proportions with prop.table(): prop.table(table(pedons$taxsubgrp)) ## ## mollic haploxeralfs typic haploxerepts ultic haploxeralfs ultic haploxerolls ## 0.01923077 0.11538462 0.84615385 0.01923077 table() can be used to get counts over multiple dimensions of factor levels. This is called cross tabulation. For instance, let’s cross tabulate taxonomic subgroup (taxsubgrp) and the particle size family class (taxpartsize) for pedons table(pedons$taxsubgrp, pedons$taxpartsize) ## ## clayey-skeletal coarse-loamy fine fine-loamy loamy-skeletal ## mollic haploxeralfs 0 0 0 0 1 ## typic haploxerepts 1 0 0 0 5 ## ultic haploxeralfs 2 0 1 1 40 ## ultic haploxerolls 0 1 0 0 0 As expected with gopheridge the majority of pedons are Loamy-skeletal Ultic Haploxeralfs. Since pedons contains site and horizon level data, when cross tabulating we need to be sure pair a column from the site data with other site-level columns, and the same for horizon-level. This is because all arguments to table() must have the same length. For example, let’s cross-tabulate horizon designation (hzname) with horizon texture class (texcl). We can use the addmargins() function to add the row and column sums to the margins of the table for easier interpretation when there are many rows/columns in the result. addmargins(table(pedons$hzname, pedons$texcl)) ## ## c cl l scl sic sicl sil sl Sum ## 2BCt5 0 1 0 0 0 0 0 0 1 ## 2Bt1 0 0 3 0 0 0 0 0 3 ## 2Bt2 1 2 1 1 0 0 0 0 5 ## 2Bt3 1 3 1 0 0 0 0 0 5 ## 2Bt4 2 0 0 0 0 0 0 0 2 ## 2CBt 0 0 0 1 0 0 0 0 1 ## 2Cr 0 0 0 0 0 0 0 0 0 ## 2Crt 0 0 0 0 0 0 0 0 0 ## 2R 0 0 0 0 0 0 0 0 0 ## A 0 0 33 0 0 0 12 3 48 ## A1 0 0 2 0 0 0 2 0 4 ## A2 0 0 2 0 0 0 2 0 4 ## A3 0 0 1 0 0 0 0 0 1 ## AB 0 0 2 0 0 0 2 0 4 ## BA 0 0 17 0 0 1 0 0 18 ## BC 0 0 4 0 0 0 0 0 4 ## BCt 0 2 2 1 0 0 1 0 6 ## Bt 0 0 1 1 1 0 0 0 3 ## Bt1 0 3 28 1 0 3 5 0 40 ## Bt2 3 15 14 1 1 3 1 0 38 ## Bt3 4 8 6 1 0 1 1 0 21 ## Bt4 1 2 2 0 0 0 0 0 5 ## Bw 0 0 6 0 0 0 1 0 7 ## Bw1 0 0 5 0 0 0 0 0 5 ## Bw2 0 0 5 0 0 0 0 0 5 ## Bw3 0 0 3 0 0 0 0 0 3 ## C 1 0 1 0 0 0 0 0 2 ## C/Brt 0 1 0 0 0 0 0 0 1 ## CBt 0 0 0 1 0 0 0 0 1 ## Cr 0 0 1 0 0 0 0 0 1 ## Crt 0 0 0 0 0 0 0 0 0 ## Ct 0 0 1 0 0 0 0 0 1 ## Oe 0 0 0 0 0 0 0 0 0 ## Oi 0 0 0 0 0 0 0 0 0 ## R 0 0 0 0 0 0 0 0 0 ## Sum 13 37 141 8 2 8 27 3 239 2.7.2 Missing Values Missing values are encoded in R as NA. The table() function has a useNA argument that affects whether NA values are counted. Use is.na() to return a logical value that identifies missing data. table(pedons$taxsubgrp, useNA = &quot;ifany&quot;) ## ## mollic haploxeralfs typic haploxerepts ultic haploxeralfs ultic haploxerolls ## 1 6 44 1 # is.na(...) table(is.na(pedons$taxsubgrp)) ## ## FALSE ## 52 # is NOT NA !is.na(...) table(!is.na(pedons$taxsubgrp)) ## ## TRUE ## 52 # it can also be applied to horizon level columns in the SPC sort(table(pedons$texture), decreasing=TRUE) ## ## BR L GR-L GRV-L CBV-L SPM GRX-L SIL GRV-CL CBV-CL ## 58 36 33 24 18 14 12 12 9 8 ## GR-SIL CBX-L GRX-CL CBX-CL GRV-SIL CL GRV-SCL GRX-C MPM SL ## 7 6 5 4 4 3 3 3 3 3 ## CB-L GR-CL GRX-SCL PGR-C PGRX-L SICL STV-CL STV-L STX-C STX-L ## 2 2 2 2 2 2 2 2 2 2 ## C CB-C CB-CL CB-SCL CB-SIL CBV-SIL CBX-SCL CN-L CN-SICL CNX-L ## 1 1 1 1 1 1 1 1 1 1 ## CNX-SICL FLV-L GR-C GR-SIC GRV-SICL GRX-SIC GRX-SIL PCB-SICL PCBV-SICL PCN-C ## 1 1 1 1 1 1 1 1 1 1 ## PCNX-CL PGRV-C PGRV-CL PGRX-SCL PGRX-SIL ST-L STV-C STX-CL STX-SICL ## 1 1 1 1 1 1 1 1 1 2.7.3 Logical Operators Logical operators act on vectors for the purposes of comparison. == “EQUAL TO” != “NOT EQUAL TO” &lt; LESS than LESS than or equal to &lt;= &gt; GREATER than GREATER than or equal to &gt;= %in% Equivalent to IN () in SQL; same logic as match() but returns a logical, not integer Example: pedons$taxpartsize %in% c('loamy-skeletal', 'sandy-skeletal') Returns a vector of TRUE/FALSE equal in length to left-hand side &amp; logical AND | logical OR Any logical operation on NA returns NA, so it is important to make sure your data are complete before relying on logical expressions to find conditions of interest. 2.7.4 Pattern Matching The following examples use the grep() function to pattern match within the data. We do this to create an index of the SoilProfileCollection for records that match the specified pattern, and then use that index to filter to specific sites and profiles. Patterns are specified using regular expression (REGEX) syntax. This process can be applied to many different columns in the SPC based on how you need to filter the data. This example pattern matches on the taxsubgrp column, but another useful application might be to pattern match on geomorphology or parent material. Say we want to see what the variation of particle size classes are within a specific subgroup. We can use grep() to create a row index, then apply that index to the SoilProfileCollection. # create a numeric index for pedons with taxsubgroup containing &#39;typic&#39; idx &lt;- grep(&#39;typic&#39;, pedons$taxsubgrp) idx ## [1] 11 12 13 14 26 50 # use square bracket notation to subset &#39;typic&#39; soils in `subset1` object subset1 &lt;- pedons[idx,] # or use the index directly to summarize taxpartsize for &#39;typic&#39; soils sort(table(pedons$taxpartsize[idx]), decreasing = TRUE) ## ## loamy-skeletal clayey-skeletal ## 5 1 Note: grep() below has an invert argument (default FALSE). This option is very useful for excluding the results of the pattern matching process by inverting whatever the result is. grepl() is the logical version of grep(), so you can invert it using the logical NOT operator: !. Another method is to create an index using which() function. which() takes any logical vector (or expression), and it returns the indices (positions) where that expression returns TRUE. The use of which becomes more important when there are missing values (NA) in an expression. Do a graphical check to see the “typic” profiles are selected. Plot them in R using the SoilProfileCollection “plot” method (e.g., specialized version of the generic plot() function). # adjust margins par(mar=c(1,0,0,1)) # plot the first 10 profiles of subset1 plot(subset1[1:10, ], label = &#39;taxsubgrp&#39;, max.depth = 60) title(&#39;Pedons with the word &quot;typic&quot; at subgroup-level of Soil Taxonomy&#39;, line=-2) 2.7.4.1 Resources for Regular Expressions A great way to test out regular expressions, and get in-depth explanations of how the syntax is working, is to use an online expression builder. For example, Regex101 or RegExr. https://regex101.com/ &amp; https://regexr.com/ - Online regular expression testers For more information on using regular expressions in grep() for pattern matching operations, see: GNU Regular-expression-syntax Quick Start Guide :http://www.regular-expressions.info/quickstart.html Quick check: Review the chapter content, or run the commands, and review the documentation, to answer the questions. True or False: grepl() returns a numeric vector True or False: which(grepl('typic', pedons$taxsubgrp)) is the same as grep('typic', pedons$taxsubgrp). 2.7.4.2 Statistics for Soil Survey: REGEX Operator Reference . One character, any character * Zero-or-more Quantifier (of previous token) + One-or-more Quantifier (of previous token) {n} quantifier where n is the the number of a match “repeats” (of previous token) [A-Z!] ONE capital letter, or an exclamation mark [0-9]{2} TWO numbers (using { quantifier) | is equivalent to OR: Example: grep('loamy|sandy', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) “loamy OR sandy” ^ Anchor to beginning of string / line: Example: grep('^sandy', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) “STARTS WITH sandy” $ Anchor to end of string / line: Example: grep('skeletal$', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) “ENDS WITH skeletal” \\\\b Anchor to word boundary: Example: grep('\\\\bmesic', c(\"mesic\",\"thermic\",\"isomesic\")) “WORD STARTS WITH mesic” (e.g. not “isomesic”) 2.7.5 Filtering A variety of methods are available to subset or “filter” R data objects, from a simple data.frame or vector, to something more complex like a Spatial object or a SoilProfileCollection. You can index many R objects using numeric or logical expressions as above. There are also methods that make this process a little easier. The base R method for this is subset() and it works on data.frame objects. It is nice because you can specify column names without explicitly referencing the data set, since subset uses non-standard evaluation of expressions passed as arguments. 2.7.5.1 Filtering with aqp::subset() We use the SoilProfileCollection subset method, where we first specify a data (pedons) object then we can write expressions for the columns that exist in that object. Here, we combine two logical expressions to find taxsubgrp containing \"alfs\" (Alfisols) with obsdate before January 1st, 2010. subset2 &lt;- subset(pedons, grepl(&quot;alfs&quot;, taxsubgrp) &amp; obsdate &lt; as.POSIXlt(&quot;2010-01-01&quot;)) # check taxonomic range of particle size classes in the data # overwhelmingly these are described as loamy-skeletal ultic haploxeralfs sort(table(subset2$taxsubgrp), decreasing = TRUE) ## ## ultic haploxeralfs mollic haploxeralfs ## 28 1 sort(table(subset2$taxpartsize), decreasing = TRUE) ## ## loamy-skeletal clayey-skeletal fine fine-loamy ## 25 2 1 1 # check year described and taxpartsize table(subset2$taxpartsize, substr(subset2$obsdate, 0, 4)) ## ## 2007 2008 2009 ## clayey-skeletal 1 0 1 ## fine 1 0 0 ## fine-loamy 1 0 0 ## loamy-skeletal 19 1 5 # a double equal sign &#39;==&#39; is used for exact character or numeric criteria subset3 &lt;- subset(subset2, taxpartsize == &#39;loamy-skeletal&#39;) table(subset3$taxpartsize) ## ## loamy-skeletal ## 25 par(mar = c(0, 0, 2, 1)) plotSPC(subset3[1:12, ], print.id = FALSE) title(&#39;Loamy-skeletal Ultic Haploxeralfs&#39;) 2.7.6 Dates and Times Dates and times use special object types in R. The Unix time, also known as “Posix time,” is a system for describing a point in time. Unix epoch is a whole number value that is the number of seconds elapsed since the 00:00:00 UTC on 1 January 1970 minus leap seconds. We can use logical comparison operators on dates and times if their string representation such as \"01/01/1970\" is converted to a common base R UNIX time representation known as POSIXlt or POSIXct. This conversion accounts for important things such as timezone using your computer’s locale–which is important to keep in mind. When converting to POSIX time several unambiguous (year-month-day) date time formats can be detected. For instance, if you want to convert a date in the common month-day-year format, you need to specify the format argument: as.POSIXct(24*60*60, origin = &quot;1970-01-01&quot;, tz = &quot;UTC&quot;) ## [1] &quot;1970-01-02 UTC&quot; By default the timezone will match your current timezone. Dates without times are treated as being at midnight UTC. You can customize the timezone with tz argument: as.POSIXlt(&quot;01/01/1970&quot;, tz = &quot;UTC&quot;, format = &quot;%m/%d/%Y&quot;) ## [1] &quot;1970-01-01 UTC&quot; POSIXlt and POSIXct objects can be formatted with the format() function. strptime() can be used to parse character vectors into date/times. You use as.POSIXlt() with character input, and as.POSIXct() with numeric input. R also has the Date class which can be used for formatting calendar dates. You can convert POSIXlt/POSIXct objects to Date objects with as.Date() 2.8 Exercise 2: O Horizon Thickness This exercise uses a synthetic transect generated with R code. The 10 hypothetical pedons along the transect have been described using the a standardized set of horizon designations: Oe, Oa, A, Cg1, Cg2. The thicknesses of layers, the depth to gleyed matrix, and soil organic carbon content vary between pedons. We will start to examine this dataset graphically, and then will utilize some functions defined for the SoilProfileCollection to extract information from the data. Save the code you use in an R script, add answers as comments, and send to your mentor. The aqp function thicknessOf() can be used to calculate the thickness of horizons matching a regular expression pattern. For example, to calculate the total O horizon thickness we can match the pattern \"O\": ohz_thickness &lt;- thicknessOf(ohz, pattern = &quot;O&quot;, prefix = &quot;O_&quot;) ohz_thickness ## id O_thickness ## 1 P-01 23 ## 2 P-02 40 ## 3 P-03 37 ## 4 P-04 42 ## 5 P-05 24 ## 6 P-06 45 ## 7 P-07 43 ## 8 P-08 33 ## 9 P-09 47 ## 10 P-10 42 We will use the site() LEFT JOIN method to merge the calculated thickness into the SoilProfileCollection. site(ohz) &lt;- ohz_thickness Now, let’s plot the profiles in order of total O horizon thickness. We draw dashed lines at 20 and 40cm to indicate the thresholds for histic epipedon and Histosols soil order: plotSPC(ohz, plot.order = order(ohz$O_thickness)) abline(h = c(20, 40), lty = 2) Using the object ohz object we just inspected, answer the following questions using techniques from the previous sections. Check your logic by comparing to the profile sketch. Check if any soil organic carbon values are NA. Make a profile sketch (plotSPC()) using organic carbon percentage (\"soc\") to color the profiles. Then calculate a new column using the function is.na() and use that to color the profiles. Which profiles have missing data? In which horizons? Tabulate unique horizon designations (ohz$name). How many of each horizon designation are present? Use regular expressions to count the number of Cg horizons in the collection. How many Cg horizons have organic carbon data in the \"soc\" column? Filter the ohz object using aqp::subset() to select pedons where O_thickness column is greater than 20 and less than 40. This is the histic epipedon thickness requirement. How many pedons have a histic epipedon? Filter the ohz object using aqp::subset() to select pedons where O_thickness column is greater than 40. This is the Histosols order thickness requirement. How many pedons are Histosols? 2.9 Generalized Horizon Labels After review of the data, a new set of more general labels can be assigned to each horizon to define groups over which soil properties will be aggregated. These are called “Generalized Horizon Labels” (GHL). These new labels define functionally similar groups that usually span multiple genetic horizons. Consider this situation: you have a collection of pedons that have been correlated to a named soil series or component. You would like to objectively compute a range in characteristics (“low-rv-high” values) and the typical range of horizon depths. Most collections of pedon data have variation in description style and horizons used, horizon depths, and number of horizons described. In the scenario depicted in the image above there are several obvious “micro-correlation” decisions that can be to group horizons for aggregation. For example: what horizon prototype scheme (e.g., A-Bt1-Bt2-Bt3-Cr-R) best conveys the concept of this soil series or soil component? Does it make sense to group the Bt3, Bt4, BCt, and CBt horizons for aggregation? What about grouping Bt3 and 2Bt3 horizons? Do BA and AB horizons occur frequently enough to be included as their own group? 2.9.1 Formal Approach Generalized horizon labels are an aggregation strategy. Here is a basic outline of the process: Select a set of GHL that best represents a group of pedons to be aggregated. This could be based on series descriptions, expert knowledge, or inspection of the most frequently described horizon designations. Assign GHL to each horizon using whatever information is available for grouping horizons. This micro-correlation of horizon designations will often require slightly different rules for each dataset and group. Careful inspection of horizon designation and observed properties is critical. Evaluate GHL assignments and manually refine as needed. Keep track of final GHL assignments in NASIS (dspcomplayerid) column, an R script, or a text file. Compute range in characteristics, aka low-rv-high values, for clay, sand, pH, etc. using the labels as grouping variables. Estimate most likely top and bottom depths for each GHL group. 2.9.2 Functions for Handling Generalized Horizon Labels The aqp package defines several functions that can be used to assign GHLs or summarize the data associated with them. The function generalize.hz() can be used to apply new labels based on regular expression pattern matching. The function GHL() can be used to get or set the column containing generalized horizon labels for a SoilProfileCollection. Exercise 4 in this chapter will let you work through the process of summarizing data using GHLs with your own data. But first we continue with tools to inspect your data retrieved from NASIS with soilDB. It is very important that basic QC/QA be performed on your data before proceeding with any statistical summaries. 2.10 fetchNASIS() data checks fetchNASIS() does a lot of the hard work for you by pulling out a variety of child tables and joining them to site/horizon tables. This results in a SoilProfileCollection object that has many of the NASIS data columns. A variety of internal processes are performed to get data that conforms with site or horizon tables. Output is generated in the console that you should always look at. We will walk through a few of the most common outputs. Quality control and limited “filling” of fragments, horizon depth data, site data, tables replacing missing lower horizon depths with top depth + 1cm ... [19 horizons] -&gt; QC: horizon errors detected: Use `get(&#39;bad.pedon.ids&#39;, envir=soilDB.env)` for pedon record IDs (peiid) Use `get(&#39;bad.horizons&#39;, envir=soilDB.env)` for horizon designations -&gt; QC: pedons missing bottom hz depths: Use `get(&#39;missing.bottom.depths&#39;, envir=soilDB.env)` for pedon record IDs (peiid) Notes about default settings and handling of NULL (missing data elements or records) NOTE: all records are missing surface fragment cover NOTE: all records are missing rock fragment volume 2.10.1 Inspecting Results We will analyze occurrence of andic soil properties in pedons from MT647. This is a demonstration of inspecting a NASIS dataset by splitting it based on a specific diagnostic feature. We will download a “selected set” from the course website from an .rda file to save you the effort of crafting your selected set just for this example. Downloading and installing the .rda is equivalent to NASIS query SSRO_Northwest: _PedonPC_Plus_DataDump_select for User Site ID: %MT647%, NASIS Site: SSRO_Northwest, and NASIS Group: NW-MIS Point Data. After populating your NASIS selected set mt647 is created in R with mt647 &lt;- fetchNASIS(rmHzErrors=TRUE) and mt647err with mt647err &lt;- fetchNASIS(rmHzErrors=FALSE). rmHzErrors=FALSE is the default behavior, we are creating two objects to demonstrate how to identify and fix common horizon errors. 2.10.1.1 Load Example Data To load the sample object data into R, just use load() and the path to the .rda file. In this case we are using a remote path, so we use url() to open the connection: load(url(&quot;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/book/02/mt647.rda&quot;)) 2.10.2 Select Pedons with Andic Soil Properties length(mt647) ## [1] 481 table(site(mt647)$andic.soil.properties, useNA = &quot;ifany&quot;) ## ## FALSE TRUE &lt;NA&gt; ## 2 83 396 # get just the profiles with andic.soil.properties == TRUE mt647.asp &lt;- subset(mt647, andic.soil.properties) mt647.asp ## SoilProfileCollection with 83 profiles and 446 horizons ## profile ID: peiid | horizon ID: phiid ## Depth range: 20 - 305 cm ## ## ----- Horizons (6 / 446 rows | 10 / 77 columns) ----- ## peiid phiid hzdept hzdepb hzname texture pedon_id dspcomplayerid bounddistinct boundtopo ## 828140 4005861 0 2 Oe &lt;NA&gt; P23-134 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828140 4005863 2 13 E CBV-L P23-134 &lt;NA&gt; clear smooth ## 828140 4005859 13 23 Bs1 CBV-L P23-134 &lt;NA&gt; clear smooth ## 828140 4005862 23 33 Bs2 CBV-L P23-134 &lt;NA&gt; gradual smooth ## 828140 4005864 33 69 BC GRX-FSL P23-134 &lt;NA&gt; gradual smooth ## 828140 4005860 69 152 2C GRX-FSL P23-134 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## [... more horizons ...] ## ## ----- Sites (6 / 83 rows | 10 / 132 columns) ----- ## siteiid peiid ecositeid ecositenm ecositecorrdate es_classifier siteecositehistory.classifier ## 845415 828140 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 845429 828152 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 845430 828153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 845432 828155 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 845434 828157 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 845451 828176 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## es_selection_method upedonid siteobsiid ## &lt;NA&gt; P23-134 821442 ## &lt;NA&gt; P92-052 821456 ## &lt;NA&gt; P92-046 821457 ## &lt;NA&gt; P93-053 821459 ## &lt;NA&gt; P91-103 821461 ## &lt;NA&gt; P89-011 821478 ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] length(mt647.asp) ## [1] 83 We can compare this to what we see in the NASIS Pedon Diagnostic Features table: 2.10.2.1 Using fetchNASIS() output to find errors Any profiles that have have logic errors detected are stored in soilDB.env bad.pedon.ids variable after you run fetchNASIS. If this variable does not exist either you have not run fetchNASIS() in the current session or there are no errors. # these are the troublesome user pedon IDs get(&#39;bad.pedon.ids&#39;, envir = soilDB.env) ## [1] &quot;P93-037&quot; &quot;P90-008&quot; &quot;P90-004&quot; &quot;P90-009&quot; &quot;P93-058&quot; &quot;P93-059&quot; &quot;P90-025&quot; &quot;P90-002&quot; &quot;P90-012&quot; &quot;P92-001&quot; ## [11] &quot;P92-085&quot; &quot;P90-019&quot; &quot;P90-010&quot; &quot;P90-015&quot; &quot;P91-094&quot; &quot;P92-029&quot; &quot;P92-076&quot; &quot;P93-026&quot; &quot;P93-035&quot; &quot;P93-063&quot; ## [21] &quot;P93-064&quot; &quot;P93-041&quot; &quot;P93-043&quot; &quot;P93-044&quot; &quot;P93-083&quot; &quot;P93-112&quot; &quot;P93-113&quot; &quot;P93-124&quot; &quot;P93-001&quot; &quot;P96-007&quot; ## [31] &quot;P91-025&quot; &quot;P93-078&quot; &quot;P92-044&quot; &quot;P91-112&quot; &quot;P92-038&quot; &quot;P90-018&quot; &quot;P93-057&quot; &quot;P93-084&quot; &quot;P90-016&quot; &quot;P92-063&quot; ## [41] &quot;P92-048&quot; &quot;P93-052&quot; &quot;F01-230&quot; &quot;F95-420&quot; &quot;F95-114&quot; &quot;F96-205&quot; &quot;P75-006&quot; &quot;P91-105&quot; &quot;P91-059&quot; # rmHzErrors=TRUE removes the &quot;bad&quot; illogical pedons any(mt647$upedonid %in% get(&#39;bad.pedon.ids&#39;, envir=soilDB.env)) ## [1] FALSE # mt647err is created with rmHzErrors=FALSE any(mt647err$upedonid %in% get(&#39;bad.pedon.ids&#39;, envir=soilDB.env)) ## [1] TRUE # mt647.asp is a subset of mt647, so no errors any(mt647.asp$upedonid %in% get(&#39;bad.pedon.ids&#39;, envir=soilDB.env)) ## [1] FALSE When fetchNASIS(..., rmHzErrors = TRUE) any horizons that were omitted from the SoilProfileCollection will be stored in the bad.horizons variable in the soilDB.env environment. head(get(&#39;bad.horizons&#39;, envir=soilDB.env)) ## peiid phiid upedonid hzname hzdept hzdepb ## 67 868038 4270406 F01-230 E NA NA ## 68 868038 4270407 F01-230 Bw NA NA ## 95 868072 4270514 F95-114 &lt;NA&gt; NA NA ## 99 868048 4270437 F95-420 &lt;NA&gt; NA NA ## 111 868074 4270519 F96-205 B NA NA ## 112 868074 4270521 F96-205 C NA NA 2.10.2.2 Logic Checks for the SoilProfileCollection The aqp package has several functions that do logic checks on SoilProfileCollection objects. The main method that does this in aqp is checkHzDepthLogic() which returns a data.frame of results of running four logic tests on the horizon data from each profile. Checks for: bottom depths less than top depth / bad top depth order (\"depthLogic\") bottom depths equal to top depth (\"sameDepth\") overlaps and gaps (\"overlapOrGap\") missing depths (\"missingDepth\") logic_tests &lt;- checkHzDepthLogic(mt647err) # look at first few (look OK; valid == TRUE) head(logic_tests) ## peiid valid depthLogic sameDepth missingDepth overlapOrGap ## 1 828138 TRUE FALSE FALSE FALSE FALSE ## 2 828139 TRUE FALSE FALSE FALSE FALSE ## 3 828140 TRUE FALSE FALSE FALSE FALSE ## 4 828141 TRUE FALSE FALSE FALSE FALSE ## 5 828142 TRUE FALSE FALSE FALSE FALSE ## 6 828143 TRUE FALSE FALSE FALSE FALSE # these all have overlapOrGap errors head(logic_tests[!logic_tests$valid, ]) ## peiid valid depthLogic sameDepth missingDepth overlapOrGap ## 11 828148 FALSE FALSE FALSE FALSE TRUE ## 23 828160 FALSE FALSE FALSE FALSE TRUE ## 25 828162 FALSE FALSE FALSE FALSE TRUE ## 34 828171 FALSE FALSE FALSE FALSE TRUE ## 35 828172 FALSE FALSE FALSE FALSE TRUE ## 36 828173 FALSE FALSE FALSE FALSE TRUE # join the logic test data into the site table site(mt647err) &lt;- logic_tests Use the $valid vector in result to select profiles with depth logic issues (logic_tests$valid == FALSE) bad.profiles &lt;- subset(mt647err, !valid) bad.profiles ## SoilProfileCollection with 49 profiles and 338 horizons ## profile ID: peiid | horizon ID: phiid ## Depth range: 15 - 152 cm ## ## ----- Horizons (6 / 338 rows | 10 / 77 columns) ----- ## peiid phiid hzdept hzdepb hzname texture pedon_id dspcomplayerid bounddistinct boundtopo ## 828148 4005908 0 3 Oe &lt;NA&gt; P93-037 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828148 4005907 3 5 Oi &lt;NA&gt; P93-037 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828148 4005905 5 15 E CB-FSL P93-037 &lt;NA&gt; clear smooth ## 828148 4005911 15 23 Bs CB-SIL P93-037 &lt;NA&gt; abrupt smooth ## 828148 4005909 23 25 2E CBV-FSL P93-037 &lt;NA&gt; clear smooth ## 828148 4005910 25 58 2B CBV-FSL P93-037 &lt;NA&gt; gradual smooth ## [... more horizons ...] ## ## ----- Sites (6 / 49 rows | 10 / 137 columns) ----- ## peiid siteiid ecositeid ecositenm ecositecorrdate es_classifier siteecositehistory.classifier ## 828148 845423 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828160 845435 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828162 845437 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828171 845446 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828172 845447 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828173 845448 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## es_selection_method upedonid siteobsiid ## &lt;NA&gt; P93-037 821450 ## &lt;NA&gt; P90-008 821462 ## &lt;NA&gt; P90-004 821464 ## &lt;NA&gt; P90-009 821473 ## &lt;NA&gt; P93-058 821474 ## &lt;NA&gt; P93-059 821475 ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] length(bad.profiles) ## [1] 49 Alternately, we can keep only the “valid” ones (where logic_tests$valid == TRUE): good.profiles &lt;- subset(mt647err, logic_tests$valid) good.profiles ## SoilProfileCollection with 481 profiles and 2536 horizons ## profile ID: peiid | horizon ID: phiid ## Depth range: 14 - 1552 cm ## ## ----- Horizons (6 / 2536 rows | 10 / 77 columns) ----- ## peiid phiid hzdept hzdepb hzname texture pedon_id dspcomplayerid bounddistinct boundtopo ## 828138 4005848 0 5 O &lt;NA&gt; P91-043 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828138 4005852 5 18 E GR-L P91-043 &lt;NA&gt; clear smooth ## 828138 4005850 18 38 Bw1 GRV-L P91-043 &lt;NA&gt; gradual wavy ## 828138 4005849 38 51 Bw2 CBV-FSL P91-043 &lt;NA&gt; gradual wavy ## 828138 4005853 51 71 BC CBV-SL P91-043 &lt;NA&gt; clear wavy ## 828138 4005851 71 81 R &lt;NA&gt; P91-043 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## [... more horizons ...] ## ## ----- Sites (6 / 481 rows | 10 / 137 columns) ----- ## peiid siteiid ecositeid ecositenm ecositecorrdate es_classifier siteecositehistory.classifier ## 828138 845413 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828139 845414 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828140 845415 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828141 845416 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828142 845417 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 828143 845418 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## es_selection_method upedonid siteobsiid ## &lt;NA&gt; P91-043 821440 ## &lt;NA&gt; P91-029 821441 ## &lt;NA&gt; P23-134 821442 ## &lt;NA&gt; P93-025 821443 ## &lt;NA&gt; P93-075 821444 ## &lt;NA&gt; P93-074 821445 ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] length(good.profiles) ## [1] 481 Once you have created a subset SoilProfileCollection, you can manually inspect the data in that set to identify areas where there are issues. Ideally you would fix these in NASIS. If you don’t have the ability to fix them in NASIS, you might write some R code into your analysis script to apply patches to missing or invalid data before proceeding with summaries. 2.11 Extended Data Functions Additional data related to both site and horizon information can be fetched using the get_extended_data_from_NASIS() function. This function returns a named list object with several tables that fetchNASIS() draws from. There are a variety of calculated fields that are included in the default fetchNASIS() result based on these extended data. Sometimes you need to work with a less aggregated representation of the source data to chase down data issues. 2.11.1 Elements of get_extended_data_from_NASIS() Ecological Site History (\"ecositehistory\") Diagnostic Features (\"diagnostic\") Diagnostic Feature TRUE/FALSE Summary (\"diagHzBoolean\") Restrictions (\"restriction\") Fragment and Texture Summaries Horizon Fragments (\"frag_summary\") Horizon Artifacts (\"art_summary\") Surface Fragments (\"surf_frag_summary\") Texture Class Modifiers (\"texmodifier\") Geomorphic Table Summaries (\"geomorph\") Parent Material Summaries (\"pm\") Taxonomic History (\"taxhistory\") Site Text Notes w/ Photo links(\"photo\") Horizon Structure (\"struct\") Horizon Designation (\"hzdesgn\") 2.11.2 Load Example Data Below is a summary of additional information that can be readily brought into R from your NASIS selected set via the get_extended_data_from_NASIS() function. Before continuing, imagine opening the NASIS client, populating your selected set with 2015MT663% using a query like “NSSC Pangaea – POINT-Pedon/Site by User Pedon ID” Load the data like we did above. load(url(&quot;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/book/02/mt663.rda&quot;)) ## fetch extended site and horizon data from local NASIS # mt663 &lt;- fetchNASIS() # mt663ext &lt;- get_extended_data_from_NASIS_db() We could use the get_extended_data_from_NASIS_db() function if 2015MT663% or other data were in the selected set, but we will use the mt663ext data we loaded from the .rda file. The column names are the names of variables that you could join to your site or horizon data by various means. Generally these variable names, with a few exceptions, mirror the NASIS 7 data model names. # site and pedon related extended data # list all dataframes in the extended data str(mt663ext, 1) ## List of 14 ## $ ecositehistory:&#39;data.frame&#39;: 0 obs. of 7 variables: ## $ siteaoverlap :&#39;data.frame&#39;: 127 obs. of 5 variables: ## $ diagnostic :&#39;data.frame&#39;: 292 obs. of 4 variables: ## $ diagHzBoolean :&#39;data.frame&#39;: 115 obs. of 20 variables: ## $ restriction :&#39;data.frame&#39;: 11 obs. of 8 variables: ## $ frag_summary :&#39;data.frame&#39;: 561 obs. of 18 variables: ## $ art_summary :&#39;data.frame&#39;: 561 obs. of 14 variables: ## $ texmodifier :&#39;data.frame&#39;: 622 obs. of 5 variables: ## $ geomorph :&#39;data.frame&#39;: 241 obs. of 9 variables: ## $ taxhistory :&#39;data.frame&#39;: 115 obs. of 23 variables: ## $ photo :&#39;data.frame&#39;: 793 obs. of 4 variables: ## $ pm :&#39;data.frame&#39;: 179 obs. of 9 variables: ## $ struct :&#39;data.frame&#39;: 444 obs. of 6 variables: ## $ hzdesgn :&#39;data.frame&#39;: 561 obs. of 19 variables: # vegetation data summary colnames(mt663ext$ecositehistory) ## [1] &quot;siteiid&quot; &quot;ecositeid&quot; &quot;ecositenm&quot; ## [4] &quot;ecositecorrdate&quot; &quot;es_classifier&quot; &quot;siteecositehistory.classifier&quot; ## [7] &quot;es_selection_method&quot; # diagnostic features colnames(mt663ext$diagnostic) ## [1] &quot;peiid&quot; &quot;featkind&quot; &quot;featdept&quot; &quot;featdepb&quot; # surface rock fragments colnames(mt663ext$surf_frag_summary) ## NULL # geomorphic description colnames(mt663ext$geomorph) ## [1] &quot;peiid&quot; &quot;geomicrorelief&quot; &quot;geommicelev&quot; &quot;geomfmod&quot; &quot;geomfname&quot; &quot;geomfeatid&quot; ## [7] &quot;existsonfeat&quot; &quot;geomfiidref&quot; &quot;geomftname&quot; # taxonomic history data colnames(mt663ext$taxhistory) ## [1] &quot;peiid&quot; &quot;classdate&quot; &quot;classifier&quot; &quot;classtype&quot; &quot;taxonname&quot; ## [6] &quot;localphase&quot; &quot;taxonkind&quot; &quot;seriesstatus&quot; &quot;taxclname&quot; &quot;taxpartsize&quot; ## [11] &quot;taxorder&quot; &quot;taxsuborder&quot; &quot;taxgrtgroup&quot; &quot;taxsubgrp&quot; &quot;soiltaxedition&quot; ## [16] &quot;osdtypelocflag&quot; &quot;taxmoistcl&quot; &quot;taxtempregime&quot; &quot;taxfamother&quot; &quot;taxreaction&quot; ## [21] &quot;taxfamhahatmatcl&quot; &quot;psctopdepth&quot; &quot;pscbotdepth&quot; # linked photo stored in site textnotes colnames(mt663ext$photo) ## [1] &quot;siteiid&quot; &quot;recdate&quot; &quot;textcat&quot; &quot;imagepath&quot; # site parent materials colnames(mt663ext$pm) ## [1] &quot;siteiid&quot; &quot;seqnum&quot; &quot;pmorder&quot; &quot;pmdept&quot; &quot;pmdepb&quot; &quot;pmmodifier&quot; &quot;pmgenmod&quot; &quot;pmkind&quot; ## [9] &quot;pmorigin&quot; ### ### horizon related extended data ### # rock fragments colnames(mt663ext$frag_summary) ## [1] &quot;phiid&quot; &quot;fine_gravel&quot; &quot;gravel&quot; &quot;cobbles&quot; ## [5] &quot;stones&quot; &quot;boulders&quot; &quot;channers&quot; &quot;flagstones&quot; ## [9] &quot;parafine_gravel&quot; &quot;paragravel&quot; &quot;paracobbles&quot; &quot;parastones&quot; ## [13] &quot;paraboulders&quot; &quot;parachanners&quot; &quot;paraflagstones&quot; &quot;unspecified&quot; ## [17] &quot;total_frags_pct_nopf&quot; &quot;total_frags_pct&quot; # soil texture modifers colnames(mt663ext$texmodifier) ## [1] &quot;peiid&quot; &quot;phiid&quot; &quot;phtiid&quot; &quot;seqnum&quot; &quot;texmod&quot; # soil structure data colnames(mt663ext$struct) ## [1] &quot;phiid&quot; &quot;structgrade&quot; &quot;structsize&quot; &quot;structtype&quot; &quot;structid&quot; &quot;structpartsto&quot; 2.11.3 Visualizing Common Landforms The following code generates a simple graphical summary of the 10 most commonly occurring \"landform_string\" (a calculated field in fetchNASIS()) to inspect which are the most common. # load data from a NASIS selected set (or sample object) pedons &lt;- mt663 # create &#39;lf&#39; object of landform factors sorted in descending order lf &lt;- sort(table(pedons$landform_string), decreasing = TRUE) # plot top 10 or length, whichever is shorter Hmisc::dotchart2(lf[1:pmin(10, length(lf))], col = &#39;black&#39;, xlim = c(0, max(lf)), cex.labels = 0.75) For a challenge and to further inspect your own data try the above code with some other summaries of geomorphic data produced by fetchNASIS(). You can swap landform_string for: landscape_string (landscape), hillslopeprof (2D), geomposmntn, geomposhill, geompostrce, geomposflats (3D), slope_shape, shapeacross, shapedown (slope shape across/down), microfeature_string (microfeature), or geomicrorelief_string (site observation microrelief). 2.11.4 Diagnostic Features 2.11.4.1 Boolean Diagnostic Features in fetchNASIS() If diagnostic features are populated in the Pedon Diagnostic Features table in NASIS, then Boolean (TRUE or FALSE, or “logical”) fields are created for each diagnostic feature type found in the data brought in by fetchNASIS. These fields can be used to model presence / absence of a diagnostic soil feature by extracting the site data from the SoilProfileCollection with site(). 2.11.4.2 Thickness from Diagnostic Features Table The following is an example of how you could use the diagnostic features (if populated!) from the extended data to determine the thickness of a diagnostic feature of interest. # get diagnostic features associated with pedons loaded from selected set d &lt;- diagnostic_hz(mt663) # summary of the diagnostic features in your data! unique(d$featkind) ## [1] &quot;ochric epipedon&quot; &quot;cambic horizon&quot; &quot;lithic contact&quot; ## [4] &quot;mollic epipedon&quot; &quot;argillic horizon&quot; &quot;redox concentrations&quot; ## [7] &quot;andic soil properties&quot; &quot;secondary carbonates&quot; &quot;sapric soil materials&quot; ## [10] &quot;aquic conditions&quot; &quot;reduced matrix&quot; &quot;albic horizon&quot; ## [13] &quot;spodic horizon&quot; &quot;glossic horizon&quot; &quot;spodic materials&quot; ## [16] &quot;lithologic discontinuity&quot; &quot;densic materials&quot; &quot;umbric epipedon&quot; ## [19] &quot;albic materials&quot; NA # tabulate sort(table(droplevels(factor(d$featkind))), decreasing = TRUE) ## ## ochric epipedon cambic horizon argillic horizon mollic epipedon ## 61 54 43 42 ## andic soil properties lithic contact secondary carbonates umbric epipedon ## 30 20 7 7 ## albic horizon spodic horizon glossic horizon reduced matrix ## 6 4 3 3 ## sapric soil materials lithologic discontinuity albic materials aquic conditions ## 3 2 1 1 ## densic materials redox concentrations spodic materials ## 1 1 1 # subset argillic horizons d &lt;- d[d$featkind == &#39;argillic horizon&#39;, ] # create a new column and subtract the upper from the lower depth d$argillic_thickness_cm &lt;- d$featdepb - d$featdept # create another new column with the upper depth to the diagnostic feature d$depth_to_argillic_cm &lt;- d$featdept # omit NA values d &lt;- na.omit(d) # subset to pedon records IDs and calculated thickness d &lt;- d[, c(&#39;peiid&#39;, &#39;argillic_thickness_cm&#39;, &#39;depth_to_argillic_cm&#39;)] head(d) ## peiid argillic_thickness_cm depth_to_argillic_cm ## 7 1092610 56 30 ## 24 1092617 38 34 ## 26 1092618 29 23 ## 28 1092619 38 32 ## 30 1092620 29 24 ## 33 1092621 23 19 # left-join with existing site data site(mt663) &lt;- d # plot as histogram par(mar = c(4.5, 4.5, 1, 1)) # note additional arguments to adjust figure labels hist( mt663$argillic_thickness_cm, xlab = &#39;Thickness of argillic (cm)&#39;, main = &#39;&#39;, las = 1 ) hist( mt663$depth_to_argillic_cm, xlab = &#39;Depth to argillic top depth (cm)&#39;, main = &#39;&#39;, las = 1 ) Quick check: What can you do with the boolean diagnostic feature data stored in the site table of a fetchNASIS() SoilProfileCollection? 2.11.4.3 Diagnostic Feature Diagrams # work up diagnostic plot based on the mt663 dataset loaded above library(aqp) library(soilDB) library(sharpshootR) # can limit which diagnostic features to show by setting &#39;v&#39; manually v &lt;- c(&#39;ochric.epipedon&#39;, &#39;mollic.epipedon&#39;, &#39;andic.soil.properties&#39;, &#39;argillic.horizon&#39;, &#39;cambic.horizon&#39;, &#39;lithic.contact&#39;) # the default concatenated landform_string may have multiple levels # depending on how the geomorphic tables were populated # these are concatenated using the ampersand (&amp;) character # so take the first string split using ampersand as a delimiter mt663$first_landform &lt;- sapply(strsplit(mt663$landform_string, &quot;&amp;&quot;), function(x) x[[1]]) # plot with diagnostic features ordered according to co-occurrence # v: site-level attributes to consider # k: number of clusters to identify diagnosticPropertyPlot( mt663[1:30, ], v = v, k = 5, grid.label = &#39;usiteid&#39;, dend.label = &#39;first_landform&#39;, sort.vars = TRUE ) 2.12 Exercise 3: Diagnostic Horizons in Your Own Data Save the code you use in an R script, add answers as comments, and send to your mentor. Use the following script to generate a diagnostic-feature diagram for the pedon data you’ve loaded from your NASIS selected set. Alternately, you may use the MT663 data from the example above, just substitute the object mt663 for f. You can select a subset of desired diagnostic properties or use all diagnostic feature columns. library(aqp) library(soilDB) library(sharpshootR) # Load data f &lt;- fetchNASIS(from = &#39;pedons&#39;) # ... May need to use subset() to reduce the number of pedons! # get all diagnostic feature columns from site data # by pattern matching on &#39;[.]&#39; in the site attribute names # this is not a generic rule, but works here idx &lt;- grep(&#39;[.]&#39;, siteNames(f)) v &lt;- siteNames(f)[idx] # inspect v v # insert diagnostics of interest from the possible list in &#39;v&#39; v &lt;- c(&#39;ochric.epipedon&#39;, &#39;cambic.horizon&#39;, &#39;argillic.horizon&#39;, &#39;paralithic.contact&#39;, &#39;lithic.contact&#39;) # generate diagnostic property diagram diagnosticPropertyPlot( f, v = v, k = 5, grid.label = &#39;usiteid&#39;, dend.label = &#39;taxonname&#39; ) Questions: Use the results of site(), horizons(), diagnostic_hz() to answer the following questions about your own data. These functions return data.frame objects from a SoilProfileCollection derived from your local NASIS data. How many pedons do you have in your selected set? Check the number of rows in the site portion of the SoilProfileCollection with nrow(), or use the method length() on the collection. How many diagnostic features do you have for those pedons? How many different \"featkind\"? You can calculate the number of unique values with with length() and unique() Interpret: What diagnostic features did you choose to plot? Did you notice any patterns in the diagnostic features that tend to occur together? For more information on generating diagnostic feature diagrams, see the following tutorial: Diagnostic Feature Property Plots. 2.13 Custom Queries to Local NASIS Database fetchNASIS() and related convenience functions are wrappers around commonly used chunks of SQL (Structured Query Language). Queries of the NASIS local database can be written in T-SQL which is the dialect of SQL used to communicate with Microsoft SQL Server. This is the connection that you set for the pre-course. To create a DBIConnection object that can access the NASIS local database, use dbConnectNASIS() or simply NASIS(). This will look for an ODBC connection of the name \"nasis_local\" and will authenticate with the read-only credentials. You can also pass an existing DBIConnection or path to SQLite file as the dsn argument to connect to an alternate source. The default driver type used for NASIS connections is OdbcConnection from the odbc package. To query or execute SQL commands on a connection you created, you can use dbGetQuery(connection, query), where connection is your DBIConnection object and query is the SQL to execute. The soilDB package also includes a helper function dbQueryNASIS(), which functions just like dbGetQuery() except the default is to close the connection after returning the result. This can be convenient for interactive use or cases where a function needs to only execute a single query, as it will prevent you from leaving connections open accidentally. 2.13.1 Example: Site Soil Temperature Data from CA792 (Sequoia-Kings Canyon National Park) The following example will return all records in your selected set sitesoiltemp table, along with a couple of fields from the site, siteobs, and pedon tables. This is a convenient way to collect all of the field-based soil temperature data associated with the pedons in your selected set for further analysis. You can use the CA792 (Sequoia-Kings Canyon National Park) pedons as an example. Use a query that searches user pedon ID for the following pattern %CA792% to download and populate a selected set in the NASIS client. library(soilDB) # write query as a character string q &lt;- &quot;SELECT siteiid as siteiid, peiid, usiteid, obsdate, soitemp, soitempdep FROM site_View_1 INNER JOIN siteobs_View_1 ON site_View_1.siteiid = siteobs_View_1.siteiidref LEFT OUTER JOIN sitesoiltemp_View_1 ON siteobs_View_1.siteobsiid = sitesoiltemp_View_1.siteobsiidref LEFT OUTER JOIN pedon_View_1 ON siteobs_View_1.siteobsiid = pedon_View_1.siteobsiidref ORDER BY obsdate, siteiid;&quot; # setup connection to local NASIS channel &lt;- NASIS() # exec query d &lt;- dbQueryNASIS(channel, q) The functions dbConnectNASIS() (alias NASIS()) and dbQueryNASIS() allow you to create a connection to the NASIS local database and send queries to that connection, respectively. By default, dbQueryNASIS() will close your connection after completing the query; you can change this by setting close=FALSE. # check results str(d) # remove records missing values d &lt;- na.omit(d) # tabulate unique soil depths table(d$soitempdep) # extract doy of year d$doy &lt;- as.integer(format(d$obsdate, &quot;%j&quot;)) # when where measurements collected? hist( d$doy, xlim = c(1, 366), las = 1, main = &#39;Soil Temperature Measurements&#39;, xlab = &#39;Day of Year&#39; ) # soil temperature by day of year plot( soitemp ~ doy, data = d, main = &#39;Soil Temperature Measurements (CA792)\\nNASIS &quot;Site Soil Temperature&quot; table&#39;, type = &#39;p&#39;, pch = 21, bg = &#39;royalblue&#39;, xlim = c(1, 366), ylim = c(-1, 26), xlab = &#39;Day of Year&#39;, ylab = &#39;Soil Temperature at 50cm (deg C)&#39;, las = 1 ) # vernal equinox, summer solstice, autumnal equinox, winter solstice x &lt;- as.Date(c(&#39;2022-03-20&#39;, &#39;2022-06-21&#39;, &#39;2022-09-23&#39;, &#39;2022-12-21&#39;)) # convert dates -&gt; Julian date, or day-of-year doy &lt;- as.integer(format(x, &quot;%j&quot;)) # add vertical lines abline(v = doy, lty = 3, col = grey(0.45)) # annotate # pos argument: left-center offset text( x = doy, y = -1, labels = c(&#39;vernal equinox&#39;, &#39;summer solstice&#39;, &#39;autumnal equinox&#39;, &#39;winter solstice&#39;), pos = 2, cex = 0.75, font = 3 ) # box/whisker plot showing distribution of day-of-year # when measurements (pedon descriptions) were collected boxplot(d$doy, at = 26, horizontal = TRUE, add = TRUE, lty = 1, width = 1, col = &#39;white&#39;, axes = FALSE) + 2.14 Exercise 4: Generalized Horizons with Loafercreek Here we will introduce the concept of using regular expressions to apply “Generalized Horizon Labels” based on the field soil description horizon designations. This demonstrates one way of grouping horizon data for determining the Range in Characteristics of layers within a Soil Series or a SSURGO component. Generalized Horizon Labels with Loafercreek For the exercise during class we will give you some time to read over the materials and apply the process to the sample loafercreek data. The code to apply the process to loafercreek is given. Then we ask that you apply the process to your own data, adjusting your generalized horizon patterns as needed for your local soils. This may take more time than we have during class itself, so you should follow up as needed with your mentor to complete the exercise. Chapter 3 Exploratory Data Analysis will get deeper into some of the topics that are referenced in the loafercreek code, such as summary statistics on grouped data. You will send a table and profile plots to your mentor when you are done. Further discussion of generalized horizon labels to order profiles based on horizon-level properties can be found in this tutorial: Pair-Wise Distances by Generalized Horizon Labels tutorial "],["eda.html", "Chapter 3 Exploratory Data Analysis 3.1 Objectives (Exploratory Data Analysis) 3.2 Statistics 3.3 Data Inspection 3.4 Calculating Weights 3.5 Exercise 1: Fetch and Inspect Data 3.6 Descriptive Statistics 3.7 Exercise 2: Compute Descriptive Statistics 3.8 Graphical Methods 3.9 Exercise 3: Graphical Methods 3.10 Transformations 3.11 The Shiny Package 3.12 Exercise 4: Using the North Central Region Web App 3.13 soilReports 3.14 Exercise 5: Run Lab Summary By Taxon Name Soil Report 3.15 Exercise 6: Run Mapunit Comparison 3.16 Exercise 7: Run Shiny Pedon Summary 3.17 Additional Reading (Exploratory Data Analysis)", " Chapter 3 Exploratory Data Analysis NASIS has an abundance of tabular data. These records capture a large amount of raw information about our soil series and map unit component concepts. In many cases, the data are not as clear to interpret or detailed as we would like. Before embarking on developing statistical models and generating predictions, it is essential to understand your data. This chapter will demonstrate ways to characterize a variety of different data types. John Tukey ((Tukey 1977)) advocated the practice of exploratory data analysis (EDA) as a critical part of the scientific process: “No catalog of techniques can convey a willingness to look for what can be seen, whether or not anticipated. Yet this is at the heart of exploratory data analysis. The graph paper and transparencies are there, not as a technique, but rather as a recognition that the picture examining eye is the best finder we have of the wholly unanticipated.” Fortunately, we can dispense with the graph paper and use software that makes it easy to develop graphical output and descriptive statistics for our data. 3.1 Objectives (Exploratory Data Analysis) This chapter will demonstrate the following: Methods for estimating Low, RV, and High values Methods for visualizing soil data Methods for data transformation Effects of weighting and missing values on summary statistics 3.2 Statistics Descriptive statistics include: Mean - arithmetic average Median - middle value Mode - most frequent value Standard Deviation - variation around the mean Interquartile Range - range encompasses 50% of the values Kurtosis - peakedness of the data distribution Skewness - symmetry of the data distribution Graphical methods include: Histogram - a bar plot where each bar represents the frequency of observations for a given range of values Density estimation - an estimation of the frequency distribution based on the sample data Quantile-quantile plot - a plot of the actual data values against a normal distribution Box plots - a visual representation of median, quartiles, symmetry, skewness, and outliers Scatter plots - a graphical display of one variable plotted on the x axis and another on the y axis Radial plots - plots formatted for the representation of circular data 3.3 Data Inspection Before you start an EDA, you should inspect your data and correct all typos and blatant errors. EDA can then be used to identify additional errors such as outliers and help you determine the appropriate statistical analyses. For this chapter we’ll use the loafercreek dataset from the CA630 Soil Survey Area. library(dplyr) # Load from the the loakercreek dataset data(&quot;loafercreek&quot;, package = &quot;soilDB&quot;) # Extract the horizon table h &lt;- aqp::horizons(loafercreek) # Construct generalized horizon designations n &lt;- c(&quot;A&quot;, &quot;BAt&quot;, &quot;Bt1&quot;, &quot;Bt2&quot;, &quot;Cr&quot;, &quot;R&quot;) # REGEX rules p &lt;- c(&quot;A&quot;, &quot;BA|AB&quot;, &quot;Bt|Bw&quot;, &quot;Bt3|Bt4|2B|C&quot;, &quot;Cr&quot;, &quot;R&quot;) # Compute genhz labels and add to loafercreek dataset h$genhz &lt;- aqp::generalize.hz(h$hzname, n, p) # Examine genhz vs hznames (wide format) table(h$genhz, h$hzname) ## ## 2BC 2BCt 2Bt1 2Bt2 2Bt3 2Bt4 2Bt5 2CB 2CBt 2Cr 2Crt 2R A A1 A2 AB ABt Ad Ap B BA BAt BC BCt ## A 0 0 0 0 0 0 0 0 0 0 0 0 97 7 7 0 0 1 1 0 0 0 0 0 ## BAt 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 31 8 0 0 ## Bt1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 ## Bt2 1 1 3 8 8 6 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 4 16 ## Cr 0 0 0 0 0 0 0 0 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ## R 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 ## not-used 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## ## Bt Bt1 Bt2 Bt3 Bt4 Bw Bw1 Bw2 Bw3 C CBt Cd Cr Cr/R Crt H1 Oi R Rt ## A 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## BAt 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Bt1 8 93 88 0 0 10 2 2 1 0 0 0 0 0 0 0 0 0 0 ## Bt2 0 0 0 47 8 0 0 0 0 6 6 1 0 0 0 0 0 0 0 ## Cr 0 0 0 0 0 0 0 0 0 0 0 0 49 0 20 0 0 0 0 ## R 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 40 1 ## not-used 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 24 0 0 # Examine matching pairs (long format) h %&gt;% group_by(genhz, hzname) %&gt;% count() ## # A tibble: 43 × 3 ## # Groups: genhz, hzname [43] ## genhz hzname n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 A A 97 ## 2 A A1 7 ## 3 A A2 7 ## 4 A Ad 1 ## 5 A Ap 1 ## 6 BAt AB 1 ## 7 BAt BA 31 ## 8 BAt BAt 8 ## 9 Bt1 ABt 2 ## 10 Bt1 Bt 8 ## # ℹ 33 more rows As noted in Chapter 1, a visual examination of the raw data is possible by clicking on the dataset in the environment tab, or via commandline: View(h) This view is fine for a small dataset, but can be cumbersome for larger ones. The summary() function can be used to quickly summarize a dataset however, even for our small example dataset, the output can be voluminous. Therefore in the interest of saving space we’ll only look at a sample of columns. h %&gt;% select(genhz, claytotest, total_frags_pct, phfield, effclass) %&gt;% summary() ## genhz claytotest total_frags_pct phfield effclass ## A :113 Min. :10.00 Min. : 0.00 Min. :4.90 Length:626 ## BAt : 40 1st Qu.:18.00 1st Qu.: 0.00 1st Qu.:6.00 Class :character ## Bt1 :206 Median :22.00 Median : 5.00 Median :6.30 Mode :character ## Bt2 :118 Mean :23.63 Mean :13.88 Mean :6.18 ## Cr : 75 3rd Qu.:28.00 3rd Qu.:20.00 3rd Qu.:6.50 ## R : 48 Max. :60.00 Max. :95.00 Max. :7.00 ## not-used: 26 NA&#39;s :167 NA&#39;s :381 The summary() function is known as a generic R function. It will return a summary for any R object. Because h is a data frame, we get a summary of each column. Factors will be summarized by their frequency (i.e., number of observations), while numeric or integer variables will print out a five number summary, and characters simply print their length. The number of missing observations for any variable will also be printed if they are present. If any of these metrics look unfamiliar to you, don’t worry we’ll cover them shortly. When you do have missing data and the function you want to run will not run with missing values, the following options are available: Exclude all rows or columns that contain missing values using the function na.exclude(), such as h2 &lt;- na.exclude(h). However this can be wasteful because it removes all rows (e.g., horizons), regardless if the row only has 1 missing value. Instead it’s sometimes best to create a temporary copy of the variable in question and then remove the missing variables, such as claytotest &lt;- na.exclude(h$claytotest). Replace missing values with another value, such as zero, a global constant, or the mean or median value for that column, such as h$claytotest &lt;- ifelse(is.na(h$claytotest), 0, h$claytotest) # or h[is.na(h$claytotest), ] &lt;- 0. Read the help file for the function you’re attempting to use. Many functions have additional arguments for dealing with missing values, such as na.rm. A quick check for typos would be to examine the list of levels for a factor or character, such as: # just for factors levels(h$genhz) ## [1] &quot;A&quot; &quot;BAt&quot; &quot;Bt1&quot; &quot;Bt2&quot; &quot;Cr&quot; &quot;R&quot; &quot;not-used&quot; # for characters and factors sort(unique(h$hzname)) ## [1] &quot;2BC&quot; &quot;2BCt&quot; &quot;2Bt1&quot; &quot;2Bt2&quot; &quot;2Bt3&quot; &quot;2Bt4&quot; &quot;2Bt5&quot; &quot;2CB&quot; &quot;2CBt&quot; &quot;2Cr&quot; &quot;2Crt&quot; &quot;2R&quot; &quot;A&quot; &quot;A1&quot; ## [15] &quot;A2&quot; &quot;AB&quot; &quot;ABt&quot; &quot;Ad&quot; &quot;Ap&quot; &quot;B&quot; &quot;BA&quot; &quot;BAt&quot; &quot;BC&quot; &quot;BCt&quot; &quot;Bt&quot; &quot;Bt1&quot; &quot;Bt2&quot; &quot;Bt3&quot; ## [29] &quot;Bt4&quot; &quot;Bw&quot; &quot;Bw1&quot; &quot;Bw2&quot; &quot;Bw3&quot; &quot;C&quot; &quot;CBt&quot; &quot;Cd&quot; &quot;Cr&quot; &quot;Cr/R&quot; &quot;Crt&quot; &quot;H1&quot; &quot;Oi&quot; &quot;R&quot; ## [43] &quot;Rt&quot; If the unique() function returned typos such as “BT” or “B t”, you could either fix your original dataset or you could make an adjustment in R, such as: h$hzname &lt;- ifelse(h$hzname == &quot;BT&quot;, &quot;Bt&quot;, h$hzname) Typo errors such as these are a common problem with old pedon data in NASIS. 3.4 Calculating Weights Before we can begin calculating statistics from our data we need to calculate their weight. A weight (w) is the number of observations a sample (n) represents within a population (N). w = N / n N = sum(w) Weights will either inflate or deflate the contribution of each observation to the final results. The more unbalanced the sample is from the population, the more substantial the weights impact will be. Inflation w = 100 / 10 = 10 each 1 sample represents 10 units within the population Diluation w = 10 / 30 = 0.33 each 1 samples represents 0.33 units within the population Commonly we use the weighted mean for depth-weighted averages in individual pedons or components and, also, for map unit averages. Depth-weighted averages commonly use horizon thicknesses as weights, while map unit averages commonly use component percentages (or component acres) as weights. When we want to scale the contribution of particular observations or variables to a final result, weights may be derived from expert knowledge or some other data source. The example below calculates both the horizon thickness and genhz weight for each pedon and horizon. Each of these measures can be used as weights # calculate thickness h &lt;- h %&gt;% mutate( thk = hzdepb - hzdept ) # calculate a genhz weight by pedon h &lt;- h %&gt;% group_by(peiid, genhz) %&gt;% reframe( phiid, genhz_wt = thk / sum(thk) ) %&gt;% select(phiid, genhz_wt) %&gt;% left_join(x = h, y = ., by = &quot;phiid&quot;) %&gt;% as.data.frame() 3.5 Exercise 1: Fetch and Inspect Data Save the code you use in an R script, add answers as comments, and send to your mentor. Exercises 2 and 3 will build on Exercise 1. Load the gopheridge dataset found within the soilDB package or use your own data. Inspect the gopheridge object. How many sites or profiles are in the collection? How many horizons? Apply the generalized horizon rules below or develop your own, see the following job-aid and the Pattern Matching section of Chapter 2 for details. # gopheridge rules n &lt;- c(&#39;A&#39;, &#39;Bt1&#39;, &#39;Bt2&#39;, &#39;Bt3&#39;,&#39;Cr&#39;,&#39;R&#39;) p &lt;- c(&#39;^A|BA$&#39;, &#39;Bt1|Bw&#39;,&#39;Bt$|Bt2&#39;, &#39;Bt3|CBt$|BCt&#39;, &#39;Cr&#39;, &#39;R&#39;) Summarize the hzdept, genhz, texture_class, sandtotest, and fine gravel columns using the summary() function. 3.6 Descriptive Statistics Table 3.1: Short Description of Summary Statistics and R Functions. Parameter NASIS Description R functions Mean RV ? arithmetic average mean() Median RV middle value, 50% quantile median() Mode RV most frequent value DescTools::Mode() Standard Deviation L &amp; H ? variation around mean sd() Quantiles L &amp; H percent rank of values, such that all values are &lt;= p quantile() 3.6.0.1 Other Weighted Statistics There are many other weighted statistics available. Essentially all will use a matrix of data values and a matrix of weights to scale the contribution of data values to the final statistic value. The package Hmisc defines several weighted variants of common summary statistics, notably Hmisc::wtd.quantile(). The DescTools package also defines Quantile() which strictly follows the Eurostat definition of a weighted quantile. 3.6.1 Measures of Central Tendency These measures are used to determine the mid-point of the range of observed values. In NASIS speak this should ideally be equivalent to the representative value (RV) for numeric and integer data. The mean and median are the most commonly used statistics for central tendency. 3.6.1.1 Mean Mean - is the arithmetic average all are familiar with, formally expressed as: \\(\\bar{x} =\\frac{\\sum_{i=1}^{n}x_i}{n}\\) which sums ( \\(\\sum\\) ) all the X values in the sample and divides by the number (n) of samples. It is assumed that all references in this document refer to samples rather than a population. The mean clay content from the loafercreek dataset may be determined: mean(h$claytotest, na.rm = TRUE) ## [1] 23.62767 3.6.1.2 Weighted Mean Weighted Mean - is the weighted arithmetic average. In the ordinary arithmetic average, each data point is assigned the same weight, whereas in a weighted average the contribution of each data point to the mean can vary. The function weighted.mean() takes two major inputs: x, a vector of data values, and w, a vector of weights. Like other statistics, you can remove missing values with na.rm=TRUE. # calculate horizon thickness h$hzthk &lt;- h$hzdepb - h$hzdept # weighted mean (across all horizons and profiles) weighted.mean(h$claytotest, h$hzthk, na.rm = TRUE) ## [1] 25.24812 The above example is applied to all profiles and all horizons. We only get one value across 106 profiles and 626 horizons. A more practical application of weighted.mean() is Particle Size Control Section (PSCS) weighted average properties. For this we need to use the horizon thickness within the control section as weights. The aqp function trunc() allows you to remove portions of profiles that are outside specified depth intervals (such as the PSCS boundaries). profileApply() allows for iterating over all profiles in a SoilProfileCollection to call some function on each one. Here we calculate the PSCS weighted mean clay content for each profile in loafercreek: # calculate a &quot;truncated&quot; SPC with just the PSCS; drop = FALSE retains empty profiles for missing PSCS loafercreek_pscs &lt;- trunc(loafercreek, loafercreek$psctopdepth, loafercreek$pscbotdepth, drop = FALSE) # calculate weighted mean for each profile, using the truncated horizon thicknesses as weights loafercreek$pscs_clay &lt;- profileApply(loafercreek_pscs, function(p) { weighted.mean(p$claytotest, w = p$hzdepb - p$hzdept, na.rm = TRUE) }) # inspect loafercreek$pscs_clay ## [1] 28.76471 28.76471 25.88000 NaN 16.00000 37.32000 32.63415 NaN 33.78571 29.57576 29.00000 ## [12] 23.02632 25.28000 23.36364 26.25000 34.12000 47.69565 30.73171 28.32609 32.00000 27.12500 27.76316 ## [23] 25.00000 28.60870 23.39535 22.32000 24.10000 20.91600 22.52000 20.48780 44.38000 37.88000 28.70000 ## [34] 28.80000 36.46000 23.78000 30.72000 24.33333 23.76000 21.78000 28.72727 20.83871 19.56000 25.48000 ## [45] 28.92000 33.95122 20.18000 18.86486 18.12500 25.56000 17.34000 25.84000 19.00000 28.37500 27.10870 ## [56] 24.72000 26.20370 25.13514 26.00000 24.38000 24.20000 26.24000 27.44444 29.90000 24.00000 42.08000 ## [67] 28.34000 29.36842 23.10000 24.56818 29.10000 24.16000 28.82000 NaN 18.77500 NaN 45.82917 ## [78] 25.25161 NaN 20.50000 21.28889 24.86000 32.34000 33.36585 31.04000 23.05405 17.22000 26.52000 ## [89] 29.22000 20.00000 24.60000 19.74000 18.25000 20.70000 31.84000 21.97959 32.44737 29.56000 NaN ## [100] 22.09412 21.34000 27.46154 26.68000 20.10526 31.00000 22.65000 Note that several of these pedons have NaN values in the result. This can happen even with na.rm=TRUE because pedons either do not have both psctopdepth and pscbotdepth populated in NASIS or all horizons in the PSCS have NA clay content. # inspect new site-level column pscs_clay head(site(loafercreek)[, c(&quot;peiid&quot;, &quot;upedonid&quot;, &quot;psctopdepth&quot;, &quot;pscbotdepth&quot;, &quot;pscs_clay&quot;)]) # plot just the pedons with missing pscs_clay plot(subset(loafercreek, is.na(pscs_clay)), color = &quot;claytotest&quot;) You always need to pay attention to the possibility of missing data or weights because missing values can dramatically the final result–sometimes in unexpected ways! 3.6.1.3 Median Median is the middle measurement of a sample set, and as such is a more robust estimate of central tendency than the mean. This is known as the middle or 50th quantile, meaning there are an equal number of samples with values less than and greater than the median. For example, assuming there are 21 samples, sorted in ascending order, the median would be the 11th sample. The median from the sample dataset may be determined: median(h$claytotest, na.rm = TRUE) ## [1] 22 3.6.1.4 Mode Mode - is the most frequent measurement in the sample. The use of mode is typically reserved for factors, which we will discuss shortly. One issue with using the mode for numeric data is that the data need to be rounded to the level of desired precision. R does not include a function for calculating the mode, but we can calculate it using the Mode() function from the DescTools package. h$claytotest %&gt;% round() %&gt;% DescTools::Mode(na.rm = TRUE) ## [1] 25 ## attr(,&quot;freq&quot;) ## [1] 42 3.6.1.5 Frequencies Frequencies To summarize factors and characters we can examine their frequency or number of observations. This is accomplished using the table() or summary() functions. table(h$genhz) ## ## A BAt Bt1 Bt2 Cr R not-used ## 113 40 206 118 75 48 26 # or summary(h$genhz) ## A BAt Bt1 Bt2 Cr R not-used ## 113 40 206 118 75 48 26 This gives us a count of the number of observations for each horizon. If we want to see the comparison between two different factors or characters, we can include two variables. table(h$genhz, h$texcl) ## ## c cl l scl sic sicl sil sl ## A 0 0 78 0 0 0 27 6 ## BAt 0 2 31 0 0 1 4 1 ## Bt1 2 44 127 4 1 5 20 1 ## Bt2 16 54 29 5 1 3 5 0 ## Cr 1 0 0 0 0 0 0 0 ## R 0 0 0 0 0 0 0 0 ## not-used 0 1 0 0 0 0 0 0 # or h %&gt;% count(genhz, texcl) %&gt;% head() ## genhz texcl n ## 1 A l 78 ## 2 A sil 27 ## 3 A sl 6 ## 4 A &lt;NA&gt; 2 ## 5 BAt cl 2 ## 6 BAt l 31 To calculate a weighted frequencies we need to use the xtabs() function, which uses a different syntax style, but can perform more complex tabulations. Notice how when the thickness is incorporated as a weight how it impacts the resulting frequencies. In the following example the results represent the total thickness of each texcl. xtabs(thk ~ texcl, data = h) ## texcl ## c cl l scl sic sicl sil sl ## 338 1948 3659 173 40 150 776 81 If instead we do a cross tabulation and weight by the genhz_wt we created early, the results represent the total number of pedons (i.e. peiid) that have the same genhz. tb &lt;- xtabs(genhz_wt ~ genhz + texcl, data = h, addNA = TRUE) %&gt;% round() tb ## texcl ## genhz c cl l scl sic sicl sil sl &lt;NA&gt; ## A 0 0 73 0 0 0 24 5 2 ## BAt 0 2 30 0 0 1 4 1 1 ## Bt1 1 25 61 2 1 3 10 1 1 ## Bt2 8 33 18 4 1 2 4 0 3 ## Cr 1 0 0 0 0 0 0 0 74 ## R 0 0 0 0 0 0 0 0 48 ## not-used 0 1 0 0 0 0 0 0 25 sum(tb) ## [1] 470 sum(h$genhz_wt) ## [1] 469 paste(h$peiid, h$genhz) %&gt;% unique() %&gt;% length() ## [1] 469 We can also add margin totals to tables or convert the table frequencies to proportions. # append the table with row and column sums addmargins(table(h$genhz, h$texcl)) ## ## c cl l scl sic sicl sil sl Sum ## A 0 0 78 0 0 0 27 6 111 ## BAt 0 2 31 0 0 1 4 1 39 ## Bt1 2 44 127 4 1 5 20 1 204 ## Bt2 16 54 29 5 1 3 5 0 113 ## Cr 1 0 0 0 0 0 0 0 1 ## R 0 0 0 0 0 0 0 0 0 ## not-used 0 1 0 0 0 0 0 0 1 ## Sum 19 101 265 9 2 9 56 8 469 # calculate the proportions relative to the rows, margin = 1 calculates for rows, margin = 2 calculates for columns, margin = NULL calculates for total observations table(h$genhz, h$texture_class) %&gt;% prop.table(margin = 1) %&gt;% round(2) * 100 ## ## br c cb cl gr l pg scl sic sicl sil sl spm ## A 0 0 0 0 0 70 0 0 0 0 24 5 0 ## BAt 0 0 0 5 0 79 0 0 0 3 10 3 0 ## Bt1 0 1 0 22 0 62 0 2 0 2 10 0 0 ## Bt2 0 14 1 46 2 25 1 4 1 3 4 0 0 ## Cr 97 2 0 0 2 0 0 0 0 0 0 0 0 ## R 100 0 0 0 0 0 0 0 0 0 0 0 0 ## not-used 0 0 0 4 0 0 0 0 0 0 0 0 96 To determine the mean by a group or category, use the group_by and summarize functions: h %&gt;% group_by(genhz) %&gt;% summarize( clay_avg = mean(claytotest, na.rm = TRUE), clay_med = median(claytotest, na.rm = TRUE) ) ## # A tibble: 7 × 3 ## genhz clay_avg clay_med ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 16.2 16 ## 2 BAt 19.5 19 ## 3 Bt1 24.0 24 ## 4 Bt2 31.2 30 ## 5 Cr 15 15 ## 6 R NaN NA ## 7 not-used NaN NA 3.6.2 Measures of Dispersion These are measures used to determine the spread of values around the mid-point. This is useful to determine if the samples are spread widely across the range of observations or concentrated near the mid-point. In NASIS speak these values might equate to the low (L) and high (H) values for numeric and integer data. 3.6.2.1 Variance Variance is a positive value indicating deviation from the mean: \\(s^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} {n - 1}\\) This is the square of the sum of the deviations from the mean, divided by the number of samples minus 1. It is commonly referred to as the sum of squares. As the deviation increases, the variance increases. Conversely, if there is no deviation, the variance will equal 0. As a squared value, variance is always positive. Variance is an important component for many statistical analyses including the most commonly referred to measure of dispersion, the standard deviation. Variance for the sample dataset is: var(h$claytotest, na.rm=TRUE) ## [1] 64.27607 3.6.2.2 Standard Deviation Standard Deviation is the square root of the variance: \\(s = \\sqrt\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} {n - 1}\\) The units of the standard deviation are the same as the units measured. From the formula you can see that the standard deviation is simply the square root of the variance. Standard deviation for the sample dataset is: sd(h$claytotest, na.rm = TRUE) ## [1] 8.017236 # or sqrt(var(h$claytotest, na.rm = TRUE)) ## [1] 8.017236 3.6.2.3 Coefficient of Variation Coefficient of Variation (CV) is a relative (i.e., unitless) measure of standard deviation: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\) CV is calculated by dividing the standard deviation by the mean and multiplying by 100. Since standard deviation varies in magnitude with the value of the mean, the CV is useful for comparing relative variation amongst different datasets. However Webster (2001) discourages using CV to compare different variables. Webster (2001) also stresses that CV is reserved for variables that have an absolute 0, like clay content. CV may be calculated for the sample dataset as: # remove NA values and create a new variable clay &lt;- na.exclude(h$claytotest) sd(clay) / mean(clay) * 100 ## [1] 33.93156 3.6.2.4 Quantiles (Percentiles) Quantiles (a.k.a. Percentiles) - the percentile is the value that cuts off the first nth percent of the data values when sorted in ascending order. The default for the quantile() function returns the min, 25th percentile, median or 50th percentile, 75th percentile, and max, known as the five number summary originally proposed by Tukey. Other probabilities however can be used. At present the 5th, 50th, and 95th are being proposed for determining the range in characteristics (RIC) for a given soil property. quantile(h$claytotest, na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 10 18 22 28 60 # or quantile(h$claytotest, probs = c(0.05, 0.5, 0.95), na.rm = TRUE) ## 5% 50% 95% ## 13.0 22.0 38.1 # or DescTools::Quantile(h$claytotest, weights = h$thk, na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 10 20 25 29 60 Thus, for the five number summary 25% of the observations fall between each of the intervals. Quantiles are a useful metric because they are largely unaffected by the distribution of the data, and have a simple interpretation. The National Soil Survey Handbook Part 618.2 specifies that new aggregate component data in NASIS should use quantiles for low, RV, and high values. For low and high the 5th, 10th, 20th and 80th, 90th, 95th can be used, respectively, according to which percentiles best capture the spread of a particular dataset. For newly populated information in NASIS, the RV is used to approximate the 50th percentile (median) of a dataset. 3.6.2.5 Range Range is the difference between the highest and lowest measurement of a group. Using a vector of sample data (clay) the range can be may be determined with: range(clay) ## [1] 10 60 which returns the minimum and maximum values observed. To get the width of the range you can do: diff(range(clay)) ## [1] 50 # or max(clay) - min(clay) ## [1] 50 3.6.2.6 Interquartile Range Interquartile Range (IQR) is the range from the upper (75%) quartile to the lower (25%) quartile. This represents 50% of the observations occurring in the mid-range of a sample. IQR is a robust measure of dispersion, unaffected by the distribution of data. In soil survey lingo you could consider the IQR to estimate the central concept of a soil property. IQR may be calculated for the sample dataset as: IQR(clay) ## [1] 10 # or diff(quantile(clay, p = c(0.25, 0.75))) ## 75% ## 10 3.6.3 Correlation A correlation matrix is a table of the calculated correlation coefficients of all variables. This provides a quantitative measure to guide the decision making process. The following will produce a correlation matrix for the sp4 dataset: h$hzdepm &lt;- (h$hzdepb + h$hzdept) / 2 # Compute the middle horizon depth h %&gt;% select(hzdepm, claytotest, sandtotest, total_frags_pct, phfield) %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% round(2) ## hzdepm claytotest sandtotest total_frags_pct phfield ## hzdepm 1.00 0.59 -0.08 0.50 -0.03 ## claytotest 0.59 1.00 -0.17 0.28 0.13 ## sandtotest -0.08 -0.17 1.00 -0.05 0.12 ## total_frags_pct 0.50 0.28 -0.05 1.00 -0.16 ## phfield -0.03 0.13 0.12 -0.16 1.00 Variables are perfectly correlated with themselves so they have a correlation coefficient of 1.0. Negative values indicate a negative relationship between variables. What is considered “highly correlated”? A good rule of thumb is anything with a value of 0.7 or greater is considered highly correlated. 3.7 Exercise 2: Compute Descriptive Statistics Add to your existing R script from Exercise 1, add answers as comments, and send to your mentor. Exercise 3 will build on Exercises 1 and 2. Aggregate by genhz and calculate several descriptive statistics for hzdept, gravel and phfield. Repeat step one, but include the genhz_wt as a weight in the statistics. How different are the results from step 1? Cross-tabulate geomposhill and argillic.horizon from the site table, as a percentage. Compute a correlation matrix between hzdept, gravel and phfield. 3.8 Graphical Methods Now that we’ve checked for missing values and typos and made corrections, we can graphically examine the sample data distribution of our data. Frequency distributions are useful because they can help us visualize the center (e.g., RV) and spread or dispersion (e.g., low and high) of our data. Typically in introductory statistics the normal (i.e., Gaussian) distribution is emphasized. Table 3.2: Short Description of Graphical Methods Plot Types Description Bar a plot where each bar represents the frequency of observations for a ‘group’ Histogram a plot where each bar represents the frequency of observations for a ‘given range of values’ Density an estimation of the frequency distribution based on the sample data Quantile-Quantile a plot of the actual data values against a normal distribution Box-Whisker a visual representation of median, quartiles, symmetry, skewness, and outliers Scatter &amp; Line a graphical display of one variable plotted on the x axis and another on the y axis Table 3.3: Comparison of R’s 3 Graphing Systems and their Equivalent Functions for Plotting Plot Types Base R lattice ggplot2 geoms Bar barplot() barchart() geom_bar() Histogram hist() histogram() geom_histogram() Density plot(density()) densityplot() geom_density() Quantile-Quantile qqnorm() qq() geom_qq() Box-Whisker boxplot() bwplot() geom_boxplot() Scatter &amp; Line plot() xyplot geom_point() The ggplot2 geoms allow for the incorporation of weights using the weight argument in the aes() function, where appropriate. 3.8.1 Distributions 3.8.2 Bar Plot A bar plot is a graphical display of the frequency (i.e. number of observations (count or n)), such as soil texture, that fall within a given class. It is a graphical alternative to to the table() function. library(ggplot2) # bar plot ggplot(h, aes(x = texcl, weight = thk)) + geom_bar() 3.8.3 Histogram A histogram is similar to a bar plot, except that instead of summarizing categorical data, it categorizes a continuous variable like clay content into non-overlappying intervals for the sake of display. The number of intervals can be specified by the user, or can be automatically determined using an algorithm, such as nclass.Sturges(). Since histograms are dependent on the number of bins, for small datasets they’re not the best method of determining the shape of a distribution. ggplot(h, aes(x = claytotest)) + geom_histogram(bins = nclass.Sturges(h$claytotest)) 3.8.4 Density Curve A density estimation, also known as a Kernel density plot, generally provides a better visualization of the shape of the distribution in comparison to the histogram. Compared to the histogram where the y-axis represents the number or percent (i.e., frequency) of observations, the y-axis for the density plot represents the probability of observing any given value, such that the area under the curve equals one. One curious feature of the density curve is the hint of two peaks (i.e. bimodal distribution?). Given that our sample includes a mixture of surface and subsurface horizons, we may have two different populations. However, considering how much the two distributions overlap, it seems impractical to separate them in this instance. ggplot(h, aes(x = claytotest)) + geom_density() 3.8.5 Box plots Box plots are a graphical representation of the five number summary, depicting quartiles (i.e. the 25%, 50%, and 75% quantiles), minimum, maximum and outliers (if present). Boxplots convey the shape of the data distribution, the presence of extreme values, and the ability to compare with other variables using the same scale, providing an excellent tool for screening data, determining thresholds for variables and developing working hypotheses. The parts of the boxplot are shown in the figure below. The “box” of the boxplot is defined as the 1st quartile and the 3rd quartile. The median, or 2nd quartile, is the dark line in the box. The whiskers (typically) show data that is 1.5 * IQR above and below the 3rd and 1st quartile. Any data point that is beyond a whisker is considered an outlier. That is not to say the outlier points are in error, just that they are extreme compared to the rest of the data set. However, you may want to evaluate these points to ensure that they are correct. ggplot(h, aes(x = genhz, y = claytotest)) + geom_boxplot() The above box plot shows a steady increase in clay content with depth. Notice the outliers in the box plots, identified by the individual circles. 3.8.6 Quantile comparison plots (QQplot) A QQ plot is a plot of the actual data values against a normal distribution (which has a mean of 0 and standard deviation of 1). # QQ Plot for Clay ggplot(h, aes(sample = claytotest)) + geom_qq() + geom_qq_line() # QQ Plot for Frags ggplot(h, aes(sample = total_frags_pct)) + geom_qq() + geom_qq_line() If the data set is perfectly symmetric (i.e. normal), the data points will form a straight line. Overall this plot shows that our clay example is more or less symmetric. However the second plot shows that our rock fragments are far from evenly distributed. A more detailed explanation of QQ plots may be found on Wikipedia: https://en.wikipedia.org/wiki/QQ_plot 3.8.7 The ‘Normal’ distribution What is a normal distribution and why should you care? Many statistical methods are based on the properties of a normal distribution. Applying certain methods to data that are not normally distributed can give misleading or incorrect results. Most methods that assume normality are robust enough for all data except the very abnormal. This section is not meant to be a recipe for decision making, but more an extension of tools available to help you examine your data and proceed accordingly. The impact of normality is most commonly seen for parameters used by pedologists for documenting the ranges of a variable (i.e., Low, RV and High values). Often a rule-of thumb similar to: “two standard deviations” is used to define the low and high values of a variable. This is fine if the data are normally distributed. However, if the data are skewed, using the standard deviation as a parameter does not provide useful information of the data distribution. The quantitative measures of Kurtosis (peakedness) and Skewness (symmetry) can be used to assist in accessing normality and can be found in the fBasics package, but Webster (2001) cautions against using significance tests for assessing normality. The preceding sections and chapters will demonstrate various methods to cope with alternative distributions. A Gaussian distribution is often referred to as “Bell Curve”, and has the following properties: Gaussian distributions are symmetric around their mean The mean, median, and mode of a Gaussian distribution are equal The area under the curve is equal to 1.0 Gaussian distributions are denser in the center and less dense in the tails Gaussian distributions are defined by two parameters, the mean and the standard deviation 68% of the area under the curve is within one standard deviation of the mean Approximately 95% of the area of a Gaussian distribution is within two standard deviations of the mean Viewing a histogram or density plot of your data provides a quick visual reference for determining normality. Distributions are typically normal, Bimodal or Skewed: Examples of different types of distributions Occasionally distributions are Uniform, or nearly so: With the loafercreek dataset the mean and median for clay were only slightly different, so we can safely assume that we have a normal distribution. However many soil variables often have a non-normal distribution. For example, let’s look at graphical examination of the mean vs. median for clay and rock fragments: The solid lines represent the breakpoint for the mean and standard deviations. The dashed lines represents the median and quantiles. The median is a more robust measure of central tendency compared to the mean. In order for the mean to be a useful measure, the data distribution must be approximately normal. The further the data departs from normality, the less meaningful the mean becomes. The median always represents the same thing independent of the data distribution, namely, 50% of the samples are below and 50% are above the median. The example for clay again indicates that distribution is approximately normal. However for rock fragments, we commonly see a long tailed distribution (e.g., skewed). Using the mean in this instance would overestimate the rock fragments. Although in this instance the difference between the mean and median is only 9 percent. 3.8.8 Scatterplots and Line Plots Plotting points of one ratio or interval variable against another is a scatter plot. Plots can be produced for a single or multiple pairs of variables. Many independent variables are often under consideration in soil survey work. This is especially common when GIS is used, which offers the potential to correlate soil attributes with a large variety of raster datasets. The purpose of a scatterplot is to see how one variable relates to another. With modeling in general the goal is parsimony (i.e., simple). The goal is to determine the fewest number of variables required to explain or describe a relationship. If two variables explain the same thing, i.e., they are highly correlated, only one variable is needed. The scatterplot provides a perfect visual reference for this. Create a basic scatter plot using the loafercreek dataset. # scatter plot ggplot(h, aes(x = claytotest, y = hzdepm)) + geom_point() + ylim(100, 0) # line plot ggplot(h, aes(y = claytotest, x = hzdepm, group = peiid)) + geom_line() + coord_flip() + xlim(100, 0) This plots clay on the X axis and depth on the X axis. As shown in the scatterplot above, there is a moderate correlation between these variables. The function below produces a scatterplot matrix for all the numeric variables in the dataset. This is a good command to use for determining rough linear correlations for continuous variables. library(GGally) h %&gt;% select(hzdepm, claytotest, phfield, total_frags_pct) %&gt;% ggpairs() 3.8.9 3rd Dimension - Color, Shape, Size, Layers, etc… 3.8.9.1 Color and Groups # scatter plot ggplot(h, aes(x = claytotest, y = hzdepm, color = genhz)) + geom_point(size = 3) + ylim(100, 0) # density plot ggplot(h, aes(x = claytotest, color = genhz)) + geom_density(size = 2) # bar plot ggplot(h, aes(x = genhz, fill = texture_class)) + geom_bar() # box plot ggplot(h, aes(x = genhz, y = claytotest)) + geom_boxplot() # heat map (pseudo bar plot) s &lt;- aqp::site(loafercreek) ggplot(s, aes(x = landform_string, y = pmkind)) + geom_tile(alpha = 0.2) 3.8.9.2 Facets - box plots library(tidyr) # convert to long format df &lt;- h %&gt;% select(peiid, genhz, hzdepm, claytotest, phfield, total_frags_pct) %&gt;% pivot_longer(cols = c(&quot;claytotest&quot;, &quot;phfield&quot;, &quot;total_frags_pct&quot;)) ggplot(df, aes(x = genhz, y = value)) + geom_boxplot() + xlab(&quot;genhz&quot;) + facet_wrap(~ name, scales = &quot;free_y&quot;) 3.8.9.3 Facets - depth plots data(loafercreek, package = &quot;soilDB&quot;) s &lt;- aqp::slab(loafercreek, fm = ~ claytotest + phfield + total_frags_pct, slab.structure = 0:100, slab.fun = function(x) quantile(x, c(0.1, 0.5, 0.9), na.rm = TRUE)) ggplot(s, aes(x = top, y = X50.)) + # plot median geom_line() + # plot 10th &amp; 90th quantiles geom_ribbon(aes(ymin = X10., ymax = X90., x = top), alpha = 0.2) + # invert depths xlim(c(100, 0)) + # flip axis coord_flip() + facet_wrap(~ variable, scales = &quot;free_x&quot;) 3.9 Exercise 3: Graphical Methods Add to your existing R script from Exercise 2, add answers as comments, and send to your mentor. Create a faceted boxplot of genhz vs gravel and phfield, and use genhz as a weight in the aes() function. Create a faceted depth plot for gravel and phfield. Why is it not necessary to include a weight in the faceted depth plot? 3.10 Transformations Slope aspect and pH are two common variables warranting special consideration. 3.10.1 pH There is debate about the best way to average pH since is it a log transformed variable. Remember, pHs of 6 and 5 correspond to hydrogen ion concentrations of 0.000001 and 0.00001 respectively. The actual average is 5.26; -log10((0.000001 + 0.00001) / 2). If no conversions are made for pH, the mean and sd in the summary are considered the geometric mean and sd, not the arithmetic. The wider the pH range, the greater the difference between the geometric and arithmetic mean. The difference between the arithmetic average of 5.26 and the geometric average of 5.5 is small. Boyd, Tucker, and Viriyatum (2011) examined the issue in detail, and suggests that regardless of the method used, the choice should be documented. If you have a table with pH values and wish to calculate the arithmetic mean using R, this example will work: # arithmetic mean log10(mean(1 / 10^-h$phfield, na.rm = TRUE)) ## [1] 6.371026 # geometric mean mean(h$phfield, na.rm = TRUE) ## [1] 6.18 3.10.2 Circular data Slope aspect - requires the use of circular statistics for numerical summaries, or graphical interpretation using circular plots. For example, if soil map units being summarized have a uniform distribution of slope aspects ranging from 335 degrees to 25 degrees, the Zonal Statistics tool in ArcGIS would return a mean of 180. The most intuitive means available for evaluating and describing slope aspect are circular plots available with the circular package in R and the radial plot option in the TEUI Toolkit. The circular package in R will also calculate circular statistics like mean, median, quartiles etc. library(circular) # Extract the site table s &lt;- aqp::site(loafercreek) aspect &lt;- s$aspect_field aspect &lt;- circular(aspect, template=&quot;geographic&quot;, units=&quot;degrees&quot;, modulo=&quot;2pi&quot;) summary(aspect) ## n Min. 1st Qu. Median Mean 3rd Qu. Max. Rho NA&#39;s ## 101.0000 12.0000 255.0000 195.0000 199.5000 115.0000 20.0000 0.1772 5.0000 The numeric output is fine, but a following graphic is more revealing, which shows the dominant Southwest slope aspect. rose.diag(aspect, bins = 8, col=&quot;grey&quot;) 3.10.3 Texture Class and Fine-earth Separates Many pedon descriptions include soil texture class and modifiers, but lack numeric estimates such as clay content and rock fragments percentage. This lack of “continuous” numeric data makes it difficult to analyze and estimate certain properties precisely. While numeric data on textural separates may be missing, it can still be estimated by the class ranges and averages. NASIS has many calculations used to estimate missing values. To facilitate this process in R, several new functions have recently been added to the aqp package. These new aqp functions are intended to impute missing values or check existing values. The ssc_to_texcl() function uses the same logic as the particle size estimator calculation in NASIS to classify sand and clay into texture class. The results are stored in data(soiltexture) and used by texcl_to_ssc() as a lookup table to convert texture class to sand, silt and clay. The function texcl_to_ssc() replicates the functionality described by Levi (2017). Unlike the other functions, texture_to_taxpartsize() is intended to be computed on weighted averages within the family particle size control section. Below is a demonstration of these new aqp R functions. library(aqp) library(soiltexture) # example of texcl_to_ssc(texcl) texcl &lt;- c(&quot;s&quot;, &quot;ls&quot;, &quot;l&quot;, &quot;cl&quot;, &quot;c&quot;) test &lt;- texcl_to_ssc(texcl) head(cbind(texcl, test)) # example of ssc_to_texcl() ssc &lt;- data.frame( CLAY = c(55, 33, 18, 6, 3), SAND = c(20, 33, 42, 82, 93), SILT = c(25, 34, 40, 12, 4) ) texcl &lt;- ssc_to_texcl(sand = ssc$SAND, clay = ssc$CLAY) ssc_texcl &lt;- cbind(ssc, texcl) head(ssc_texcl) # plot on a textural triangle TT.plot( class.sys = &quot;USDA-NCSS.TT&quot;, tri.data = ssc_texcl, pch = 16, col = &quot;blue&quot; ) # example of texmod_to_fragvoltol() frags &lt;- c(&quot;gr&quot;, &quot;grv&quot;, &quot;grx&quot;, &quot;pgr&quot;, &quot;pgrv&quot;, &quot;pgrx&quot;) test &lt;- texmod_to_fragvoltot(frags)[1:4] head(test) # example of texture_to_taxpartsize() tex &lt;- data.frame( texcl = c(&quot;c&quot;, &quot;cl&quot;, &quot;l&quot;, &quot;ls&quot;, &quot;s&quot;), clay = c(55, 33, 18, 6, 3), sand = c(20, 33, 42, 82, 93), fragvoltot = c(35, 15, 34, 60, 91) ) tex$fpsc &lt;- texture_to_taxpartsize( texcl = tex$texcl, clay = tex$clay, sand = tex$sand, fragvoltot = tex$fragvoltot ) head(tex) 3.11 The Shiny Package Shiny is an R package which combines R programming with the interactivity of the web. install.packages(&quot;shiny&quot;) Methods for Use Online Locally The shiny package, created by RStudio, enables users to not only use interactive applications created by others, but to build them as well. 3.11.1 Online Easiest Method Click a Link: https://gallery.shinyapps.io/lake_erie_fisheries_stock_assessment_app/ Open a web browser Navigate to a URL The ability to use a shiny app online is one of the most useful features of the package. All of the R code is executed on a remote computer which sends the results over a live http connection. Very little is required of the user in order to obtain results. 3.11.2 Locally No Internet required once configured Install R and RStudio (done) Install Required packages (app dependent) Download, open in RStudio and click “Run App” The online method may be easy for the user, but it does require a significant amount of additional effort for the creator. We won’t get into those details here! The local method, however simply requires the creator to share a single app.R file. It is the user which needs to put forth the additional effort. 3.11.3 Web App Demonstration Online: https://usda.shinyapps.io/ncr_app/ Local: https://github.com/ncss-tech/vitrusa/raw/master/r11_smp_app/app.R Online apps such as the North Central Region Web App are useful tools, available for all to use during soil survey, ecological site development, or other evaluations. The North Central Region app is however limited to data which is already available online, such as SDA (Soil Data Access) and NASIS (National Soil Information System) Web Reports. It is also reliant on the successful operation of those data systems. If the NASIS Web Reports or SDA is down for maintenance, the app fails. Local apps have the ability to leverage local data systems more easily like NASIS or other proprietary data. 3.11.4 Troubleshooting Errors Reload the app and try again. (refresh browser, or click stop, and run app again in RStudio) When the app throws an error, it stops. All other tabs/reports will no longer function until the app is reloaded. Read the getting started section on the home page. This is a quick summary of tips to avoid issues, and will be updated as needed. Check to see if SDA and NASIS Web Reports are operational, if they aren’t working, then the app won’t work either. Double check your query inputs. (typos, wildcards, null data, and too much data, can be common issues) 5 minutes of inactivity will cause the connection to drop, be sure you are actively using the app. Run the app locally - the online app does not show console activity, which can be a big help when identifying problems. Check the app issues page to see if the issue you are experiencing is already documented. (Polite but not required) Contact the app author (john.hammerly@usda.gov) When you run into trouble, there are a few steps you can take on your own to get things working again. This list may help you get your issue resolved. If not, contact (john.hammerly@usda.gov) for assistance. 3.11.5 Shiny App Embedding Shiny apps are extremely versatile, they can be embedded into presentations, Markdown, or HTML Those same formats can also be embedded in to a shiny app. This is a very simple example of a shiny app which consists of an interactive dropdown menu which controls what region is displayed in the bar chart. Let’s take a look at the code. 3.11.5.1 Shiny App Code shinyApp( # Use a fluid Bootstrap layout ui = fluidPage( # Give the page a title titlePanel(&quot;Telephones by region&quot;), # Generate a row with a sidebar sidebarLayout( # Define the sidebar with one input sidebarPanel( selectInput(&quot;region&quot;, &quot;Region:&quot;, choices = colnames(datasets::WorldPhones)), hr(), helpText(&quot;Data from AT&amp;T (1961) The World&#39;s Telephones.&quot;) ), # Create a spot for the barplot mainPanel( plotOutput(&quot;phonePlot&quot;) ) ) ), # Define a server function for the Shiny app server = function(input, output) { # Fill in the spot we created for a plot output$phonePlot &lt;- renderPlot({ # Render a barplot barplot( datasets::WorldPhones[, input$region] * 1000, main = input$region, ylab = &quot;Number of Telephones&quot;, xlab = &quot;Year&quot; ) }) } ) Shiny apps consist of a ui and a server. The ui is the part of the shiny app the user sees, the user interface. In the ui, a user can choose or enter inputs for processing and viewing the results. The server takes the inputs, performs some data processing and rendering of those inputs and generates the outputs for the ui to display. 3.11.6 Questions What new features in RStudio are available for you to use once the shiny package is installed? The Region 11 Web App uses which two data sources for reports? If an error occurs while using the Region 11 Web App, what should you do? 3.11.7 Examples 3.11.7.1 NASIS Reports The NASIS Reports button is a link to a master list of NASIS Web reports for Regions 10 and 11. 3.11.7.2 Water Table The query method option allows you to choose between MUKEY, NATSYM, MUNAME. It also has a radio button for switching between flooding and ponding. Plots and Data Tables are on separate sub-menu items. 3.11.7.3 Organic Matter Same options as the Water Table Tab except no radio button. The query method option allows you to choose between MUKEY, NATSYM, MUNAME. Plots and Data Tables are on separate sub-menu items. 3.11.7.4 Project Report The project report can accept multiple projects. Use the semicolon (;) as a separator. You can also save a copy of the report by clicking the link below the submit button. 3.11.7.5 Project Extent Type in Year, type or select office and type project name for the Project extent query. Zoom and pan to view extent. Use the layers button to change the basemap or toggle layers. Click the link below the submit button to download a .zip containing the extent as a ESRI shapefile. 3.11.7.6 Long Range Plan Enter an office symbol to generate a long range plan report. 3.11.7.7 Interpretations Enter the national mapunit symbol to plot all available interpretations for the mapunit from SDA. 3.12 Exercise 4: Using the North Central Region Web App 3.12.0.1 Project Report Use the project report to generate a report on a project in your own area. Save the results and explain the results of pH plots for one of your components. 3.12.0.2 Project Extent Map an extent of a project. How many layers are available to choose from as a basemap? How many layers can be toggled on or off? 3.12.0.3 Long Range Plan Choose an office to generate a long range plan. What is the highest acreage project for 2025? 3.13 soilReports One of the strengths of NASIS is that it that has many queries and reports to access the complex data. This makes it easy for the average user to load their data, process it, and run numerous reports. The soilReports R package is essentially just a collection of R Markdown (.Rmd) documents. R Markdown is a plain text markup format for creating reproducible, dynamic reports with R. These .Rmd files can be used to generate HTML, PDF, Word, Markdown documents with a variety of forms, templates and applications. Example report output can be found at the following link: https://github.com/ncss-tech/soilReports#example-output. Detailed instructions are provided for each report: https://github.com/ncss-tech/soilReports#choose-an-available-report Install the soilReports package. This package is updated regularly and should be installed from GitHub. # Install the soilReports package from GitHub remotes::install_github(&quot;ncss-tech/soilReports&quot;, dependencies = FALSE, build = FALSE) To view the list of available reports first load the package then use the listReports() function. # Load the soilReports package library(soilReports) # List reports listReports() ## name version ## 1 national/DT-report 1.0 ## 2 national/NASIS-site-export 1.0 ## 3 northcentral/component_summary_by_project 0.1 ## 4 northcentral/lab_summary_by_taxonname 1.1 ## 5 northcentral/mupolygon_summary_by_project 0.1 ## 6 northcentral/pedon_summary_by_taxonname 1.2 ## 7 southwest/dmu-diff 0.7 ## 8 southwest/dmu-summary 0.5 ## 9 southwest/edit-soil-features 0.2.1 ## 10 southwest/gdb-acreage 1.0 ## 11 southwest/mlra-comparison-dynamic 0.1 ## 12 southwest/mlra-comparison 2.0 ## 13 southwest/mu-comparison-dashboard 0.0.0 ## 14 southwest/mu-comparison 4.0.3 ## 15 southwest/mu-summary 1 ## 16 southwest/pedon-summary 1.1 ## 17 southwest/QA-summary 0.6 ## 18 southwest/shiny-pedon-summary 1.2 ## 19 southwest/spatial-pedons 1.1 ## 20 templates/minimal 1.0 ## description ## 1 Create interactive data tables from CSV files ## 2 Export NASIS Sites to Spatial Layer ## 3 summarize component data for an MLRA project ## 4 summarize lab data from NASIS Lab Layer table ## 5 summarize mupolygon layer from a geodatabase ## 6 summarize field pedons from NASIS pedon table ## 7 Differences between select DMU ## 8 DMU Summary Report ## 9 Generate summaries of NASIS components for EDIT Soil Features sections ## 10 Geodatabase Mapunit Acreage Summary Report ## 11 compare MLRA/LRU-scale delineations, based on mu-comparison report ## 12 compare MLRA using pre-made, raster sample databases ## 13 interactively subset and summarize SSURGO data for input to `southwest/mu-comparison` report ## 14 compare stack of raster data, sampled from polygons associated with 1-8 map units ## 15 summarize raster data for a large collection of map unit polygons ## 16 Generate summaries from NASIS pedons and associated spatial data ## 17 QA Summary Report ## 18 Interactively subset and summarize NASIS pedon data from one or more map units ## 19 Visualize NASIS pedons in interactive HTML report with overlays of SSURGO, STATSGO or custom polygons ## 20 A minimal soilReports template 3.13.1 Extending soilReports Each report in soilReports has a “manifest” that describes any dependencies, configuration files or inputs for your R Markdown report document. If you can identify these things it is easy to convert your own R-based analyses to the soilReports format. The .Rmd file allows R code and text with Markdown markup to be mingled in the same document and then “executed” like an R script. 3.14 Exercise 5: Run Lab Summary By Taxon Name Soil Report For another exercise you can use the region11/lab_summary_by_taxonname report report to summarize laboratory data for a soil series. This report requires you to get some data from the Pedon “NCSS Lab” tables in NASIS. 3.14.1 Requirements Data are properly populated, otherwise the report may fail. Common examples include: Horizon depths don’t lineup Either the Pedon or Site tables isn’t loaded ODBC connection to NASIS is set up NASIS local database and selected set contains site, pedon, NCSS Lab Pedon Lab Data tables Beware each report has a unique configuration file that may need to be edited. 3.14.2 Instructions Load your NASIS selected set. Run a query such as “POINT - Pedon/Site/NCSSlabdata by taxonname” from the NSSC Pangaea report folder to load your selected set. You can use Miami as the taxon name, or any taxon of your choice that has lab data. Be sure to target both the site, pedon and lab layer objects in query of national database. Remove from your selected set any pedons and sites you wish to exclude from the report. Copy the lab summary to your working directory. copyReport(reportName = &quot;region11/lab_summary_by_taxonname&quot;, outputDir = &quot;C:/workspace2/lab_sum&quot;) Examine the report folder contents. The report is titled report.Rmd. Notice there are several other support files. The parameters for the report are contained in the config.R file. Check or create a genhz_rules file for a soil series. In order to aggregate the pedons by horizon designation, a genhz_rules file (e.g., Miami_rules.R) is needed. See above. If none exists see the following job aid on how to create one, Assigning Generalized Horizon Labels. Pay special attention to how caret ^ and dollar $ symbols are used in REGEX. They function as anchors for the beginning and end of the string, respectively. A ^ placed before an A horizon, ^A, will match any horizon designation that starts with A, such as Ap, Ap1, but not something merely containing A, such as BA. Placing a $ after a Bt horizon, Bt$, will match any horizon designation that ends with Bt, such as 2Bt or 3Bt, but not something with a vertical subdivision, such as Bt2. Wrapping pattern with both ^ and $ symbols will result only in exact matches – i.e. that start and end with the contents between ^ and $. For example ^[AC]$, will only match A or C, not Ap, Ap2, or Cg. Execute the report. Command-line approach library(rmarkdown) # Set report parameters series &lt;- &quot;Miami&quot; genhz_rules &lt;- &quot;C:/workspace2/lab_sum/Miami_rules.R&quot; # report file path report_path &lt;- &quot;C:/workspace2/lab_sum/report.Rmd&quot; # Run the report render( input = report_path, output_dir = &quot;C:/workspace2&quot;, output_file = &quot;C:/workspace2/lab_sum.html&quot;, envir = new.env(), params = list(series = series, genhz_rules = genhz_rules) ) Manual approach Open the report.Rmd, ckucj the Knit drop down arrow, and select Knit with Parameters. Save the report. The report is automatically saved upon creation in the same folder as the R report. However, it is given the same generic name as the R report (i.e., “C:/workspace/lab_sum/report.html”), and will be overwritten the next time the report is run. Therefore, if you wish to save the report, rename the .html file to a name of your choosing and/or convert it to a PDF. Sample pedon report Brief summary of steps: Load your selected set with the pedon and site table for an existing GHL file, or make your own (highly encouraged) Run the lab_summary_by_taxonname report on a soil series of your choice by opening report.Rmd and clicking Knit with Parameters in RStudio, or using the render() function. Show your work and submit the results to your mentor. 3.15 Exercise 6: Run Mapunit Comparison Another popular report in soilReports is the region2/mu-comparison report. This report uses constant density sampling (sharpshootR::constantDensitySampling()) to extract numeric and categorical values from multiple raster data sources that overlap a set of user-supplied polygons. In this example, we clip a small portion of SSURGO polygons from the CA630 soil survey area extent. We then select a small set of mapunit symbols (5012, 3046, 7083, 7085, 7088) that occur within the clipping extent. These mapunits have soil forming factors we expect to contrast with one another in several ways. You can inspect other mapunit symbols by changing mu.set in config.R. Download the demo data: # set up ch4 path and path for report mucomp.path &lt;- &quot;C:/workspace2/chapter3/mucomp&quot; # create any directories that may be missing dir.create(mucomp.path, recursive = TRUE, showWarnings = FALSE) mucomp.zip &lt;- file.path(mucomp.path, &#39;mucomp-data.zip&#39;) # download raster data, SSURGO clip from CA630, and sample script for clipping your own raster data download.file(&#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_3-mucomp-data/mucomp-data.zip&#39;, mucomp.zip) unzip(mucomp.zip, exdir = mucomp.path, overwrite = TRUE) Create an instance of the region2/mu-comparison report with soilReports: # create new instance of reports library(soilReports) # get report dependencies reportSetup(&#39;region2/mu-comparison&#39;) # create report instance copyReport(&#39;region2/mu-comparison&#39;, outputDir = mucomp.path, overwrite = TRUE) If you want, you can now set up the default config.R that is created by copyReport() to work with your own data. OR you can use the “sample” config.R file (called new_config.R) in the ZIP file downloaded above. Run this code to replace the default config.R with the sample data config.R: # copy config file containing relative paths to rasters downloaded above file.copy(from = file.path(mucomp.path, &quot;new_config.R&quot;), to = file.path(mucomp.path, &quot;config.R&quot;), overwrite = TRUE) Open report.Rmd in the C:/workspace2/chapter3/mucomp folder and click the “Knit” button at the top of the RStudio source pane to run the report. Inspect the report output HTML file, as well as the spatial and tabular data output in the output folder. Question: What differences can you see between the five mapunit symbols that were analyzed? 3.16 Exercise 7: Run Shiny Pedon Summary The region2/shiny-pedon-summary report is an interactive Shiny-based report that uses flexdashboard to help the user subset and summarize NASIS pedons from a graphical interface. You can try a ShinyApps.io demo here The “Shiny Pedon Summary” allows one to rapidly generate reports from a large set of pedons in their NASIS selected set. The left INPUT sidebar has numerous options for subsetting pedon data. Specifically, you can change REGEX patterns for mapunit symbol, taxon name, local phase, and User Pedon ID. Also, you can use the drop down boxes to filter on taxon kind or compare different “modal”/RV pedons. Example: Analyzing the Loafercreek Taxadjuncts Create an instance of the region2/shiny-pedon-summary report with soilReports: # create new instance of reports library(soilReports) # set path for shiny-pedon-summary report instance shinyped.path &lt;- &quot;C:/workspace2/chapter3/shiny-pedon&quot; # create directories (if needed) dir.create(shinyped.path, recursive = TRUE, showWarnings = FALSE) # get report dependencies reportSetup(&#39;region2/shiny-pedon-summary&#39;) # copy report contents to target path copyReport(&#39;region2/shiny-pedon-summary&#39;, outputDir = shinyped.path, overwrite = TRUE) Update the config.R file You can update the config.R file in \"C:/workspace2/chapter3/shiny-pedon\" (or wherever you installed the report) to use the soilDB datasets loafercreek and gopheridge by setting demo_mode &lt;- TRUE. This is the simplest way to demonstrate how this report works. Alternately, when demo_mode &lt;- FALSE, pedons will be loaded from your NASIS selected set. config.R also allows you to specify a shapefile for overlaying the points on–to determine mapunit symbol–as well as several raster data sources whose values will be extracted at point locations and summarized. The demo data does not use either of these by default due to large file sizes. A very general default set of REGEX generalized horizon patterns is provided to assign generalized horizon labels for provisional grouping. These patterns are unlikely to cover ALL cases and always need to be modified for final correlation. The default config.R settings use the general patterns: use_regex_ghz &lt;- TRUE. You are welcome to modify the defaults. If you want to use the values you have populated in NASIS Pedon Horizon Component Layer ID, set use_regex_ghz &lt;- FALSE. Run the report via shiny.Rmd This report uses the Shiny flexdashboard interface. Open up shiny.Rmd and click the “Run Document” button to start the report. This will load the pedon and spatial data specified in config.R. NOTE: If a Viewer Window does not pop-up right away, click the gear icon to the right of the “Run Document” button. Be sure the option “Preview in Window” is checked, then click “Run Document” again. All of the subset parameters are in the left-hand sidebar. Play around with all of these options. The graphs and plots in the tabs to the right will automatically update as you make changes. When you like what you have, you can export a non-interactive HTML file for your records. To do this first set the “Report name:” box to something informative; this will be your report output file name. Then, scroll down to the bottom of the INPUT sidebar and click “Export Report” button. When rendering completes check the “output” folder in the directory where you installed the report. 3.17 Additional Reading (Exploratory Data Analysis) Healy, K., 2018. Data Visualization: a practical introduction. Princeton University Press. http://socviz.co/ Helsel, D.R., Hirsch, R.M., Ryberg, K.R., Archfield, S.A., and Gilroy, E.J., 2020, Statistical methods in water resources: U.S. Geological Survey Techniques and Methods, book 4, chap. A3, 458 p., https://doi.org/10.3133/tm4a3. [Supersedes USGS Techniques of Water-Resources Investigations, book 4, chap. A3, version 1.1.] Kabacoff, R.I., 2018. Data Visualization in R. https://rkabacoff.github.io/datavis/ Peng, R. D., 2016. Exploratory Data Analysis with R. Leanpub. https://bookdown.org/rdpeng/exdata/ Wilke, C.O., 2019. Fundamentals of Data Visualization. O’Reily Media, Inc. https://clauswilke.com/dataviz/ References "],["spatial.html", "Chapter 4 Spatial Data in R 4.1 Objectives (Spatial Data) 4.2 Making Maps with R 4.3 Spatial Data Sources 4.4 Viewing Pedon Locations 4.5 Exercise 1: Pedon Locations 4.6 Packages for Vector Data 4.7 Exercise 2: Map your favorite soil series extents 4.8 Packages for Raster Data 4.9 Coordinate Reference Systems (CRS) 4.10 Load Required Packages 4.11 Download Example Data 4.12 Load Example MLRA Data 4.13 Vector Data 4.14 Raster Data 4.15 Exercise 3: Creating a Slope Map 4.16 Exercise 4: Extracting Raster Data 4.17 Additional Reading (Spatial)", " Chapter 4 Spatial Data in R This chapter is a brief demonstration of possible ways to process spatial data in R. This will help you to develop a strategy for selecting spatial data processing methods. 4.1 Objectives (Spatial Data) Gain experience creating, editing, and exporting spatial data objects in R. Learn the basics of sf package for vector (points, lines, polygons) data Learn the basics of terra classes and functions for vector and raster data Learn about some interfaces to NCSS spatial data sources Create maps with R The next sections will require loading these libraries into the R session. # SPC and soil database interface library(aqp) library(soilDB) # &quot;Simple Feature&quot; (vector) data structures library(sf) # superseded by sf -- spatial object classes e.g. SpatialPoints/SpatialPolygons library(sp) # gridded data management / analysis library(terra) # superseded by terra # library(raster) # interactive maps with leaflet library(mapview) There are many packages available for working with spatial data, however we only have time to cover introducing a few common libraries. A couple resources are linked here for 5 packages that provide different ways displaying spatial data graphically: tmap ggplot2, ggmap mapview mapdeck leaflet 4.2 Making Maps with R R has become a powerful tool for visualization and interaction with spatial data. There are many tools available for making maps with R! It is not all geostatistics and coordinate reference system transformations. There are powerful ways to automate your GIS workflow from beginning to end–from creating terrain derivatives from a source DEM, to high-quality, publication-ready maps and interactive HTML/JavaScript widgets. An excellent resource for making maps with R is the chapter in “Geocomputation with R” geocompr: Making maps with R 4.3 Spatial Data Sources Spatial data sources: “raster” and “vector” Raster data sources (grids/images): GeoTIFF, ERDAS, BIL, ASCII grid, WMS, … Vector data sources (points/lines/polygons): Shape File, ESRI File Geodatabase, KML, GeoJSON, GML, WFS, … Conventional data sources that can be upgraded to be spatial data because they contain GeoJSON, WKT, or point coordinates include: NASIS/LIMS reports Web pages and Web Services Excel and CSV files Photo EXIF information Here are some R-based interfaces to NCSS data sources via soilDB package. Functions that return tabular data contain longitude, latitude coordinates: fetchNASIS(): NASIS “site” data fetchLDM(): KSSL “site” data from Lab Data Mart fetchKSSL(): KSSL “site” data from SoilWeb Functions that return spatial data: fetchSDA_spatial(): polygon, bounding box and centroid data from SSURGO, STATSGO and the sapolygon (Soil Survey Area Polygon) from Soil Data Access (SDA) fetchHenry(): sensor / weather station locations as points SDA_query(): SSURGO data as points, lines, polygons (via SDA) SDA_spatialQuery(): use points or polygons as a “query” to SDA seriesExtent() and taxaExtent(): extent of series and taxonomic classes derived from SSURGO (SoilWeb) in vector and raster format (800m resolution). The vector output is identical to series extents reported by Series Extent Explorer mukey.wcs() and ISSR800.wcs() provide an interface to gSSURGO (mukey), gNATSGO (mukey), and the ISSR-800 (gridded soil property) data. 4.4 Viewing Pedon Locations In this section we will introduce the sf package with mapview. 4.4.1 Plotting Geographic Data Making maps of data gives you some idea how data look spatially and whether their distribution is what you expect. Typos are relatively common when coordinates are manually entered. Viewing the data spatially is a quick way to see if any points plot far outside of the expected geographic area of interest. # plot the locations of the gopheridge pedons with R # # Steps: # 1) create and inspect an sf data.frame object # 2) plot the data with mapview # load libraries library(aqp) library(soilDB) library(sf) library(mapview) # this creates sample gopheridge object in your environment data(&quot;gopheridge&quot;, package = &quot;soilDB&quot;) # replace gopheridge object with fetchNASIS() (your data) # gopheridge &lt;- fetchNASIS() # create simple features POINT geometry data.frame # st_as_sf(): convert data.frame to spatial simple features, with points in $geometry # st_crs(): set EPSG:4326 Coordinate Reference System (CRS) as Well-Known Text (WKT) gopher.locations &lt;- st_as_sf( site(gopheridge), coords = c(&#39;longstddecimaldegrees&#39;,&#39;latstddecimaldegrees&#39;), crs = st_crs(4326) ) # create interactive map with sfc_POINT object # use site_id in sf data.frame as labels mapview(gopher.locations, label = gopher.locations$site_id) 4.5 Exercise 1: Pedon Locations In this exercise, you will create an interactive map with the pedons in your selected set. Then you will export them to a shapefile. Send a screenshot of your interactive map and your R script with any comments to your mentor. Modify the code snippets below to make an R plot and a shapefile of pedon data loaded from your NASIS selected set. You will plot pedon locations using the standard WGS84 longitude/latitude decimal degrees fields from Site table of NASIS. In some cases, these data might be incomplete. You will create a subset SoilProfileCollection for the pedons that have complete spatial data (longstddecimaldegrees and latstddecimaldegrees). Load the aqp, soilDB, sf and mapview packages and some pedons via fetchNASIS() (or similar source). You can use the sample datasets from Chapter 2 if you would like. library(aqp) library(soilDB) library(sf) library(mapview) # get pedons from the selected set pedons &lt;- fetchNASIS(from = &#39;pedons&#39;) Use the base R subset() function to create a subset of your SoilProfileCollection using is.na() to detect missing spatial data. longstddecimaldegrees and latstddecimaldegrees variables contain WGS84 longitude and latitude in decimal degrees. This is the standard format for location information used in NASIS. # modify this code (replace ...) to create a subset pedons.sp &lt;- aqp::subset(pedons, ...) Create a sf data.frame from the site data in the SoilProfileCollection object pedons.sp using aqp::site(). Replace the ... in the following code. Promoting a data.frame to sf POINT geometry requires that X and Y coordinate columns be specified. pedon.locations &lt;- sf::st_as_sf( ..., coords = c(&#39;longstddecimaldegrees&#39;, &#39;latstddecimaldegrees&#39;), crs = sf::st_crs(4326) #WGS84 GCS ) View your sf object pedon.locations interactively with mapview::mapview(), and change the map.types argument to 'Esri.WorldImagery'. Use the pedon.locations column named site_id for the label argument. # plot an interactive map mapview(pedon.locations, legend = FALSE, map.types = &#39;OpenStreetMap&#39;, ...) Create a subset sf data.frame with only the following “site data” columns: pedlabsampnum, upedonid, taxonname, hillslopeprof, elev_field, slope_field, aspect_field, plantassocnm, bedrckdepth, bedrckkind, pmkind, pmorigin. Select the target columns with dplyr::select() (or another method) by replacing the ... in the following code. pedon.locations_sub &lt;- dplyr::select(pedon.locations, ...) # see also base::subset(x, select=...) Export the spatial information in pedon.locations_sub to a shape file (.shp) with sf::st_write() # write to SHP; output CRS is geographic coordinate system WGS84 sf::st_write(pedon.locations_sub, &quot;./NASIS-pedons.shp&quot;) 4.6 Packages for Vector Data 4.6.1 The sf package Simple Features Access is a set of standards that specify a common storage and access model of geographic features. It is used mostly for two-dimensional geometries such as point, line, polygon, multi-point, multi-line, etc. This is one of many ways of modeling the geometry of shapes in the real world. This model happens to be widely adopted in the R ecosystem via the sf package, and very convenient for typical data encountered by soil survey operations. The sf package represents the latest and greatest in spatial data processing within the comfort of an R session. It provides a “main” object class sf to contain geometric data and associated tabular data in a familiar data.frame format. sf methods work on a variety of different levels of abstraction and manipulation of those geometries. Most of the sf package functions start with the prefix st_, such as: st_crs() (get/set coordinate reference system), st_transform() (project feature class to different coordinate reference system), st_bbox() (bounding box), st_buffer() (buffer). Many of these are “verbs” that are common GIS operations. 4.6.1.1 sf vignettes You can the following sf package vignettes for details, sample data sets and usage of sf objects. Simple Features for R Reading, Writing and Converting Simple Features Manipulating Simple Feature Geometries Manipulating Simple Features Plotting Simple Features Miscellaneous Spherical geometry in sf using s2geometry 4.6.2 The sp Package The data structures (“classes”) and functions provided by the sp package have served a foundational role in the handling of spatial data in R for years. Many of the following examples will reference names such as SpatialPoints, SpatialPointsDataFrame, and SpatialPolygonsDataFrame. These are specialized (S4) classes implemented by the sp package. Objects of these classes maintain linkages between all of the components of spatial data. For example, a point, line, or polygon feature will typically be associated with: coordinate geometry bounding box coordinate reference system attribute table 4.6.3 Converting sp and sf sp provides access to the same compiled code libraries (PROJ, GDAL, GEOS) through sf package. For now the different package object types are interchangeable, and you may find yourself having to do this for a variety of reasons. You can convert between object types as needed using sf::as_Spatial() or sf::st_as_sf(). Check the documentation (?functionname) to figure out what object types different methods need as input; and check an input object’s class with class() or inherits(). 4.6.4 Importing / Exporting Vector Data Import a feature class from a ESRI File Geodatabase or shape file. If you have a .shp file, you can specify the whole path, including the file extension in the dsn argument, or just the folder. For a Geodatabase, you should specify the feature class using the layer argument. Note that a trailing “/” is omitted from the dsn (data source name) and the “.shp” suffix is omitted from the layer. 4.6.4.1 sf x &lt;- sf::st_read(dsn = &#39;E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb&#39;, layer = &#39;ca630_a&#39;) x &lt;- sf::read_sf(dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;) sf::st_write(x, dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;) sf::write_sf(x, dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;) 4.6.4.2 sp Export object x to shapefile using the sf syntax. rgdal is no longer available on CRAN. The sf st_read() / read_sf() / st_write() / write_sf() functions have many arguments, so it is worth spending some time reviewing the associated manual pages. 4.6.5 Interactive mapping with mapview and leaflet The mapview and leaflet packages make it possible to display interactive maps of sf objects in RStudio viewer pane, or within an HTML document generated via R Markdown (e.g. this document). mapview package Basics Advanced Features See other “Articles” in this series, you can make complex, interactive maps using the mapview package. leaflet package leafem: ‘leaflet’ Extensions for ‘mapview’ 4.7 Exercise 2: Map your favorite soil series extents The seriesExtent function in soilDB returns an sf object showing generalized extent polygons for a given soil series. # load required packages, just in case library(soilDB) library(sf) library(mapview) # series extents from SoilWeb (sf objects) pentz &lt;- seriesExtent(&#39;pentz&#39;) amador &lt;- seriesExtent(&#39;amador&#39;) # combine into a single object s &lt;- rbind(pentz, amador) # colors used in the map # add more colors as needed cols &lt;- c(&#39;royalblue&#39;, &#39;firebrick&#39;) # make a simple map, colors set by &#39;series&#39; column mapview(s, zcol = &#39;series&#39;, col.regions = cols, legend = TRUE) The following code demonstrates how to fetch / convert / map soil series extents, using a vector of soil series names. Results appear in the RStudio “Viewer” pane. Be sure to try the “Export” and “show in window” (next to the broom icon) buttons. # load required packages, just in case library(soilDB) library(sf) library(mapview) # vector of series names, letter case does not matter # try several (2-9)! series.names &lt;- c(&#39;auberry&#39;, &#39;sierra&#39;, &#39;holland&#39;, &#39;cagwin&#39;) # iterate over series names, get extent # result is a list of sf objects s &lt;- lapply(series.names, soilDB::seriesExtent) # flatten list -&gt; single sf object s &lt;- do.call(&#39;rbind&#39;, s) # colors used in the map # note trick used to dynamically set the number of colors cols &lt;- RColorBrewer::brewer.pal(n = length(series.names), name = &#39;Set1&#39;) # make a simple map, colors set by &#39;series&#39; column # click on polygons for details # try pop-out / export buttons mapview(s, zcol = &#39;series&#39;, col.regions = cols, legend = TRUE) Question: What do you notice about the areas where the extent polygons occur? Share your thoughts with your peers or mentor 4.8 Packages for Raster Data 4.8.1 The terra Package The terra package package provides most of the commonly used grid and vector processing functionality that one might find in a conventional GIS. It provides high-level data structures and functions for the GDAL (Geospatial Data Abstraction Library). resampling (terra::resample()) projection and warping (terra::project()) cropping, mosaicing, masking (terra::crop(), terra::mosaic(), terra::merge(), terra::mask()) local and focal functions (terra::local(), terra::focal()) raster algebra (arithmetic operators, terra::xapp()) sampling (terra::spatSample()) contouring (terra::contour()) raster/vector conversions (terra::rasterize(), terra::as.polygons()) terrain analysis (terra::terrain()) model-based prediction and interpolation (terra::predict(), terra::interpolate(); more on this in Part 2) 4.8.1.1 Terra Example This is a brief demonstration using sample data files with terra. # use an example from the terra package f &lt;- system.file(&quot;ex&quot;, &quot;elev.tif&quot;, package = &quot;terra&quot;) # corresponding luxembourg vector (polygon) data g &lt;- system.file(&quot;ex&quot;, &quot;lux.shp&quot;, package = &quot;terra&quot;) r &lt;- terra::rast(f) r ## class : SpatRaster ## dimensions : 90, 95, 1 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : 5.741667, 6.533333, 49.44167, 50.19167 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : elev.tif ## name : elevation ## min value : 141 ## max value : 547 v &lt;- terra::vect(g) v ## class : SpatVector ## geometry : polygons ## dimensions : 12, 6 (geometries, attributes) ## extent : 5.74414, 6.528252, 49.44781, 50.18162 (xmin, xmax, ymin, ymax) ## source : lux.shp ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## names : ID_1 NAME_1 ID_2 NAME_2 AREA POP ## type : &lt;num&gt; &lt;chr&gt; &lt;num&gt; &lt;chr&gt; &lt;num&gt; &lt;num&gt; ## values : 1 Diekirch 1 Clervaux 312 1.808e+04 ## 1 Diekirch 2 Diekirch 218 3.254e+04 ## 1 Diekirch 3 Redange 259 1.866e+04 # show SpatRaster details print(r) ## class : SpatRaster ## dimensions : 90, 95, 1 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : 5.741667, 6.533333, 49.44167, 50.19167 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : elev.tif ## name : elevation ## min value : 141 ## max value : 547 # default plot method plot(r) lines(v) # interactive (leaflet) plot method p &lt;- plet(r, tiles = &quot;OpenTopoMap&quot;) lines(p, v) The R object only stores a reference to the data until they are needed to be loaded into memory. This allows for internal raster manipulation algorithms to intelligently deal with very large grids that may not fit in memory. 4.8.1.2 Other approaches to raster data 4.8.1.2.1 raster A more complete background on the capabilities of the raster package, and the replacement terra, are described in the Spatial Data Science with R online book. # convert r to a RasterLayer object r2 &lt;- raster::raster(f) # show RasterLayer details print(r2) ## class : RasterLayer ## dimensions : 90, 95, 8550 (nrow, ncol, ncell) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : 5.741667, 6.533333, 49.44167, 50.19167 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs ## source : elev.tif ## names : elevation ## values : 141, 547 (min, max) Introduction to the raster package vignette 4.8.1.2.2 stars There is also a package called stars (Spatiotemporal Arrays: Raster and Vector Datacubes) that is the sf-centric way of dealing with higher dimensional raster and vector “datacubes.” Data cubes have additional dimensions related to time, spectral band, or sensor type. The stars data structures are often used for processing spectral data sources from satellites. 4.8.1.3 Related Links sf package website rspatial.org - Spatial Data Science with R Goodbye PROJ.4 strings! How to specify a coordinate reference system in R? 4.8.2 Converting Vector to Raster 4.8.2.1 terra::rasterize() 4.8.2.2 fasterize::fasterize() 4.8.3 Converting Raster to Vector 4.8.3.1 terra::as.polygons() 4.9 Coordinate Reference Systems (CRS) Spatial data aren’t all that useful without an accurate description of the Coordinate Reference System (CRS). This type of information is typically stored within the “.prj” component of a shapefile, or in the header of a GeoTIFF. Without a CRS it is not possible to perform coordinate transformations (e.g. conversion of geographic coordinates to projected coordinates), spatial overlay (e.g. intersection), or geometric calculations (e.g. distance or area). The “old” way (PROJ.4) of specifying coordinate reference systems is using character strings containing, for example: +proj or +init arguments. In general, this still “works,” so you may encounter it and need to know about it. But you also may encounter cases where CRS are specified using integers, strings of the form authority:code, or well-known text (WKT). Some common examples of coordinate system “EPSG” codes and their legacy “PROJ.4” strings. 4 “EPSG” stands for European Petroleum Survey Group. The “EPSG Geodetic Parameter Dataset” is a public registry of geodetic datums, spatial reference systems, Earth ellipsoids, coordinate transformations and related units of measurement. “OGC” refers to the Open Geospatial Consortium, which is an example of another important authority:code. “ESRI” (company that develops ArcGIS) also defines many CRS codes. “PROJ” is the software responsible for transforming coordinates from one CRS to another. The current version of PROJ available is 9, and in PROJ &gt; 6 major changes were made to the way that coordinate reference systems are defined and transformed led to the “PROJ.4” syntax falling out of favor. EPSG: 4326 / PROJ.4:+proj=longlat +datum=WGS84 - geographic, WGS84 datum (NASIS Standard) OGC:CRS84 - geographic, WGS84 datum (same as above but explicit longitude, latitude XY order) EPSG: 4269 / PROJ.4:+proj=longlat +datum=NAD83 - geographic, NAD83 datum EPSG: 4267 / PROJ.4:+proj=longlat +datum=NAD27 - geographic, NAD27 datum EPSG: 26910 / PROJ.4:+proj=utm +zone=10 +datum=NAD83 - projected (UTM zone 10), NAD83 datum EPSG: 5070 / PROJ.4: +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23.0 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs - Albers Equal Area CONUS (gSSURGO) More on the EPSG codes and specifics of CRS definitions: https://spatialreference.org/ref/epsg/ https://epsg.io/ While you may encounter PROJ.4 strings, these are no longer considered the preferred method of referencing Coordinate Reference Systems – and, in general, newer methods are “easier.” Well-known text (WKT) is a human- machine-readable standard format for geometry, so storing the Coordinate Reference System information in a similar format makes sense. This format is returned by the sf::st_crs() method. For example: the WKT representation of EPSG:4326: st_crs(4326) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2296)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] This is using the OGC WKT CRS standard. Adoption of this standard caused some significant changes in packages in the R ecosystem. So you can get familiar, what follows are several examples of doing the same thing: setting the CRS of spatial objects with WGS84 longitude/latitude geographic coordinates. If you have another target coordinate system, it is just a matter of using the correct codes to identify it. 4.9.1 Assigning and Transforming Coordinate Systems Returning to the example from above, lets assign a CRS to our series extent s using different methods. s &lt;- seriesExtent(&#39;san joaquin&#39;) The following sections give equivalent sf versus sp syntax. 4.9.1.1 sf Use st_crs&lt;- to set, or st_crs() get CRS of sf objects. Supply the target EPSG code as an integer. # the CRS of s is EPSG:4326 st_crs(s) == st_crs(4326) ## [1] TRUE # set CRS using st_crs&lt;- (replace with identical value) st_crs(s) &lt;- st_crs(4326) Transformation of points, lines, and polygons with sf requires an “origin” CRS be defined in the argument x. The “target” CRS is defined as an integer (EPSG code) in the crs argument or is the output of st_crs(). # transform to UTM zone 10 s.utm &lt;- st_transform(x = s, crs = 26910) # transform to GCS NAD27 s.nad27 &lt;- st_transform(x = s, crs = st_crs(4267)) 4.9.1.2 sp You can do the same thing several different ways with sp objects. An equivalent EPSG, OGC and PROJ.4 can be set or get using proj4string&lt;-/proj4string and either a sp CRS object or a PROJ.4 string for Spatial objects. # s is an sf object (we converted it), convert back to Spatial* object s.sp &lt;- sf::as_Spatial(s) # these all create the same internal sp::CRS object proj4string(s.sp) &lt;- sp::CRS(&#39;EPSG:4326&#39;) # proj &gt;6; EPSG proj4string(s.sp) &lt;- sp::CRS(&#39;OGC:CRS84&#39;) # proj &gt;6; OGC proj4string(s.sp) &lt;- &#39;+init=epsg:4326&#39; # proj 4 style +init string (deprecated) proj4string(s.sp) &lt;- &#39;+proj=longlat +datum=WGS84&#39; # proj 4 style +proj string Here, we do the same transformations we did above only using sp::spTransform(). # transform to UTM zone 10 s.utm &lt;- spTransform(s.sp, CRS(&#39;+proj=utm +zone=10 +datum=NAD83&#39;)) # transform to GCS NAD27 s.nad27 &lt;- spTransform(s.sp, CRS(&#39;+proj=longlat +datum=NAD27&#39;)) 4.9.1.3 terra and raster To assign or get the coordinate reference system for raster, terra or sp CRS objects use the crs() functions. r &lt;- terra::rast(system.file(&quot;ex&quot;, &quot;elev.tif&quot;, package=&quot;terra&quot;)) # inspect CRS terra::crs(r) # r is a SpatRaster object; set CRS to current CRS terra::crs(r) &lt;- terra::crs(&quot;OGC:CRS84&quot;) “Transforming” or “warping” a raster is a different from with a vector as it requires interpolation of pixels to a target resolution and CRS. The method provided by terra is project() and in raster it is projectRaster(). It works the same as the above “transform” methods in that you specify an object to transform, and the target reference system or a template for the object. t.wgs84 &lt;- terra::project(r, terra::crs(&quot;+proj=igh&quot;)) r.wgs84 &lt;- raster::projectRaster(raster::raster(r), crs = CRS(&quot;+proj=igh&quot;)) Note that the default warping of raster uses bilinear interpolation (method='bilinear'), which is appropriate for continuous variables. You also have the option of using nearest-neighbor (method='ngb') for categorical variables (class maps) where interpolation would not make sense. If we want to save this transformed raster to file, we can use something like this for terra terra::writeRaster(t.wgs84, filename=&#39;s_wgs84.tif&#39;, gdal=c(&quot;COMPRESS=LZW&quot;)) Similarly for raster: terra::writeRaster(r.wgs84, filename=&#39;s_wgs84.tif&#39;, gdal=c(&quot;COMPRESS=LZW&quot;)) 4.10 Load Required Packages Load required packages into a fresh RStudio Session (Ctrl + Shift + F10) library(aqp) library(soilDB) library(sf) library(terra) 4.11 Download Example Data Run the following to create a path for the example data. Be sure to set a valid path to a local disk. # store path as a variable, in case you want to keep it somewhere else ch4.data.path &lt;- &#39;C:/workspace2/chapter-4&#39; # make a place to store chapter 2b example data dir.create(ch4.data.path, recursive = TRUE) # download polygon example data from github download.file( &#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_4-spatial-data/chapter-4-mu-polygons.zip&#39;, file.path(ch4.data.path, &#39;chapter-4-mu-polygons.zip&#39;) ) # download raster example data from github download.file( &#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_4-spatial-data/chapter-4-PRISM.zip&#39;, file.path(ch4.data.path, &#39;chapter-4-PRISM.zip&#39;) ) # unzip unzip( file.path(ch4.data.path, &#39;chapter-4-mu-polygons.zip&#39;), exdir = ch4.data.path, overwrite = TRUE ) unzip( file.path(ch4.data.path, &#39;chapter-4-PRISM.zip&#39;), exdir = ch4.data.path, overwrite = TRUE ) 4.12 Load Example MLRA Data We will be using polygons associated with MLRA 15 and 18 as part of this demonstration. Import these data with sf::st_read(). # load MLRA polygons mlra &lt;- sf::st_read(file.path(ch4.data.path, &#39;mlra-18-15-AEA.shp&#39;)) ## alternately, use your own MLRA # mlra &lt;- soilDB::fetchSDA_spatial(c(&quot;15&quot;, &quot;18&quot;), by.col=&quot;MLRARSYM&quot;, geom.src = &quot;MLRAPOLYGON&quot;) |&gt; sf::st_transform(&quot;EPSG:5070&quot;) We will load the sample MLRA 15 and 18 (California) raster data (PRISM derived) using terra::rast(). If using your own MLRA, you will need to update file paths to use your own rasters. # mean annual air temperature, Deg C maat &lt;- terra::rast(file.path(ch4.data.path, &#39;MAAT.tif&#39;)) # mean annual precipitation, mm map &lt;- terra::rast(file.path(ch4.data.path, &#39;MAP.tif&#39;)) # frost-free days ffd &lt;- terra::rast(file.path(ch4.data.path, &#39;FFD.tif&#39;)) # growing degree days gdd &lt;- terra::rast(file.path(ch4.data.path, &#39;GDD.tif&#39;)) # percent of annual PPT as rain rain_fraction &lt;- terra::rast(file.path(ch4.data.path, &#39;rain_fraction.tif&#39;)) # annual sum of monthly PPT - ET_p ppt_eff &lt;- terra::rast(file.path(ch4.data.path, &#39;effective_precipitation.tif&#39;)) Sometimes it is convenient to “stack” raster data that share a common grid size, extent, and coordinate reference system into a multilayer terra SpatRaster object. Calling terra::rast() on a list of SpatRaster is equivalent to making a RasterStack from several RasterLayer with raster::stack(). # create a raster stack (multiple rasters aligned) rs &lt;- terra::rast(list(maat, map, ffd, gdd, rain_fraction, ppt_eff)) # inspect rs ## class : SpatRaster ## dimensions : 762, 616, 6 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -123.2708, -118.1375, 34.44583, 40.79583 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (EPSG:4269) ## sources : MAAT.tif ## MAP.tif ## FFD.tif ## ... and 3 more sources ## names : MAAT, MAP, FFD, GDD, rain_~ction, effec~ation ## min values : -4.073542, 114, 35, 76, 12, -825.5897 ## max values : 18.676420, 2958, 365, 3173, 100, 2782.3914 plot(rs) 4.13 Vector Data 4.13.1 sf p &lt;- sf::st_as_sf(data.frame(x = -120, y = 37.5), coords = c(&quot;x&quot;, &quot;y&quot;), crs = 4326) p.aea &lt;- st_transform(p, &quot;EPSG:5070&quot;) In sf the functions used to do this are st_intersects() or st_intersection(). st_intersects(p.aea, mlra) ## Sparse geometry binary predicate list of length 1, where the predicate was `intersects&#39; ## 1: 2 st_intersection(p.aea, mlra) ## Simple feature collection with 1 feature and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -2079434 ymin: 1870764 xmax: -2079434 ymax: 1870764 ## Projected CRS: NAD83 / Conus Albers ## MLRARSYM MLRA_ID MLRA_NAME LRRSYM ## 1 18 23 Sierra Nevada Foothills C ## LRR_NAME geometry ## 1 California Subtropical Fruit, Truck, and Specialty Crop Region POINT (-2079434 1870764) 4.13.2 terra p &lt;- terra::vect(data.frame(x = -120, y = 37.5), geom = c(&quot;x&quot;, &quot;y&quot;), crs = &quot;EPSG:4326&quot;) p.aea &lt;- project(p, &quot;EPSG:5070&quot;) In terra the functions used to determine the intersection is relate(). mlra[relate(vect(mlra), p.aea, relation = &quot;intersects&quot;), ] ## Simple feature collection with 1 feature and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -2181926 ymin: 1548989 xmax: -1970476 ymax: 2264711 ## Projected CRS: Albers ## MLRARSYM MLRA_ID MLRA_NAME LRRSYM ## 2 18 23 Sierra Nevada Foothills C ## LRR_NAME geometry ## 2 California Subtropical Fruit, Truck, and Specialty Crop Region POLYGON ((-2160599 2264711,... 4.13.3 sp In sp objects, you do these operations with the sp::over() function. Access the associated vignette by running vignette(\"over\") in the console when the sp package is loaded. # hand make a SpatialPoints object # note that this is GCS p &lt;- SpatialPoints(coords = cbind(-120, 37.5), proj4string = CRS(&#39;+proj=longlat +datum=WGS84&#39;)) mlra.sp &lt;- sf::as_Spatial(mlra) # spatial extraction of MLRA data requires a CRS transformation p.aea &lt;- spTransform(p, proj4string(mlra.sp)) over(p.aea, mlra.sp) 4.14 Raster Data 4.14.1 Object Properties SpatRaster and RasterLayer objects are similar to sf, sp and other R spatial objects in that they keep track of the linkages between data, coordinate reference system, and optional attribute tables. Getting and setting the contents of raster objects should be performed using functions such as: terra::NAflag(r) / raster::NAvalue(r): get / set the NODATA value terra::crs(r) / raster::wkt(r) : get / set the coordinate reference system terra::res(r) / raster::res(r): get / set the resolution terra::ext(r) / raster::extent(r): get / set the extent terra::datatype(r) / raster::dataType(r): get / set the data type … many more, see the raster and terra package manuals 4.14.2 Rasters “In Memory” vs. “File-Based” Processing of raster data in memory is always faster than processing on disk, as long as there is sufficient memory. The terra package handles basically all of the logic delegating in vs. out of memory processing internally–so it is rare that any adjustments to defaults are required. With the raster package, the initial file/disk-based reference can be converted to an in-memory RasterLayer with the readAll() function. You can achieve a similar effect in terra by doing set.values(object). 4.14.3 Rasters “Continuous” vs. “Categorical” Rasters can represent both continuous and categorical (factor) variables. Raster categories are stored as integers with one or more associated labels. The categories can be viewed using the levels() and terra::cats() functions. You can have multiple category columns, and the “active” category can be set with terra::activeCat(). You can use the terra::classify() function to assign integer values to each pixel that can be the basis for your categories. Then, you can set the category labels associated with each integer value. For example, we classify the terra sample elevation dataset into high and low elevation areas. We supply a reclassification matrix of values with three columns. The first column is the “low” end of the class range, the second column is the high end of the class range. The third column contains the new values to assign. x &lt;- terra::rast(system.file(&quot;ex&quot;, &quot;elev.tif&quot;, package = &quot;terra&quot;)) rcl &lt;- cbind(c(0, 300), c(300, 600), 1:2) colnames(rcl) &lt;- c(&quot;low&quot;, &quot;high&quot;, &quot;new_value&quot;) rcl ## low high new_value ## [1,] 0 300 1 ## [2,] 300 600 2 y &lt;- terra::classify(x, rcl) plot(y) Once we classify a raster into a set of integer values, we can assign labels or categories to each value with levels(): new_levels &lt;- data.frame( values = 1:2, region = c(&quot;low elevation&quot;, &quot;high elevation&quot;), otherlabel = c(&quot;A&quot;, &quot;B&quot;) ) new_levels ## values region otherlabel ## 1 1 low elevation A ## 2 2 high elevation B levels(y) &lt;- new_levels plot(y) Our categories had two columns with labels. The first one (region) is selected by default. We can use the second (otherlabel) if we set it as the active category with terra::activeCat(). terra::activeCat(y) &lt;- &quot;otherlabel&quot; plot(y) We can also handle values that are not matched in classify() matrix with the others argument. Here we set others = 3 so that any cell values that are not included in rcl get assigned value 3. rcl &lt;- cbind(c(200, 300), c(300, 600), 1:2) colnames(rcl) &lt;- c(&quot;low&quot;, &quot;high&quot;, &quot;new_value&quot;) rcl ## low high new_value ## [1,] 200 300 1 ## [2,] 300 600 2 y2 &lt;- terra::classify(x, rcl, others = 3) plot(y2) We have not provided handling for NA values so they are not altered by the above classification. We can convert NA values explicitly by adding them to rcl: rcl &lt;- cbind(c(200, 300, NA), c(300, 600, NA), c(1:2, 4)) colnames(rcl) &lt;- c(&quot;low&quot;, &quot;high&quot;, &quot;new_value&quot;) rcl ## low high new_value ## [1,] 200 300 1 ## [2,] 300 600 2 ## [3,] NA NA 4 y3 &lt;- terra::classify(x, rcl, others = 3) plot(y3) Note that classify() works with the “raw” values of categorical rasters, ignoring the categories. To simply change the labels of categorical rasters, use terra::subst() instead. 4.14.4 Writing Rasters to File Exporting data requires consideration of the output format, datatype, encoding of NODATA, and other options such as compression. With terra, \"COMPRESS=LZW\" option is used by default when writing GeoTIFF files. Using the gdal argument e.g.: terra::writeRaster(..., gdal=) is equivalent to specifying options argument to raster::writeRaster(). # using previous example data set terra::writeRaster(x, filename = &#39;t.wgs84.tif&#39;) For example, a RasterLayer object that you wanted to save to disk as an internally-compressed GeoTIFF: # using previous example data set raster::writeRaster(x, filename = &#39;r.tif&#39;, options = c(&quot;COMPRESS=LZW&quot;)) 4.14.4.1 Writing Categorical Rasters to File When you write categorical rasters to file, categories will either be stored within the file itself, or in a Persistent Auxiliary Metadata (PAM) into an .aux.xml file automatically. If only using terra or other GDAL tools to work with raster data this is usually sufficient. You can also write a Value Attribute Table (VAT) as a .vat.dbf file containing categorical information. Writing this file can be important if you want to use your categories in other GIS software such as ArcGIS (which does not necessarily make full use of GDAL PAM). We can write a data.frame containing the levels of our raster to file using foreign::write.dbf() function. You will append the “.vat.dbf” extension to the base filename of your data. x &lt;- terra::rast(system.file(&quot;ex&quot;, &quot;elev.tif&quot;, package = &quot;terra&quot;)) rcl &lt;- cbind(c(0, 300), c(300, 600), 1:2) colnames(rcl) &lt;- c(&quot;low&quot;, &quot;high&quot;, &quot;new_value&quot;) rcl y &lt;- terra::classify(x, rcl) plot(y) terra::writeRaster(y, &quot;my_categorical_data.tif&quot;, overwrite = TRUE) my_categories &lt;- data.frame( values = 1:2, region = c(&quot;low elevation&quot;, &quot;high elevation&quot;), otherlabel = c(&quot;A&quot;, &quot;B&quot;) ) foreign::write.dbf(my_categories, file = &quot;my_categorical_data.tif.vat.dbf&quot;) 4.14.5 Data Types Commonly used raster datatype include: “unsigned integer”, “signed integer”, and “floating point” of variable precision. INT1U: integers from 0 to 255 INT2U: integers from 0 to 65,534 INT2S: integers from -32,767 to 32,767 INT4S: integers from -2,147,483,647 to 2,147,483,647 FLT4S: floating point from -3.4e+38 to 3.4e+38 FLT8S: floating point from -1.7e+308 to 1.7e+308 It is wise to manually specify an output datatype that will “just fit” the required precision. For example, if you have generated a SpatRaster that warrants integer precision and ranges from 0 to 100, then the INT1U data type would provide enough precision to store all possible values and the NODATA value, with the smallest possible file size. Raster data stored as integers will always be smaller (sometimes 10-100x) than those stored as floating point, especially when compressed. # integer grid with a range of 0-100 raster::writeRaster(x, filename = &#39;r.tif&#39;, datatype = &#39;INT1U&#39;) # floating point grid with very wide range terra::writeRaster(x, filename = &#39;r.tif&#39;, datatype = &#39;FLT4S&#39;) 4.14.5.1 Notes on Compression It is often a good idea to create internally-compressed raster data. The GeoTiff format can accommodate many different compression algorithms, including lossy (JPEG) compression. Usually, the default “LZW” or “DEFLATE” compression will result in significant savings, especially for data encoded as integers. For example, the CONUS gSSURGO map unit key grid at 30m resolution is about 55Gb (GeoTiff, no compression) vs. 2.4Gb after LZW compression. # reasonable compression using LZW is the default, compare to raster::writeRaster(x, filename=&#39;r.tif&#39;, options=c(&quot;COMPRESS=NONE&quot;)) # takes longer to write the file, but better compression terra::writeRaster(x, filename=&#39;r.tif&#39;, gdal=c(&quot;COMPRESS=DEFLATE&quot;, &quot;PREDICTOR=2&quot;, &quot;ZLEVEL=9&quot;) See this article for some ideas on optimization of file read/write times and associated compressed file sizes. 4.15 Exercise 3: Creating a Slope Map In this exercise you will create a continuous and categorical slope gradient maps from a digital elevation model. Use the sample Tahoe-area LiDAR elevation dataset from the gdalUtilities package or your own dataset as input. If you use your own data, you may want to make a smaller extent with terra::crop(). tahoe &lt;- terra::rast(system.file(\"extdata\", \"tahoe_lidar_bareearth.tif\", package = \"gdalUtilities\")) Run terra::terrain() to create a slope map with unit=\"radians\". Convert radians to percent slope (divide by 2*pi, multiply by 100). Make a plot of the continuous percent slope. You can use terra::plot() for static map or terra::plet() for an interactive map. Make a plot of the binned percent slope. You can use terra::plot(..., type=\"interval\") to do this. You can experiment with custom class breaks using the breaks argument. Use terra::classify() to create a categorical map of custom slope classes. Use the following breaks and assign the integer values 1 through 5 from lowest to highest. 0 to 3% 3 to 15% 15 to 30% 30 to 60% &gt;60% Create a data.frame containing the integer values and class labels. Use levels() to set the categories for your raster. Write the raster data to a GeoTIFF file with terra::writeRaster() Write the raster attribute table to a .vat.dbf file with foreign::write.dbf() Bonus: Load the GeoTIFF file in the GIS software of your choice to inspect. ## Spatial Overlay Operations Spatial data are lot more useful when “related” (overlay, intersect, spatial query, etc.) to generate something new. The CRS of the two objects being overlaid must match. 4.15.1 Working with Vector and Raster Data Typically, spatial queries of raster data by geometry features (point, line, polygon) are performed in two ways: For each geometry, collect all pixels that overlap (exactextractr approach) For each geometry, collect a sample of pixels defined by sampling points The first method ensures that all data are included in the analysis, however, processing can be slow for multiple/detailed rasters, and the results may not fit into memory. The second method is more efficient (10-100x faster), requires less memory, and can remain statistically sound–as long as a reasonable sampling strategy is applied. Sampling may also help you avoid low-acreage “anomalies” in the raster product. More on sampling methods in the next chapter. The extract() function can perform several operations in one call, such as buffering (in projected units) with buffer argument. See the manual page for an extensive listing of optional arguments and what they do. Sampling and extraction with terra the results in a SpatVector object. Sampling and extraction with raster methods results in a matrix object. # sampling single layer SpatRaster terra::spatSample(maat, size = 10) ## MAAT ## 1 16.182417 ## 2 16.682579 ## 3 8.404512 ## 4 10.425341 ## 5 14.306839 ## 6 11.652991 ## 7 15.888107 ## 8 13.861547 ## 9 10.101608 ## 10 13.503528 # sampling SpatRaster terra::spatSample(rs, size = 10) ## MAAT MAP FFD GDD rain_fraction effective_precipitation ## 1 12.304620 8.350e+02 1.97e+02 1.884e+03 9.0e+01 1.518918e+02 ## 2 17.136898 7.050e+02 3.07e+02 2.769e+03 9.9e+01 -2.136817e+02 ## 3 4.474953 1.193e+03 8.00e+01 7.680e+02 5.9e+01 7.497932e+02 ## 4 NA -1.700e+308 -1.70e+308 -1.700e+308 -1.7e+308 -1.700000e+308 ## 5 11.628613 1.280e+02 1.63e+02 2.001e+03 9.5e+01 -5.752902e+02 ## 6 18.305830 2.500e+02 3.33e+02 3.085e+03 1.0e+02 -7.095153e+02 ## 7 15.479310 3.340e+02 3.13e+02 2.474e+03 1.0e+02 -4.216306e+02 ## 8 NA -1.700e+308 -1.70e+308 -1.700e+308 -1.7e+308 -1.700000e+308 ## 9 6.143990 1.341e+03 1.27e+02 8.060e+02 6.0e+01 8.644851e+02 ## 10 11.603814 1.440e+02 1.59e+02 2.010e+03 9.6e+01 -5.602892e+02 par(mfcol = c(1, 2), mar = c(1, 1, 3, 1)) # regular sampling + extraction of raster values x.regular &lt;- terra::spatSample( maat, method = &quot;regular&quot;, size = 100, as.points = TRUE ) x.regular ## class : SpatVector ## geometry : points ## dimensions : 112, 1 (geometries, attributes) ## extent : -123, -118.4083, 34.64167, 40.6 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (EPSG:4269) ## names : MAAT ## type : &lt;num&gt; ## values : NA ## 16.4 ## 10.89 # see also raster::sampleRegular() plot(maat, axes = FALSE, legend = FALSE, main = &#39;Regular Sampling&#39;) points(x.regular) # random sample + extraction of raster values # note that NULL values are removed x.random &lt;- terra::spatSample( maat, size = 100, as.points = TRUE, na.rm = TRUE ) # see also raster::sampleRandom() plot(maat, axes = FALSE, legend = FALSE, main = &#39;Random Sampling with NA Removal&#39;) points(x.random) Note that the mean can be efficiently estimated, even with a relatively small number of samples. # all values: slow for large grids mean(terra::values(maat), na.rm = TRUE) # regular sampling: efficient, central tendency comparable to above mean(x.regular$MAAT, na.rm = TRUE) # this value will be pseudorandom # depends on number of samples, pattern of NA mean(x.random$MAAT, na.rm = TRUE) Just how much variation can we expect when collecting 100, randomly-located samples over such a large area? # 10 replications of samples of n=100 z &lt;- replicate(10, { mean(terra::spatSample(maat, size = 100, na.rm = TRUE)$MAAT, na.rm = TRUE) }) # 90% of the time the mean MAAT values were within: quantile(z, probs = c(0.05, 0.95)) Do the above routine 100 times: compute the mean MAAT from 100 randomly-located samples. Does it make a difference in your estimates? # MLRA polygons in native coordinate system plot(sf::st_geometry(mlra), main = &#39;MLRA 15 and 18&#39;) box() # MAAT raster plot(maat, main = &#39;PRISM Mean Annual Air Temperature (deg C)&#39;) # plot MAAT raster with MLRA polygons on top # this requires transforming to CRS of MAAT mlra.gcs &lt;- sf::st_transform(mlra, sf::st_crs(maat)) plot(maat, main = &#39;PRISM Mean Annual Air Temperature (deg C)&#39;) plot(sf::st_geometry(mlra.gcs), main = &#39;MLRA 15 and 18&#39;, add = TRUE) 4.16 Exercise 4: Extracting Raster Data 4.16.1 Raster Summary By Point: NASIS Pedon Locations In this extendex example and exercise, we will extract PRISM data at the coordinates associated with NASIS pedons that have been correlated to the Loafercreek series. We will use the sample dataset loafercreek from the soilDB package to get NASIS data. This example can be easily adapted to your own pedon data extracted from NASIS using fetchNASIS(), but if your points are not in California, you will need to supply your own raster data. Get some NASIS data and upgrade the “site” data to a sf object. data(&quot;loafercreek&quot;, package=&quot;soilDB&quot;) # result is a SoilProfileCollection object pedons &lt;- loafercreek ## alternately, use fetchNASIS() # pedons &lt;- fetchNASIS() # extract site data s &lt;- sf::st_as_sf(aqp::site(pedons), coords = c(&quot;longstddecimaldegrees&quot;, &quot;latstddecimaldegrees&quot;), crs = 4326, na.fail = FALSE) Extract PRISM data (the SpatRaster object we made earlier) at the Loafercreek pedon locations and summarize. # convert sf object s to terra SpatVector # and project to CRS of the raster s2 &lt;- project(terra::vect(s), rs) # pass to terra::extract() e &lt;- terra::extract(rs, s2, df = TRUE) # summarize: remove first (ID) column using [, -1] j index summary(e[, -1]) ## MAAT MAP FFD GDD rain_fraction effective_precipitation ## Min. :13.15 Min. : 432.0 Min. :189.0 Min. :2085 Min. :96.00 Min. :-433.14 ## 1st Qu.:15.59 1st Qu.: 576.0 1st Qu.:261.2 1st Qu.:2479 1st Qu.:99.00 1st Qu.:-263.46 ## Median :15.99 Median : 682.5 Median :285.0 Median :2540 Median :99.00 Median :-152.00 ## Mean :15.82 Mean : 680.4 Mean :281.0 Mean :2515 Mean :98.81 Mean :-146.05 ## 3rd Qu.:16.24 3rd Qu.: 771.0 3rd Qu.:307.8 3rd Qu.:2592 3rd Qu.:99.00 3rd Qu.: -36.87 ## Max. :16.58 Max. :1049.0 Max. :330.0 Max. :2654 Max. :99.00 Max. : 201.61 Join the extracted PRISM data with the original SoilProfileCollection object. # combine site data (sf) with extracted raster values (data.frame), row-order is identical, result is sf res &lt;- cbind(s, e) # extract unique IDs and PRISM data # dplyr verbs work with sf data.frames res2 &lt;- dplyr::select(res, upedonid, MAAT, MAP, FFD, GDD, rain_fraction, effective_precipitation) # join with original SoilProfileCollection object via pedon_key site(pedons) &lt;- res2 The extracted values are now part of the “pedons” SoilProfileCollection object via site(&lt;SoilProfileCollection&gt;) &lt;- data.frame LEFT JOIN method. Let’s summarize the data we extracted using quantiles. # define some custom functions for calculating range observed in site data my_low_function &lt;- function(x) quantile(x, probs = 0.05, na.rm = TRUE) my_rv_function &lt;- function(x) median(x, na.rm = TRUE) my_high_function &lt;- function(x) quantile(x, probs = 0.95, na.rm = TRUE) site(pedons) |&gt; dplyr::select(upedonid, MAAT, MAP, FFD, GDD, rain_fraction, effective_precipitation) |&gt; dplyr::summarize(dplyr::across( MAAT:effective_precipitation, list(low = my_low_function, rv = my_rv_function, high = my_high_function) )) ## MAAT_low MAAT_rv MAAT_high MAP_low MAP_rv MAP_high FFD_low FFD_rv FFD_high GDD_low GDD_rv GDD_high ## 1 14.33665 15.98908 16.51595 479.5 682.5 904 220 285 320 2274.75 2540.5 2638.75 ## rain_fraction_low rain_fraction_rv rain_fraction_high effective_precipitation_low ## 1 97.25 99 99 -369.3428 ## effective_precipitation_rv effective_precipitation_high ## 1 -151.9985 94.25339 4.16.2 Raster Summary By Polygon: Series Extent The seriesExtent() function from the soilDB package provides a simple interface to Series Extent Explorer data files. Note that these series extents have been generalized for rapid display at regional to continental scales. A more precise representation of “series extent” can be generated from SSURGO polygons and queried from SDA. Get an approximate extent for the Loafercreek soil series from SEE. See the seriesExtent tutorial and manual page for additional options and related functions. # get (generalized) amador soil series extent from SoilWeb x &lt;- soilDB::seriesExtent(s = &#39;loafercreek&#39;) # convert to EPSG:5070 Albers Equal Area x &lt;- sf::st_transform(x, 5070) Generate 100 sampling points within the extent using a hexagonal grid. These point locations will be used to extract raster values from our SpatRaster of PRISM data. Note that using a “hexagonal” grid is not supported on geographic coordinates. samples &lt;- sf::st_sample(x, size = 100, type = &#39;hexagonal&#39;) For comparison, extract a single point from each SSURGO map unit delineation that contains Loafercreek as a major component. This will require a query to SDA for the set of matching map unit keys (mukey), followed by a second request to SDA for the geometry. The SDA_query function is used to send arbitrary queries written in SQL to SDA, the results may be a data.frame or list, depending on the complexity of the query. The fetchSDA_spatial function returns map unit geometry as either polygons, polygon envelopes, or a single point within each polygon as selected by mukey or nationalmusym. # result is a data.frame mukeys &lt;- soilDB::SDA_query(&quot;SELECT DISTINCT mukey FROM component WHERE compname = &#39;Loafercreek&#39; AND majcompflag = &#39;Yes&#39;;&quot;) # result is a sf data.frame loafercreek.pts &lt;- soilDB::fetchSDA_spatial( mukeys$mukey, by.col = &#39;mukey&#39;, method = &#39;point&#39;, chunk.size = 35 ) Graphically check both methods: # prepare samples and mapunit points for viewing on PRISM data hexagonal &lt;- sf::st_transform(samples, sf::st_crs(maat)) x_gcs &lt;- sf::st_transform(x, sf::st_crs(maat)) maatcrop &lt;- terra::crop(maat, x_gcs) # adjust margins and setup plot device for two columns par(mar = c(1, 1, 3, 1), mfcol = c(1, 2)) # first figure plot(maatcrop, main = &#39;PRISM MAAT\\n100 Sampling Points from Extent&#39;, axes = FALSE) plot(sf::st_geometry(x_gcs), add = TRUE) plot(hexagonal, cex = 0.25, add = T) plot(maatcrop, main = &#39;PRISM MAAT\\n&quot;Loafercreek&quot; Polygon Centroids&#39;, axes = FALSE) plot(loafercreek.pts, cex = 0.25, add = TRUE) Extract PRISM data (the SpatRaster object we made earlier) at the sampling locations (100 regularly-spaced and from MU polygon centroids) and summarize. Note that CRS transformations are automatic (when possible), with a warning. # return the result as a data.frame object e &lt;- terra::extract(rs, terra::vect(hexagonal), df = TRUE) e.pts &lt;- terra::extract(rs, terra::vect(loafercreek.pts), df = TRUE) # check out the extracted data summary(e[,-1]) ## MAAT MAP FFD GDD rain_fraction effective_precipitation ## Min. :13.09 Min. : 347.0 Min. :185.0 Min. :2079 Min. :95.00 Min. :-544.69 ## 1st Qu.:15.66 1st Qu.: 526.2 1st Qu.:266.2 1st Qu.:2478 1st Qu.:99.00 1st Qu.:-333.87 ## Median :16.20 Median : 660.0 Median :299.5 Median :2560 Median :99.00 Median :-163.07 ## Mean :15.90 Mean : 667.4 Mean :286.6 Mean :2529 Mean :98.71 Mean :-164.73 ## 3rd Qu.:16.46 3rd Qu.: 791.2 3rd Qu.:315.0 3rd Qu.:2628 3rd Qu.:99.00 3rd Qu.: -13.84 ## Max. :17.01 Max. :1200.0 Max. :332.0 Max. :2735 Max. :99.00 Max. : 344.15 # all pair-wise correlations knitr::kable(cor(e[,-1]), digits = 2) MAAT MAP FFD GDD rain_fraction effective_precipitation MAAT 1.00 -0.57 0.96 0.99 0.85 -0.69 MAP -0.57 1.00 -0.55 -0.62 -0.37 0.98 FFD 0.96 -0.55 1.00 0.95 0.75 -0.67 GDD 0.99 -0.62 0.95 1.00 0.83 -0.74 rain_fraction 0.85 -0.37 0.75 0.83 1.00 -0.48 effective_precipitation -0.69 0.98 -0.67 -0.74 -0.48 1.00 Quickly compare the two sets of samples. # compile results into a list maat.comparison &lt;- list(&#39;regular samples&#39; = e$MAAT, &#39;polygon centroids&#39; = e.pts$MAAT) # number of samples per method lapply(maat.comparison, length) ## $`regular samples` ## [1] 98 ## ## $`polygon centroids` ## [1] 2336 # summary() applied by group lapply(maat.comparison, summary) ## $`regular samples` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 13.09 15.66 16.20 15.90 16.46 17.01 ## ## $`polygon centroids` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 12.70 15.82 16.19 16.02 16.44 17.41 # box-whisker plot par(mar = c(4.5, 8, 3, 1), mfcol = c(1, 1)) boxplot( maat.comparison, horizontal = TRUE, las = 1, xlab = &#39;MAAT (deg C)&#39;, varwidth = TRUE, boxwex = 0.5, main = &#39;MAAT Comparison&#39; ) Basic climate summaries from a standardized source (e.g. PRISM) might be a useful addition to an OSD, or checking the ranges reported in mapunits. 4.16.3 Raster Summary By Polygon: MLRA The following example is a simplified version of what is available in the soilReports package, reports on the ncss-tech GitHub repository. The first step is to check the MLRA polygons (mlra); how many features per MLRA symbol? Note that some MLRAs have more than one polygon. table(mlra$MLRARSYM) Convert polygon area from square meters to acres and summarize. terra::expanse() returns area in meters by default. For vector data, the best way to compute area is to use the longitude/latitude CRS. This is contrary to (erroneous, but popular) belief that you should use a planar coordinate reference system. Where applicable, the transformation to lon/lat is done automatically, if transform=TRUE. poly.area &lt;- terra::expanse(terra::vect(mlra)) / 4046.86 sf::sf_use_s2(TRUE) poly.area.s2 &lt;- units::set_units(x = sf::st_area(mlra), value = &quot;acre&quot;) sf::sf_use_s2(FALSE) poly.area.sf &lt;- units::set_units(x = sf::st_area(mlra), value = &quot;acre&quot;) summary(poly.area) sum(poly.area) sum(poly.area.s2) sum(poly.area.sf) Sample each polygon at a constant sampling density of 0.001 samples per acre (1 sample for every 1,000 ac.). At this sampling density we should expect approximately 16,700 samples–more than enough for our simple example. library(sharpshootR) # the next function requires a polygon ID: # each polygon gets a unique number 1--number of polygons mlra$pID &lt;- 1:nrow(mlra) cds &lt;- constantDensitySampling(mlra, n.pts.per.ac = 0.001) Extract MLRA symbol at sample points using the sf::st_intersection() function. The result will be a sf object with attributes from our MLRA polygons which intersect the sampling points (cds). # spatial overlay: sampling points and MLRA polygons res &lt;- sf::st_intersection(sf::st_transform(sf::st_as_sf(cds), sf::st_crs(mlra)), mlra) # row / feature order is preserved, so we can directly copy cds$mlra &lt;- res$MLRARSYM # tabulate number of samples per MLRA table(cds$mlra) ## ## 15 18 ## 11658 5204 Extract values from the SpatVector of PRISM data as a data.frame. e &lt;- terra::extract(rs, terra::project(cds, terra::crs(rs))) # join columns from extracted values and sampling points s.df &lt;- cbind(as(cds, &#39;data.frame&#39;), e) # check results head(s.df) ## MLRARSYM MLRA_ID MLRA_NAME LRRSYM ## 1 15 20 Central California Coast Range C ## 2 15 20 Central California Coast Range C ## 3 15 20 Central California Coast Range C ## 4 15 20 Central California Coast Range C ## 5 15 20 Central California Coast Range C ## 6 15 20 Central California Coast Range C ## LRR_NAME pID mlra ID MAAT MAP FFD GDD ## 1 California Subtropical Fruit, Truck, and Specialty Crop Region 1 15 1 15.19286 1149 306 2303 ## 2 California Subtropical Fruit, Truck, and Specialty Crop Region 1 15 2 15.33926 1049 307 2369 ## 3 California Subtropical Fruit, Truck, and Specialty Crop Region 1 15 3 15.42254 1041 313 2381 ## 4 California Subtropical Fruit, Truck, and Specialty Crop Region 1 15 4 15.44636 1087 308 2382 ## 5 California Subtropical Fruit, Truck, and Specialty Crop Region 1 15 5 15.39205 1116 316 2349 ## 6 California Subtropical Fruit, Truck, and Specialty Crop Region 1 15 6 15.43280 1058 313 2387 ## rain_fraction effective_precipitation ## 1 99 385.6023 ## 2 99 252.4252 ## 3 99 242.8284 ## 4 99 283.1933 ## 5 99 314.3419 ## 6 99 258.3234 Summarizing multivariate data by group (MLRA) is usually much simpler after reshaping data from “wide” to “long” format. # reshape from wide to long format m &lt;- tidyr::pivot_longer(s.df, cols = c(MAAT, MAP, FFD, GDD, rain_fraction, effective_precipitation)) # check &quot;wide&quot; format head(m) ## # A tibble: 6 × 10 ## MLRARSYM MLRA_ID MLRA_NAME LRRSYM LRR_NAME pID mlra ID name value ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 15 20 Central California Coast Range C California Subtrop… 1 15 1 MAAT 15.2 ## 2 15 20 Central California Coast Range C California Subtrop… 1 15 1 MAP 1149 ## 3 15 20 Central California Coast Range C California Subtrop… 1 15 1 FFD 306 ## 4 15 20 Central California Coast Range C California Subtrop… 1 15 1 GDD 2303 ## 5 15 20 Central California Coast Range C California Subtrop… 1 15 1 rain… 99 ## 6 15 20 Central California Coast Range C California Subtrop… 1 15 1 effe… 386. A tabular summary of means by MLRA and PRISM variable using dplyr v.s. base tapply(). # tabular summary of mean values dplyr::group_by(m, mlra, name) %&gt;% dplyr::summarize(mean(value)) %&gt;% dplyr::arrange(name) ## # A tibble: 12 × 3 ## # Groups: mlra [2] ## mlra name `mean(value)` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 15 FFD NA ## 2 18 FFD 273. ## 3 15 GDD NA ## 4 18 GDD 2497. ## 5 15 MAAT NA ## 6 18 MAAT 15.7 ## 7 15 MAP NA ## 8 18 MAP 631. ## 9 15 effective_precipitation NA ## 10 18 effective_precipitation -194. ## 11 15 rain_fraction NA ## 12 18 rain_fraction 97.2 # base R tapply(m$value, list(m$mlra, m$name), mean, na.rm = TRUE) ## effective_precipitation FFD GDD MAAT MAP rain_fraction ## 15 -195.9819 284.5447 2385.213 15.24708 588.6219 98.61088 ## 18 -193.5583 273.2077 2496.876 15.66580 630.9076 97.23251 4.16.4 Zonal Statistics with exactextractr This example shows how to determine the distribution of Frost-Free Days across a soil series extent. First, we load some sample data. We use soilDB::seriesExtent() to get some extent polygons for two series of interest. Then we load some PRISM-derived Frost Free Day estimates. library(sf) library(soilDB) library(terra) library(exactextractr) series &lt;- c(&#39;holland&#39;, &#39;san joaquin&#39;) s &lt;- do.call(&#39;rbind&#39;, lapply(series, seriesExtent)) # load PRISM data r &lt;- rast(&#39;C:/workspace2/chapter-4/FFD.tif&#39;) # inspect r # transform extent to CRS of raster s &lt;- st_transform(s, crs = st_crs(r)) # inspect s 4.16.4.1 Directly Returning Extracted Values Data are extracted from the raster data source very rapidly using the exactextractr package. # use `st_union(s)` to create a MULTI- POINT/LINE/POLYGON from single # use `sf::st_cast(s, &#39;POLYGON&#39;)` to create other types system.time({ ex &lt;- exactextractr::exact_extract(r, s) }) ## | | | 0% | |================================================ | 50% | |=================================================================================================| 100% ## user system elapsed ## 0.05 0.00 0.06 # ex is a list(), with data.frame [value, coverage_fraction] # for each polygon in s (we have one MULTIPOLYGON per series) head(ex[[1]]) ## value coverage_fraction ## 1 166 0.1 ## 2 159 1.0 ## 3 158 1.0 ## 4 197 1.0 ## 5 195 1.0 ## 6 200 1.0 # combine all list elements `ex` into single data.frame `ex.all` # - use do.call(&#39;rbind&#39;, ...) to stack data.frames row-wise # - an anonymous function that iterates along length of `ex` # - adding the series name to as a new variable, calculated using `i` ex.all &lt;- do.call(&#39;rbind&#39;, lapply(seq_along(ex), function(i) { cbind(data.frame(group = series[i]), ex[[i]]) })) summary(ex.all) ## group value coverage_fraction ## Length:19796 Min. : 74.0 Min. :0.0100 ## Class :character 1st Qu.:176.0 1st Qu.:0.5000 ## Mode :character Median :226.0 Median :1.0000 ## Mean :239.3 Mean :0.7549 ## 3rd Qu.:320.0 3rd Qu.:1.0000 ## Max. :335.0 Max. :1.0000 library(lattice) # simple summary densityplot(~ value | group, data = ex.all, plot.points = FALSE, bw = 2, lwd = 2, strip = strip.custom(bg = grey(0.85)), scales = list(alternating = 1), col = c(&#39;RoyalBlue&#39;), layout = c(1, 2), ylab = &#39;Density&#39;, from = 0, to = 400, xlab = &#39;Frost-Free Days (50% chance)\\n800m PRISM Data (1981-2010)&#39;, main = &#39;FFD Estimate for Extent of San Joaquin and Holland Series&#39; ) 4.16.4.2 Predefined Summary Operations In the previous example we extracted all values and their coverage fractions into memory so we could make a graph with them in R. This operation does not scale well to very large rasters where all values would not fit in memory. exactextractr::exact_extract() has multiple built-in summary statistics we can use. These summary statistics can be processed very efficiently as all pixels do not to be loaded into memory at once. The available methods include weighted variants that account for pixel coverage fraction. You can specify summary options using the fun argument. For example, fun=\"quantile\" calculates quantiles of cell values, weighted by coverage fraction. We used two MULTIPOLYGON geometries corresponding to two series extents, so we get two sets with 3 quantiles each for the Frost Free Days (FFD) grid. ex2 &lt;- exactextractr::exact_extract( r, s, fun = &quot;quantile&quot;, quantiles = c(0.05, 0.5, 0.95), full_colnames = TRUE, append_cols = &quot;series&quot; ) ## | | | 0% | |================================================ | 50% | |=================================================================================================| 100% ex2 ## series q05.FFD q50.FFD q95.FFD ## HOLLAND HOLLAND 125.6814 185.2306 262.3804 ## SAN JOAQUIN SAN JOAQUIN 301.0902 322.9453 328.7454 The list of summary operations available for use in exact_extract() fun argument includes: \"min\", \"max\", \"count\", \"sum\", \"mean\", \"median\", \"quantile\", \"mode\", \"majority\", \"minority\", \"variety\", \"variance\", \"stdev\", \"coefficient_of_variation\", \"weighted_mean\", \"weighted_sum\", \"weighted_stdev\", \"weighted_variance\", \"frac\", and \"weighted_frac.\" Of interest beyond the typical summary statistics are frequency tables or compositional summaries. The \"frac\" and \"weighted_frac\" methods calculate composition of unique levels of the raster for the input features. For example, imagine an interpretation for crop suitability. One requirement of this hypothetical crop is a growing season length greater than 250 days. Assume we can estimate the growing season for this crop length using Frost Free Days. First we create a classified raster based on our criteria, then we summarize the raster data using the polygon boundaries and fun=\"frac\": # calculate a binary raster # 0: not suitable # 1: suitable r2 &lt;- r &gt; 250 levels(r2) &lt;- data.frame(values = 0:1, suitability = c(&quot;Not suitable&quot;, &quot;Suitable&quot;)) plot(r2) ex3 &lt;- exactextractr::exact_extract( r2, s, fun = &quot;frac&quot;, full_colnames = TRUE, append_cols = &quot;series&quot; ) ## | | | 0% | |================================================ | 50% | |=================================================================================================| 100% ex3 ## series frac_0.suitability frac_1.suitability ## HOLLAND HOLLAND 0.9294924736 0.07050756 ## SAN JOAQUIN SAN JOAQUIN 0.0002452183 0.99975479 From this output we can see that only about 7% of areas within the Holland series extent polygon have more than 250 Frost Free Days, whereas almost all of the San Joaquin soil extent polygon would meet the growing season requirement. Note that terra also supports creation of a frequency table of raster values via terra::freq(). This function works similarly to exact_extract(..., fun = \"frac\") but is built in to terra. The exactextractr::exact_extract() method is demonstrated in this chapter because it has many more summary statistics available. 4.16.5 Example: Summarizing MLRA Raster Data with lattice graphics Lattice graphics are useful for summarizing grouped comparisons. The syntax is difficult to learn and remember, but there is a lot of documentation online. library(lattice) tps &lt;- list( box.rectangle = list(col = &#39;black&#39;), box.umbrella = list(col = &#39;black&#39;, lty = 1), box.dot = list(cex = 0.75), plot.symbol = list( col = rgb(0.1, 0.1, 0.1, alpha = 0.25, maxColorValue = 1), cex = 0.25 ) ) bwplot(mlra ~ value | name, data = m, # setup plot and data source as.table=TRUE, # start panels in top/left corner varwidth=TRUE, # scale width of box by number of obs scales=list(alternating=3, relation=&#39;free&#39;), # setup scales strip=strip.custom(bg=grey(0.9)), # styling for strips par.settings=tps, # apply box/line/point styling panel=function(...) { # within in panel, do the following panel.grid(-1, -1) # make grid lines at all tick marks panel.bwplot(...) # make box-whisker plot } ) 4.16.5.0.1 Interactive Summaries Static figures are a good start, but sometimes an interactive summary is more appropriate for EDA. The histboxp() function from the Hmisc package creates interactive (HTML) chunks that can be used in RStudio, or embedded in RMarkdown documents (e.g. these notes or soilReports). Use the mouse to hover over, click, drag, etc. to interact with the data. Double-click to reset the plot. library(Hmisc) # interactive exploration of the MAAT distributions by MLRA histboxp( x = s.df$MAAT, group = s.df$mlra, bins = 100, xlab = &#39;Mean Annual Air Temperature (deg C)&#39; ) # interactive exploration of the Effective PPT distributions by MLRA histboxp( x = s.df$eff.PPT, group = s.df$mlra, bins = 100, xlab = &#39;Annual Sum of Monthly PPT - PET (mm)&#39; ) –&gt; 4.17 Additional Reading (Spatial) Ahmed, Zia. 2020. Geospatial Data Science with R. Gimond, M., 2019. Intro to GIS and Spatial Analysis https://mgimond.github.io/Spatial/ Hijmans, R.J. 2019. Spatial Data Science with R. https://rspatial.org/ Lovelace, R., J. Nowosad, and J. Muenchow, 2019. Geocomputation with R. CRC Press. https://bookdown.org/robinlovelace/geocompr/ Pebesma, E., and R.S. Bivand. 2005. Classes and methods for spatial data: The sp package. https://cran.r-project.org/web/packages/sp/vignettes/intro_sp.pdf. Pebesma, E. and R. Bivand, 2019. Spatial Data Science. https://keen-swartz-3146c4.netlify.com/ Applied Spatial Data Analysis with R "],["sampling.html", "Chapter 5 Sampling 5.1 Introduction 5.2 Sampling Strategies 5.3 Evaluating a Sampling Strategy 5.4 Additional Reading (Sampling)", " Chapter 5 Sampling 5.1 Introduction Sampling is a fundamental part of statistics. Samples are collected to achieve an understanding of a population because it is typically not feasible to observe all members of the population. The goal is to collect samples that provide an accurate representation of the population. Constraints on time and money dictate that the sampling effort must be efficient. More samples are needed to characterize the nature of highly variable populations than less variable populations. Define your purpose: What are you investigating? Examples include soil properties, soil classes, and plant productivity. How many samples are needed? The question of how many samples are required is complex. As per usual, the answer is ‘it depends’. For example, it depends on the type of analysis (e.g. non-spatial vs spatial) and properties of the data (e.g. continuous vs categorical). In all cases it requires an understanding/assumptions of the underlying population. If a scientist can estimate the several population variables, formulas exist to estimate the required number of samples. The following are a list of methods for estimating sample size: Standard error Confidence interval Power analysis (hypothesis testing) Distance/time Prediction error Rules of thumb NSSH 627.8 The first 3 methods use formulas for their corresponding statistics to solve for the sample size, each results in progressively higher sample size requirements. The PracTools R package (Valliant and Dever 2022) offers several functions for the 1st and 2nd methods. The pwr (Champely 2020) and WebPower (Zhang and Mai 2021) R packages offer several functions for the 3rd method, based on a variety of statistical models. Gelman, Hill, and Vehtari (2020) provides a nice graphical illustration of the variables involved with these methods. 5.1.1 DSM classes A variation of the 2nd method is described by Congalton and Green (2019) for use with categorical data, which is applicable to digital soil mapping. nMultinomial &lt;- function(k = NULL, p = 0.5, error = 0.1, alpha = 0.05) { # k = number of classes # p = proportion of the largest class # error = margin of error # alpha = confidence level probability ceiling(qchisq(1 - alpha / k, 1) * p * (1 - p) / error^2) } nMultinomial(k = c(10, 20, 30)) ## [1] 197 229 248 nMultinomial(k = 10, error = c(0.05, 0.1, 0.2)) ## [1] 788 197 50 nMultinomial(k = 10, alpha = c(0.05, 0.1, 0.2)) ## [1] 197 166 136 5.1.2 Rules of thumb Some rules of thumb for regression models are as follows: Use &gt; 10 observations (n) per predictor (m) (Kutner et al. (2005)). Use &gt; 20 n per m and n &gt; 104 + m to test regression coefficients (Rossiter 2017; Franklin and Miller 2009). Never use n &lt; 5*m (Rossiter 2017). 5.1.3 NSSH 627.8 Documentation requirement for the following data elements are specified in the section 627.8 of the National Soil Survey Handbook (NSSH). # NSSH 627.8 Documentation # soil series data.frame( level = &quot;soil series&quot;, n = c(5, 10), acres = c(2000, 20000) ) ## level n acres ## 1 soil series 5 2000 ## 2 soil series 10 20000 # components data.frame( level = &quot;components&quot;, n = 1, acres = 3000 ) ## level n acres ## 1 components 1 3000 # map units data.frame( level = &quot;map unit&quot;, n = cumsum(c(30, rep(10, 3))), acres = c(2000, seq(from = 4000, by = 4000, length.out = 3)) ) ## level n acres ## 1 map unit 30 2000 ## 2 map unit 40 4000 ## 3 map unit 50 8000 ## 4 map unit 60 12000 5.2 Sampling Strategies library(sf) library(ggplot2) # set the seed for the random number generator set.seed(4) # Create a sixteen square polygon bb &lt;- st_make_grid(st_bbox(c(xmin = 0, xmax = 6, ymin = 0, ymax = 6)), n = 6) grd &lt;- st_as_sf(bb) grd$ID &lt;- 1:length(bb) 5.2.1 Simple Random In simple random sampling, all samples within the region have an equal chance of being selected. A simple random selection of points can be made using either the st_sample() function within the sf R package or the Create Random Points tool in ArcGIS. Advantages Simple Unbiased (equal probability of inclusion) Requires little prior knowledge of the population Howell et al. (2004) - produced a “much more sensitive, more accurate, and greater range of estimated values” than the models from the subjective samples Disadvantages Inefficient (requires large numbers) Lower accuracy Higher cost Samples may not be representative of the feature attribute(s) Uneven spatial distribution (e.g. clustered) # Generate simple random sample test &lt;- st_sample(grd, size = 16, type = &quot;random&quot;) ggplot() + geom_sf(data = grd) + geom_sf(data = test) + ggtitle(&quot;Simple&quot;) 5.2.2 Systematic In systematic sampling, a sample is taken according to a regularized pattern. This approach ensures even spatial coverage. Patterns may be rectilinear, triangular, or hexagonal. This sampling strategy can be inaccurate if the variation in the population doesn’t coincide with the regular pattern (e.g., if the population exhibits periodicity). Advantages Simple Precise estimates Even spatial coverage Greater efficiency Disadvantages Biased estimates (particularly sampling variance) May miss individuals that don’t coincide with the sampling interval If so, the density needs to be increased Limited utility for areas larger than a single field Grid may not optimally fit irregular shapes # Generate systematic random sample test &lt;- st_sample(grd, size = 16, type = &quot;regular&quot;) ggplot() + geom_sf(data = grd) + geom_sf(data = test) + ggtitle(&quot;Systematic&quot;) 5.2.3 Stratified Random In stratified random sampling, the sampling region is spatially subset into different strata, and random sampling is applied to each strata. If prior information is available about the study area, it can be used to develop the strata. Strata may be sampled equally or in proportion to area; however, if the target of interest is rare in the population, it may be preferable to sample the strata equally Franklin and Miller (2009)]. Advantages More efficient than simple-random Higher accuracy Lower cost Sampling can be sized relative to proportion or variance Disadvantages Require pre-existing knowledge of the population (which may be flawed) If sampling is uneven, weights need to be known May need to construct the strata manually grd2 &lt;- st_cast(grd, &quot;MULTIPOLYGON&quot;) st_crs(grd2) &lt;- 5070 test &lt;- st_sample(grd2, size = 16, type = &quot;random&quot;, by_polygon = TRUE) ggplot() + geom_sf(data = grd2) + geom_sf(data = test) + ggtitle(&quot;Stratified&quot;) Note that the spsample() function only stratifies the points spatially. Other more sophisticated designs can be implemented using the spsurvey, sharpshootR, or clhs packages. 5.2.4 Multistage Stratified Random In multistage random sampling, the region is separated into different subsets that are randomly selected (i.e., first stage), and then the selected subsets are randomly sampled (i.e., second stage). This is similar to stratified random sampling, except that with stratified random sampling each strata is sampled. Advantages Most efficient Lower cost Sampling can be sized relative to proportion or variance Disadvantages Less precise Uneven spatial distribution (e.g. clustered) Require pre-existing knowledge of the population (which may be flawed) If sampling is uneven, weights need to be known May need to construct the strata manually # Select 8 samples from each square idx &lt;- sample(1:nrow(grd), size = 2, replace = FALSE) grd_sub &lt;- grd[idx, ] test &lt;- sapply(1:2, function(i) { st_coordinates(st_sample(grd_sub[i, ], size = 8, type = &quot;random&quot;)) }) test &lt;- st_as_sf(as.data.frame(test), coords = 1:2) ggplot() + geom_sf(data = grd) + geom_sf(data = test) + ggtitle(&quot;Two-stage&quot;) 5.2.5 Spatial Coverage Sampling (e.g. k-means clustering) # spcosa ---- library(spcosa) grd2 &lt;- st_crs(grd, NA) grd_sp &lt;- as(grd, &quot;Spatial&quot;) strata &lt;- stratify(grd_sp, nStrata = 16) pts &lt;- spsample(strata) plot(strata, pts) + ggtitle(&quot;Spatial Coverage&quot;) 5.2.6 Covariate space coverage sampling (e.g. k-means clustering) Covariate space coverage sampling is a stratified sampling technique to obtain representative samples from feature (attribute) space (Brus 2022). Advantages Maximumly stratifies the predictors Automated sample selection Can incorporates cost constraints Ideal for developing map unit concepts (puts points in central landscape positions) Can incorporate legacy points Easily scales to multiple GIS layers Disadvantages Not ideal for map validation (non – probability sample) Inefficient with large GIS layers library(sf) library(terra) source(&quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/exercises/fcsc.R&quot;) # import volcano DEM, details at http://geomorphometry.org/content/volcano-maungawhau data(&quot;volcano&quot;) volcano_r &lt;- rast( volcano[87:1, 61:1], crs = crs(&quot;+init=epsg:27200&quot;), extent = c( xmin = 2667405, xmax = 2667405 + 61 * 10, ymin = 6478705, ymax = 6478705 + 87 * 10 ) ) names(volcano_r) &lt;- &quot;elev&quot; # calculate slope from the DEM slope_r &lt;- terrain(volcano_r, v = &quot;slope&quot;, unit = &quot;degrees&quot;) # Stack Elevation and Slope rs &lt;- c(volcano_r, slope_r) # Covariate Space Coverage Sampling source(&quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/exercises/fcsc.R&quot;) fs &lt;- cscs(rs, vars = c(&quot;elev&quot;, &quot;slope&quot;), n = 20) fs_sf &lt;- st_as_sf(fs, coords = c(&quot;x&quot;, &quot;y&quot;), crs = crs(rs)) # Plot CSCS Samples plot(volcano_r, axes=FALSE) points(fs_sf) 5.2.7 Conditioned Latin Hypercube (cLHS) Conditioned Latin hypercube sampling is a stratified random sampling technique to obtain representative samples from feature (attribute) space (Minasny and McBratney 2006). Advantages Maximumly stratifies the predictors Automated sample selection Incorporates cost constraints Can incorporate legacy points Easily scales to multiple GIS layers Disadvantages Not ideal for map validation (non – probability sample) Not ideal for developing map unit concepts (puts points in weird landscape positions) Difficult to find alternatives for non-accessible points Inefficient with large GIS layers For example, assume you have prior knowledge of a study area and have the time and resources to collect 120 points. You also know the following variables (strata), which are represented as coregistered raster datasets, to be of importance to the soil property or class being investigated: Normalized Difference Vegetation Index (NDVI), Topographic Wetness Index (a.k.a. Wetness Index, compound topographic index), Solar insolation (potential incoming solar radiation), and Relative elevation (a.k.a. relative position, normalized slope height). The cLHS procedure iteratively selects samples from the strata variables such that they replicate the range of values from each stratum. Without a technique such as cLHS, obtaining a sample that is representative of the feature space becomes increasingly difficult as the number of variables (strata) increases. To perform cLHS using R, you can use the clhs package (Roudier 2011). rs2 &lt;- raster::stack(rs) library(clhs) # generate cLHS design cs &lt;- clhs(rs2, size = 20, progress = FALSE, simple = FALSE) # Plot cLHS Samples par(mar=c(1,1,1,4)) plot(volcano_r, axes=FALSE) points(cs$sampled_data) # Summary of clhs object summary(cs$sampled_data)$data ## elev slope ## Min. : 95.0 Min. : 0.000 ## 1st Qu.:108.8 1st Qu.: 6.701 ## Median :124.5 Median :12.979 ## Mean :130.5 Mean :14.420 ## 3rd Qu.:150.2 3rd Qu.:20.459 ## Max. :184.0 Max. :33.197 ## NA&#39;s :1 # Summary of raster objects cbind(summary(volcano_r), summary(slope_r)[1:6]) ## elev ## &quot;Min. : 94.0 &quot; &quot;Min. : 0.000 &quot; ## &quot;1st Qu.:108.0 &quot; &quot;1st Qu.: 7.054 &quot; ## &quot;Median :124.0 &quot; &quot;Median :14.103 &quot; ## &quot;Mean :130.2 &quot; &quot;Mean :14.897 &quot; ## &quot;3rd Qu.:150.0 &quot; &quot;3rd Qu.:21.666 &quot; ## &quot;Max. :195.0 &quot; &quot;Max. :43.032 &quot; Although the above example works well on the small volcano dataset, the clhs package is inefficient if you are working with large rasters with many cells. To overcome this limitation, you can first take a large random sample and then subsample it using cLHS. sub_s &lt;- spatSample(rs, size = 200, as.points = TRUE) |&gt; st_as_sf() # random sample function from the raster package s &lt;- clhs(sub_s, size = 20, progress = FALSE, simple = FALSE) 5.3 Evaluating a Sampling Strategy To gauge the representativeness of a sampling strategy, you can compare the results it produces to the results for other variables you think might coincide with the soil properties or classes of interest (Hengl 2009). Examples include slope gradient, slope aspect, and vegetative cover. These other variables may be used to stratify the sampling design or to assess the representativeness of our existing samples (e.g., NASIS pedons). The simple example below demonstrates how to compare several sampling strategies by evaluating how well they replicate the distribution of elevation. # set seed set.seed(1234) # create a polygon from the spatial extent of the volcano dataset test &lt;- st_make_grid(ext(volcano_r), n = 1) # take a large random sample sr400 &lt;- spatSample(rs, size = 1000, method = &quot;random&quot;, as.points = TRUE) # take a small random sample sr &lt;- spatSample(rs, size = 20, method = &quot;random&quot;, as.points = TRUE) # take a small systematic random sample sys &lt;- spatSample(rs, size = 20, method = &quot;regular&quot;, as.points = TRUE) # take a cLHS sample cs &lt;- clhs(rs2, size = 20, progress = FALSE, simple = FALSE) # take a CSCS sample fs &lt;- cscs(rs, vars = c(&quot;elev&quot;, &quot;slope&quot;), n = 20) # Combind and Extract Samples s &lt;- rbind( data.frame(method = &quot;Simple Random 400&quot;, as.data.frame(sr400)), data.frame(method = &quot;Simple Random&quot;, as.data.frame(sr)), data.frame(method = &quot;Systematic Random&quot;, as.data.frame(sys)), data.frame(method = &quot;cLHS&quot;, slot(cs$sampled_data, &#39;data&#39;)), data.frame(method = &quot;CSCS&quot;, fs[c(&quot;elev&quot;, &quot;slope&quot;)]) ) # Summarize the sample values aggregate(slope ~ method, data = s, function(x) round(summary(x))) ## method slope.Min. slope.1st Qu. slope.Median slope.Mean slope.3rd Qu. slope.Max. ## 1 cLHS 3 8 15 16 21 32 ## 2 CSCS 1 7 15 16 24 36 ## 3 Simple Random 0 14 17 18 22 34 ## 4 Simple Random 400 0 6 14 15 22 43 ## 5 Systematic Random 1 8 11 14 20 32 # Plot overlapping density plots to compare the distributions between the large and small samples ggplot(s, aes(x = slope, col = method)) + geom_density(cex = 2) # plot the spatial locations par(mfrow = c(2, 2), mar = c(1,2,4,5)) plot(volcano_r, main = &quot;Simple random&quot;, cex.main = 2, axes=FALSE) points(sr, pch = 3, cex = 1.2) plot(volcano_r, main = &quot;Systematic random&quot;, cex.main = 2, axes=FALSE) points(sys, pch = 3, cex = 1.2) plot(volcano_r, main = &quot;cLHS&quot;, cex.main = 2, axes=FALSE) points(cs$sampled_data, pch = 3, cex = 1.2) plot(volcano_r, main = &quot;CSCS&quot;, cex.main = 2, axes=FALSE) points(fs_sf, pch = 3, cex = 1.2) The overlapping density plots above illustrate the differences between large and small sets of samples using several sampling designs. The cLHS approach best duplicates the distribution of elevation (because elevation is explicitly used in the stratification process). The contrast is less severe in the summary metrics, but again cLHS more closely resembles the larger sample. Other comparisons are possible using the approaches in the following chapters. 5.3.1 Exercise: Design a Sampling Strategy Load the “tahoe_lidar_bareearth.tif” dataset in the gdalUtilities package (tahoe &lt;- terra::rast(system.file(\"extdata\", \"tahoe_lidar_bareearth.tif\", package = \"gdalUtilities\"))) or use your own data set. Compare two or more sampling approaches and evaluate how representative they are. Show your work and submit the results to your mentor. 5.4 Additional Reading (Sampling) Brungard, C., &amp; Johanson, J. (2015). The gate’s locked! I can’t get to the exact sampling spot … can I sample nearby? Pedometron(37), 8-9. http://www.pedometrics.org/Pedometron/Pedometron37.pdf Brus, D.J. and J.J. de Gruijter. “Random sampling or geostatistical modelling? Choosing between design-based and model-based sampling strategies for soil (with discussion).” Geoderma. Vol. 80. 1. Elsevier, 1997. 1-44. https://www.sciencedirect.com/science/article/pii/S0016706197000724 Brus, Dick J. Spatial Sampling with R. First edition, CRC Press, 2022, https://github.com/DickBrus/SpatialSamplingwithR. de Gruijter, J., D.J. Brus, M.F.P. Bierkens, and M. Knotters. 2006. Sampling for natural resource monitoring: Springer. http://www.springer.com/us/book/9783540224860. Viscarra Rossel, R.A., et al. “Baseline estimates of soil organic carbon by proximal sensing: Comparing design-based, model-assisted and model-based inference.” Geoderma 265 (2016): 152-163. https://www.sciencedirect.com/science/article/pii/S0016706115301312 References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
