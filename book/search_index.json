[["index.html", "Statistics for Soil Survey - Part 1 Pre-course Assignment 0.1 Create Workspace 0.2 Configure RStudio 0.3 Essentials 0.4 Personalization 0.5 Install Required Packages 0.6 Dealing With Errors 0.7 Packages not on CRAN 0.8 Connect Local NASIS 0.9 Proof 0.10 Additional Support/Optional Readings", " Statistics for Soil Survey - Part 1 Soil Survey Staff 2023-01-27 Pre-course Assignment 0.1 Create Workspace Make a local folder C:\\workspace2 to use as a working directory for this course. Use all lower case letters please. 0.2 Configure RStudio Open RStudio, and edit the “Global Options” (Main menu: Tools → Global Options). 0.3 Essentials These options are important for pleasant, reproducible and efficient use of the RStudio environment: Change the default working directory to C:\\workspace2 (R General Tab) Uncheck “Restore .Rdata into workspace at startup” (R General Tab) VERY IMPORTANT Figure 1: Example of RStudio General settings. RStudio detects the available R installations on your computer. Individual versions are certified for the Software Center as they become available, and sometimes there is a more recent version available for download. It is worth taking the time before installing packages to get the latest version of R available to you. This is to minimize compatibility issues which arise over time. 0.4 Personalization Figure 2: Example of RStudio Code/Editing settings. Optional: Check “Soft-wrap R source files” (Code/Editing Tab) Optional: Show help tooltips, control auto-completion and diagnostics (Code/Completion and Diagnostics Tabs) Optional: Update code font size, colors and theme (Appearance) Optional: Use RStudio Projects (top-right corner) to manage working directories 0.5 Install Required Packages Packages can be installed by name from the Comprehensive R Archive Network (CRAN) using the base R function install.packages(). There are a lot of packages out there–many more than you will download here, and many of which are useful for Soil Survey work. The first time you install packages, R may ask you if you want to create a local package library. You need to do this because we cannot write to system folders as non-administrator users on CCE machines. The default location for R package library on Windows is: C:\\Users\\&lt;User.Name&gt;\\AppData\\Local\\R\\win-library\\&lt;X.X&gt; where &lt;User.Name&gt; is the current Windows user name and &lt;X.X&gt; is the version of R packages are being installed for. If you have an existing R package library (for same minor version of R), you can copy that library into the AppData\\Local\\R folder as needed. For example, to download and install the remotes package from CRAN: install.packages(&quot;remotes&quot;) To install the R packages used in this class copy all of the code from the box below and paste into the R console window. Paste after the command prompt (&gt;) and press enter. Downloading and configuring the packages will take a while if you are installing or upgrading all of the packages in the list below. ## character vector of package names packages &lt;- c( # soil &quot;aqp&quot;, &quot;soilDB&quot;, &quot;sharpshootR&quot;, &quot;soiltexture&quot;, # gis &quot;rgdal&quot;, &quot;raster&quot;, &quot;sp&quot;, &quot;sf&quot;, &quot;terra&quot;, &quot;gdalUtilities&quot;, &quot;rgrass7&quot;, &quot;RSAGA&quot;, &quot;exactextractr&quot;, &quot;fasterize&quot;, # data management &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;devtools&quot;, &quot;roxygen2&quot;, &quot;Hmisc&quot;, &quot;circular&quot;, &quot;DT&quot;, &quot;remotes&quot;, # databases &quot;DBI&quot;, &quot;odbc&quot;, &quot;RSQLite&quot;, &quot;RODBC&quot;, # graphics &quot;ggplot2&quot;, &quot;latticeExtra&quot;, &quot;maps&quot;, &quot;spData&quot;, &quot;tmap&quot;, &quot;kableExtra&quot;, &quot;corrplot&quot;, &quot;farver&quot;, &quot;mapview&quot;, &quot;ggmap&quot;, &quot;plotrix&quot;, &quot;rpart.plot&quot;, &quot;visreg&quot;, &quot;diagram&quot;, &quot;GGally&quot;, &quot;wesanderson&quot;, # modeling &quot;car&quot;, &quot;rms&quot;, &quot;randomForest&quot;, &quot;ranger&quot;, &quot;party&quot;, &quot;caret&quot;, &quot;vegan&quot;, &quot;ape&quot;, &quot;shape&quot;, &quot;modEvA&quot;, &quot;gower&quot;, # sampling &quot;clhs&quot;, &quot;spcosa&quot; ) # ipkCRAN: a helper fuction for installing required packages from CRAN source(&quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/ipkCRAN.R&quot;) ## - p: vector of package names ## - up: logical - upgrade installed packages? Default: TRUE ## up = TRUE to download all packages ## up = FALSE to download only packages you don&#39;t already have installed ipkCRAN(p = packages, up = TRUE) The ipkCRAN function will let you know if any of the above packages fail to install. Whenever you run some code always check the console output for warnings and errors before continuing. It may be easiest to send commands individually to learn about and inspect their output, rather than running the entire file and wondering where an error occurred. 0.6 Dealing With Errors If a lot of output is produced by a command you should scroll up and sift through it as best you can. Copy and paste parts of the error message to use in internet searches, and try to find cases where folks have encountered problems. 0.6.1 No output is produced after pasting into console If you do not have a new command prompt (&gt;) and a blinking cursor on the left hand side of your console, but instead see a + after you run a command, R may think you are still in the middle of submitting input to the “read-eval-print-loop” (REPL). If this is not expected you are possibly missing closing quotes, braces, brackets or parentheses. R needs to know you were done with your expression, so you may need to supply some input to get the command to be complete. Pasting code line-by-line is useful but prone to input errors with multi-line expressions. Alternately, you can run commands or an entire file using the GUI or keyboard shortcuts such as Ctrl+Enter. You have a chance to try this in the example at the end. 0.6.2 ‘SOMEPACKAGE’ is not available (for R version X.Y.Z) This means either: A package named ‘SOMEPACKAGE’ exists but it is not available for your version of R CRAN does not have a package with that name You can try again, but first check for spelling and case-sensitivity. When in doubt search the package name on Google or CRAN to make sure you have it right. Note that not all R packages are available on CRAN: there are many other ways that you can deliver packages (including GitHub described below). 0.7 Packages not on CRAN Some R packages rely on compiled code. Windows users are limited to installing “binary” versions of such packages from CRAN unless they have “Rtools” installed. The Rtools software is available from the Software Center, and is specific to the version of R you have. One way to get the latest binary builds of R packages that use compiled code is by using https://r-universe.dev. This website provides custom repositories that can be used in addition to the defaults in install.packages() Install raster-related “rspatial” packages from r-universe.dev: install.packages(c(&#39;terra&#39;, &#39;raster&#39;), repos=&#39;https://rspatial.r-universe.dev&#39;) Install most recent build of Rcpp to avoid possible bugs with the “garbage collector”. If prompted to install from source select “Yes”. install.packages(&quot;Rcpp&quot;, repos = &quot;https://RcppCore.github.io/drat&quot;) To install the latest version of packages from the Algorithms for Quantitative Pedology (AQP) suite off GitHub we use the remotes package. The AQP packages are updated much more frequently on GitHub than they are on CRAN. Generally, the CRAN versions (installed above) are the “stable” releases whereas the GitHub repositories have new features and bug fixes. remotes::install_github(&quot;ncss-tech/aqp&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/soilDB&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/sharpshootR&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/soilReports&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) 0.8 Connect Local NASIS Establish an ODBC connection to NASIS by following the directions at the following hyperlink (ODBC Connection to NASIS). Once you’ve successfully established a ODBC connection, prove it by loading your NASIS selected set with the site and pedon tables for any pedons from your local area. You only need a few pedons at a minimum for this demo – too many (say, &gt;20) will make the example profile plot cluttered. Paste the below code at the command prompt (&gt;) and press enter, as you did above. Or create a new R script (Main menu: File → New File → R Script) and paste code into the “Source” pane (script editor window). Then, click the Run button in the top-right corner of the Script Editor or use Ctrl+Enter to run code at the cursor location / any selected code. This will execute the code in the Console. Submit the resulting plot to your mentor (from “Plot” pane (bottom-right): Export → Save as PDF…) # load packages into the current session library(aqp) # provides &quot;SoilProfileCollection&quot; object &amp; more library(soilDB) # provides database access methods # get pedons from NASIS selected set test &lt;- fetchNASIS(from = &#39;pedons&#39;) # inspect the result str(test, max.level = 2) # make a profile plot # set margins smaller than default par(mar=c(1,1,1,1)) # make profile plot of selected set, with userpedonid as label plot(test, label=&#39;pedon_id&#39;) 0.9 Proof Follow the one line example below, copy the output, and submit the results to your mentor. This will help us to verify that all of the required packages have been installed. # dump list of packages that are loaded into the current session sessionInfo() 0.10 Additional Support/Optional Readings Paul Finnel’s webinar Spatial Data Analysis and Modeling with R (highly recommended) R-Intro R for Beginners The R Inferno AQP Website and Tutorials Stats for Soil Survey Webinar Soil Data Aggregation using R Webinar "],["intro.html", "Chapter 1 Introduction to R 1.1 Outline 1.2 Course Overview 1.3 What is R? 1.4 RStudio: An Integrated Development Environment (IDE) for R 1.5 Rcmdr (R Commander): A Graphical User Interface for R 1.6 R basics 1.7 Managing Packages 1.8 Getting Help 1.9 Documenting your work 1.10 Organizing your work 1.11 Saving your work 1.12 Exercise 1 1.13 Loading Data 1.14 Data manipulation 1.15 Exercise 2 1.16 Review 1.17 Additional Reading (Introduction)", " Chapter 1 Introduction to R 1.1 Outline Course Overview Review Course Objectives Why is this training needed? Why is course organized this way? What is R? Why should I use R? What can R do? How do I get started? RStudio interface What are packages? How to navigate the Help tab How to save files Manipulating data Loading &amp; viewing data Filtering, transforming, merging, aggregating and reshaping data Exporting data 1.2 Course Overview 1.2.1 Course Objectives Develop solutions to investigate soil survey correlation problems and update activities. Evaluate investigations for interpretive results and determine how to proceed. Summarize data for population in NASIS. Analyze spatial data to investigate soil-landscape relationships Help to pursue the question “why” 1.2.2 Why is this training needed? Long standing goal of the Soil Science Division to have a course in statistics (Mausbach 2003) Opportunities to learn these techniques are limited, especially at the undergraduate level (Hennemann and Rossiter 2004) Consistent methodology (data analysis, data population, sampling design, etc.) There is continually a greater need to use these techniques: Mapping of lands at high production rates ((MacMillan, Moon, and Coupé 2007); (Kempen et al. 2012); (Brevik et al. 2016)) Ecological Sites (Maynard et al. 2019) Soil survey refinement (disaggregation) (Chaney et al. (2016)];(Ramcharan et al. 2018)) 1.2.3 Why is course organized this way? Our best judgment for assembling into 24 hours what could be 6 University level courses Mixture of slides and script enabled web pages is new for NRCS The web content is a long-term investment and should serve as a permanent reference Feel free to provide guidance for improving the class for future offerings 1.3 What is R? R is a free, open-source software and programming language developed in 1995 at the University of Auckland as an environment for statistical computing and graphics (Ihaka and Gentleman 1996). Since then R has become one of the dominant software environments for data analysis and is used by a variety of scientific disiplines, including soil science, ecology, and geoinformatics (Envirometrics CRAN Task View; Spatial CRAN Task View). R is particularly popular for its graphical capabilities, but it is also prized for it’s GIS capabilities which make it relatively easy to generate raster-based models. More recently, R has also gained several packages which are designed specifically for analyzing soil data. a software environment: statistics graphics programming calculator GIS a language to explore, summarize, and model data functions = verbs objects = nouns 1.3.1 Why Should I Learn R? While the vast majority of people use Microsoft Excel for data analysis, R offers numerous advantages, such as: Cost. R is free! (“Free as in free speech, not free beer.”) Reproducible Research (self-documenting, repeatable) repeatable: code + output in a single document (‘I want the right answer, not a quick answer’ - Paul Finnell) easier the next time (humorous example) numerous Excel horror stories of scientific studies gone wrong exist (TED Talk) scalable: applicable to small or large problems R in a Community Numerous Discipline Specific R Groups Numerous Local R User Groups (including R-Ladies Groups) Stack Overflow Learning Resources (quantity and quality) R books (Free Online) R Books R is ‘becoming’ the new norm (paradigm shift?) “If we don’t accept these challenges, others who are less qualified will; and soil scientists will be displaced by apathy.” (Arnold and Wilding 1991) While some people find the use of a commandline environment daunting, it is becoming a necessary skill for scientists as the volume and variety of data has grown. Thus scripting or programming has become a third language for many scientists, in addition to their native language and disipline specific terminology. Other popular programming languages include: SQL (i.e. NASIS), Python (i.e. ArcGIS), and JavaScript. ODBC and GDAL link R to nearly all possible formats/interfaces 1.3.2 What can R do? 1.3.3 Packages Base R (functionality is extended through packages) basic summaries of quantitative or qualitative data data exploration via graphics GIS data processing and analysis Soil Science R Packages aqp - visualization, aggregation, classification soilDB - access to commonly used soil databases soilReports - handful of report templates soiltexture - textural triangles Ecology R packages vegan - ordination, diversity analysis, etc. dismo - species distribution modeling 1.3.3.1 Soil Science Applications 1.3.3.1.1 Create Maps 1.3.3.1.2 Draw Soil Profiles 1.3.3.1.3 Draw Depth Plots 1.3.3.1.4 Estimate the Range in Characteristics (RIC) variable genhz pct10 median pct90 clay A 13 16 22 clay BAt 16 19 25 clay Bt1 18 24 32 clay Bt2 22 30 44 clay Cr 15 15 15 phfield A 6 6 7 phfield BAt 5 6 6 phfield Bt1 5 6 7 1.4 RStudio: An Integrated Development Environment (IDE) for R RStudio is an integrated development environment (IDE) that allows you to interact with R more readily. RStudio is similar to the standard RGui, but is considerably more user friendly. It has more drop-down menus, windows with multiple tabs, and many customization options. The first time you open RStudio, you will see three windows. A forth window is hidden by default, but can be opened by clicking the File drop-down menu, then New File, and then R Script. Detailed information on using RStudio can be found at at RStudio’s Website. RStudio Windows / Tabs Location Description Console Window lower-left location were commands are entered and the output is printed Source Tabs upper-left built-in text editor Environment Tab upper-right interactive list of loaded R objects History Tab upper-right list of key strokes entered into the Console Files Tab lower-right file explorer to navigate C drive folders Plots Tab lower-right output location for plots Packages Tab lower-right list of installed packages Help Tab lower-right output location for help commands and help search window Viewer Tab lower-right advanced tab for local web content 1.5 Rcmdr (R Commander): A Graphical User Interface for R While we recommend the use of RStudio for some of the reasons listed above, many people new to R (or infrequent users) might benefit from a graphical user interface (GUI) that allows the user to run basic functions using a point and click interface. Luckily for beginners R has the R Commander (Rcmdr) GUI, which is similiar to JMP. Rcmdr was created by John Fox for his introductory statistics students so they could see how the software worked without learning a large number of function names and arguments. Rcmdr is a great way to begin familiarizing yourself with R and statistics within a GUI environment. Regrettably, we know of no GUI that allows users to perform the majority of soil survey applications demonstrated in this course, and thus Rcmdr will not be covered. For those who wish to pursue Rcmdr, alternative instructions can be viewed at Andy Chang &amp; G. Jay Kerns website. To take a quick peak at Rcmdr, it can be opened by entering the following command into the R console. install.packages(&quot;Rcmdr&quot;) library(Rcmdr) 1.6 R basics R is command-line driven. It requires you to type or copy-and-paste commands after a command prompt (&gt;) that appears when you open R. This is called the “Read-Eval-Print-Loop” or REPL. After typing a command in the R console and pressing Enter on your keyboard, the command will run. If your command is not complete, R issues a continuation prompt (signified by a plus sign: +). R is case sensitive. Make sure your spelling and capitalization are correct. Commands in R are also called functions. The basic format of a function in R is: object &lt;- function.name(argument_1 = data, argument_2 = TRUE). The up arrow (^) on your keyboard can be used to bring up previous commands that you’ve typed in the R console. Any text that you do not want R to act on (such as comments, notes, or instructions) needs to be preceded by the # symbol (a.k.a. hash-tag, comment, pound, or number symbol). R ignores the remainder of the script line following #. # Math 1 + 1 10 * 10 log10(100) # combine values c(1, 2, 3) # Create sequence of values 1:10 # Implicit looping 1:10 * 5 1:10 * 1:10 # Assignment and data types ## numeric clay &lt;- c(10, 12, 15, 26, 30) ## character subgroup &lt;- c(&quot;typic haplocryepts&quot;,&quot;andic haplocryepts&quot;,&quot;typic dystrocryepts&quot;) ## logical andic &lt;- c(FALSE, TRUE ,FALSE) # Print print(clay) subgroup 1.7 Managing Packages Packages are collections of additional functions that can be loaded on demand. They commonly include example data that can be used to demonstrate those functions. Although R comes with many common statistical functions and models, most of our work requires additional packages. 1.7.1 Installing Packages To use a package, you must first install it and then load it. These steps can be done at the command line or using the Packages Tab. Examples of both approaches are provided below. R packages only need to be installed once (until R is upgraded or re-installed). Every time you start a new R session, however, you need to load every package that you intend to use in that session. Within the Packages tab you will see a list of all the packages currently installed on your computer, and 2 buttons labeled either “Install” or “Update”. To install a new package simply select the Install button. You can enter install one or more than one packages at a time by simply separating them with a comma. To find out what packages are installed on your computer, use the following commands: library() # or installed.packages() One useful package for soil scientists is the soiltexture package. It allows you to plot soil textural triangles. The following command shows how to install this package if you do not currently have it downloaded: # CRAN (static version) install.packages(c(&quot;soiltexture&quot;)) # GitHub (development version) remotes::install_github(&quot;julienmoeys/soiltexture/pkg/soiltexture&quot;, dependencies = FALSE, upgrade = FALSE, build = FALSE) 1.7.2 Loading Packages Once a package is installed, it must be loaded into the R session to be used. This can be done by using library(). The package name does not need to be quoted. library(soilDB) You can also load packages using the Packages Tab, by checking the box next to the package name. For example, documentation for the soilDB package is available from the help() function. help(package = &quot;soilDB&quot;) 1.8 Getting Help R has extensive documentation, numerous mailing lists, and countless books (many of which are free and listed at end of each chapter for this course). To learn more about the function you are using and the options and arguments available, learn to help yourself by taking advantage of some of the following help functions in RStudio: Use the Help tab in the lower-right Window to search commands (such as hist) or topics (such as histogram). Type help(read.csv) or ?read.csv in the Console window to bring up a help page. Results will appear in the Help tab in the lower right-hand window. Certain functions may require quotations, such as help(\"+\"). # Help file for a function help(read.csv) # or ?read.csv # Help files for a package help(package = &quot;soiltexture&quot;) 1.9 Documenting your work RStudio’s Source Tabs serve as a built-in text editor. Prior to executing R functions at the Console, commands are typically written down (or scripted). Scripting is essentially showing your work. The sequence of functions necessary to complete a task are scripted in order to document or automate a task. While scripting may seems cumbersome at first, it ultimately saves time in the long run, particularly for repetitive tasks (humorous YouTube Video on Scripting). Benefits include: allows others to reproduce your work, which is the foundation of science serves as instruction/reminder on how to perform a task allows rapid iteration, which saves time and allows the evaluation of incremental changes reduces the chance of human error 1.9.1 Basic Tips for Scripting To write a script, simply open a new R script file by clicking File&gt;New File&gt;R Script. Within the text editor type out a sequence of functions. Place each function (e.g. read.csv()) on a separate line. If a function has a long list of arguments, place each argument on a separate line. A command can be excuted from the text editor by placing the cursor on a line and typing Crtl + Enter, or by clicking the Run button. An entire R script file can be excuted by clicking the Source button. 1.10 Organizing your work When you first begin a project you should create a new folder and place within it all the data and code associated with the project. This simplifies the process of accessing your files from R. Using a project folder is also a good habit because it makes it easier to pickup where you left off and find your data if you need to come back to it later. Within R, your project folder is also known as your working directory. This directory will be the default location your plots and other R output are saved. You want to have inputs for your code in the working directory so that you can refer to them using relative file paths. Relative file paths make it easier if you move the folder containing your script(s) around. Or, if you share it with someone else, they will have little issue getting your code to work on their own file system. 1.10.1 Setting the Working Directory Before you begin working in R, you should set your working directory to your project folder; for example, setwd(\"C:\\\\workspace2\\\\projectx...\"). You can use RStudio to manage your projects and folders. NOTE: Beware when specifying any file paths that R uses forward slashes / instead of back slashes \\. Back slashes are reserved for use as an escape character, so you must use two of them to get one in result character string. To change the working directory in RStudio, select main menu Session &gt;&gt; Set Working Directory &gt;&gt; …. Or, from the “Files” tab click More &gt;&gt; Set As Working Directory to use the current location of the “Files” tab as your working directory. Setting the working directory can also be done via the Console with the setwd() command: setwd(&quot;C:/workspace2&quot;) To check the file path of the current working directory (which should now be \"C:\\\\workspace2\"), type: getwd() 1.10.2 RStudio Projects (.Rproj files) You can also manage your working directory using RStudio Projects. An RStudio Project file (.Rproj) is analogous to, for example, a .mxd file for ArcMap. It contains information about the specific settings you may have set for a “project”. You open or create projects using the drop down menu in the top right-hand corner of the RStudio window (shown below) RStudio Project Menu Here is what a typical Project drop-down menu looks like: RStudio Project Menu (expanded) You can create new projects from existing or new directories with “New Project…”. When you click “Open Project…”, your working directory is automatically set to the .Rproj file’s location – this is extremely handy Any projects you have created/used recently will show up in the “Project List” 1.11 Saving your work In R, you can save several types of files to keep track of the work you do. The file types include: workspace, script, history, and graphics. It is important to save often because R, like any other software, may crash periodically. Such problems are especially likely when working with large files. You can save your workspace in R via the command line or the File menu. 1.11.0.1 R script (.R) An R script is simply a text file of R commands that you’ve typed. You may want to save your scripts (whether they were written in R Editor or another program such as Notepad) so that you can reference them in the future, edit them as needed, and keep track of what you’ve done. To save R scripts in RStudio, simply click the save button from your R script tab. Save scripts with the .R extension. R assumes that script files are saved with only that extension. If you are using another text editor, you won’t need to worry about saving your scripts in R. You can open text files in the RStudio text editor, but beware copying and pasting from Word files as discussed below. To open an R script, click the file icon. 1.11.0.2 Microsoft Word Files Using Microsoft Word to write or save R scripts is generally a bad idea. Certain keyboard characters, such as quotations ““, are not stored the same in Word (e.g. they are”left” and “right” handed). The difference is hard to distinguish, but will not run in R. Also, pasting your R code or output into Wword documents manually is not reproducible, so while it may work in a pinch, it ultimately costs you time. You can use the word_document Rmarkdown template to automatically “Knit” .docx files from R code using a template, which is very handy for quickly getting a nice looking document! 1.11.0.3 R Markdown (.Rmd) R Markdown (.Rmd) documents contain information for the reproducible combination of narrative text and code to produce elegantly formatted output. You can use multiple languages in .Rmd documents including R, Python, and SQL. You can easily “knit” visually appealing and high-quality documents into rich HTML, PDF or Word documents from the RStudio interface. This document is made in bookdown, a variant of rmarkdown used for book templates involving multiple chapters. You can make blogs and websites for your R packages with blogdown and pkgdown. These are all tools based off of the powerful “pandoc” engine and the tools in the R Markdown ecosystem. 1.11.0.4 R history (.Rhistory) An R history file is a copy of all your key strokes. You can think of it as brute force way of saving your work. It can be useful if you didn’t document all your steps in an R script file. Like an R file, an Rhistory file is simply a text file that lists all of the commands that you’ve executed. It does not keep a record of the results. To load or save your R history from the History Tab click the Open File or Save button. If you load an Rhistory file, your previous commands will again become available with the up-arrow and down-arrow keys. You can also use the command line to load or save your history. savehistory(file = &quot;sand.Rhistory&quot;) loadhistory(file = &quot;sand.Rhistory&quot;) history(max.show=Inf) #displays all previous commands 1.11.0.5 R Graphics Graphic outputs can be saved in various formats. Format Function pdf pdf(“graphic.pdf”) window metafile win.metafile(“graphic.wmf”) png png(“graph.png”) jpeg jpeg(“graph.jpg”) bmp bmp(“graph.bmp”) postscript postscript(“graph.ps”) To save a graphic: (1) Click the Plots Tab window, (2) click the Export button, (3) Choose your desired format, (3) Modify the export settings as you desire, and (4) click Save. The R command for saving a graphic is: png(file = &quot;npk_yield.png&quot;) plot(npk$yield) dev.off() The first line of this command creates a blank file named sand with a JPEG extension. The second line plots the data object that you want to create a graphic of (here it is conveniently the same name as the JPEG file we are creating). The third line closes the graphics device. 1.12 Exercise 1 Using the examples discussed thus far as a guide, demonstrate your mastery of the material by performing the following tasks. Create an R script file, demonstrate 3 basic R functions, and comment (#) your code. Install the FedData R package from CRAN and GitHub. Save the commands in your R script file. Load the FedData R package and read the help file for the get_ssurgo function within the FedData package. What is the 1st input/argument? Save the R command in your R script. Save your R script, and forward to your instructor. 1.13 Loading Data R can load a variety of data formats, however tabular data is by far the most common, and what we will spend of the majority of our time working with. Typically tabular data is stored in spreadsheets (e.g. .txt, .csv, .xlsx), databases (e.g. NASIS), or webpages (.html). Within R tabular data is stored as a data.frame. 1.13.0.1 Text files Text files are a preferable format for storing and transferring small datasets. One basic command for importing text files into R is read.csv(). The command is followed by the file name or URL and then some optional instructions for how to read the file. These files can either be imported into R by clicking the Import Dataset &gt;&gt; From Text buttons from the Environment tab, or by typing the following command into the R console: # from working directory sand &lt;- read.csv(&quot;C:/workspace2/sand_example.csv&quot;) # from URL sand &lt;- read.csv(&quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/sand_example.csv&quot;) 1.13.0.2 Excel files R can import Excel files, but generally speaking it is a bad idea to use Excel. Excel has a dangerous default which automatically converts data with common notations to their standard format without warning or notice. For example, the character “11-JUN” entered into a cell automatically becomes the date 6/11/2021, even though the data is still displayed as 11-JUN. The only way to avoid this default behavior is to manually import your data into Excel via the Data Tab&gt;Get External Data Ribbon, and manually set the data type of all your columns to text. Failure to do so has resulted in numerous retracted research articles (Washington Post Article). Warnings aside, Excel files are a very common and are a format most people are familiar with. Therefore we will illustrate how to bring them into R. Download the sand Excel dataset from GitHub at https://github.com/ncss-tech/stats_for_soil_survey/blob/master/data/Pre-course/R_sand/sand_example.xlsx Excel datasets can either be imported into R by clicking the Import Dataset &gt;&gt; From Excel buttons from the Environment tab, or by typing the following command into the R console: library(readxl) sand_example &lt;- read_excel(&quot;sand_example.xlsx&quot;) 1.13.0.3 NASIS (Web) Reports NASIS provides a plethora of reports, many of which can be read into R for analysis. The soilDB R package provides a series of functions to read data from NASIS either using a local database connection or via HTML web reports. Similar functions also exist for accessing tabular data from Soil Data Access. More details on soilDB will be provided in the next chapter, but now we’ll illustrate how to access some example datasets for manipulating tabular data. library(soilDB) # get projects prj &lt;- get_project_from_NASISWebReport(mlrassoarea = &quot;11-IND&quot;, fiscalyear = 2020) # get legends leg &lt;- get_legend_from_NASISWebReport(mlraoffice = &quot;Indi%&quot;, areasymbol = &quot;%&quot;) # get map units mu &lt;- get_mapunit_from_NASISWebReport(areasymbol = c(&quot;IN001&quot;, &quot;IN11%&quot;)) 1.14 Data manipulation Before we can do any sort of analysis, analysis, our data often needs to be manipulated one way or another. Estimates vary, but an analyst typically spend 80% of their time manipulating data, and only 20% actually analyzing or modeling. Tasks generally involve filtering, transforming, merging, aggregating, and reshaping data. R has many functions and packages for manipulating data frames, but within the past several years a family of packages, known as the tidyverse, have been developed to simplify interacting with data frames (or tibbles). Within the tidyverse the most commonly used packages are dplyr and tidyr. Many of the tidyverse function names are patterned after SQL syntax. We will review the most common functions you need to know in order to accomplish the majority of data manipulation tasks. 1.14.1 Viewing and Removing Data Once a file is imported, it is imperative that you check to ensure that R correctly imported your data. Make sure numerical data are correctly imported as numerical, that your column headings are preserved, etc. To view the data simply click on the mu dataset listed in the Environment tab. This will open up a separate window that displays a spreadsheet like view. Additionally you can use the following functions to view your data in R. Function Description print() prints the entire object (avoid with large tables) head() prints the first 6 lines of your data str() shows the data structure of an R object names() lists the column names (i.e., headers) of your data ls() lists all the R objects in your workspace directory Try entering the following commands to view the mu dataset in R: str(mu) names(mu) head(mu) ls() A data object is anything you’ve created or imported and assigned a name to in R. The Environment tab allows you to see what data objects are in your R session and expand their structure. Right now sand should be the only data object listed. If you wanted to delete all data objects from your R session, you could click the broom icon from the Environments tab. Otherwise you could type: # Remove all R objects rm(list = ls(all = TRUE)) # Remove individual objects rm(mu, leg, sand) 1.14.2 Filtering or Subsetting Data When analyzing data in NASIS, filtering is typically accomplished by loading your selected set with only the records you’re interested in. However, it is often useful or necessary to subset your data after it’s loaded. This can allow you to isolate interesting records within large datasets. For these reasons R has numerous options/functions for filtering data. Data frames can be filtered by both columns and rows, using either names, position (e.g. column 1, row 5), or logical indices (e.g. TRUE/FALSE). Another particularly useful feature is the use of pattern matching which uses regular expressions to select data, which is similar to the LIKE statement from SQL. **Filtering with names and numerical indices # Filtering with names mu$areasymbol # select column names using $ mu[, c(&quot;areasymbol&quot;, &quot;musym&quot;)] # select column names using [] mu[c(&quot;1&quot;, &quot;2&quot;), ] # select row names using [] mu[c(&quot;1&quot;, &quot;2&quot;), c(&quot;areasymbol&quot;, &quot;musym&quot;)] # select column and row names using [] # Filtering by position mu[1, ] # select first row mu[, 1] # select first column mu[2, 2] # select second row and second column mu[c(1, 2, 3), ] # select multiple rows mu[c(-1, -2), ] # drop multiple rows Logical Operators == R uses a double equal sign as “equal-to” in SQL != “Not-equal-to” &lt;, &gt;, &lt;=, &gt;= Less than, greater than, less than or equal to, and greater than or equal &amp; Equivalent to AND in SQL and Soil Taxonomy, must match both conditions | Equivalent to OR in SQL and Soil Taxonomy, must match at least one condition %in% Equivalent to IN () in SQL (e.g. mu$areasymbol %in% c(\"IN001\", \"IN111\") grepl() equivalent to LIKE in SQL (e.g. grepl(\"IN%\", mu$areasymbol)) Filtering with logicals # Standard evaluation with base R [] # Filtering with logicals mu[mu$areasymbol == &quot;IN001&quot;, ] # select rows that equal IN001 mu[mu$areasymbol != &quot;IN001&quot;, ] # select rows that do not equal IN001 mu[, names(mu) == &quot;areasymbol&quot;] # select columns that equal areasymbol mu[, names(mu) %in% c(&quot;areasymbol&quot;, &quot;musym&quot;)] # select columns that match areasymbol and musym mu[grepl(&quot;Miami&quot;, mu$muname), ] # select rows that contain Miami # Non-standard evaluation with tidyverse library(dplyr) # Filtering rows filter(mu, areasymbol == &quot;IN001&quot;) filter(mu, areasymbol != &quot;IN001&quot;) filter(mu, areasymbol %in% c(&quot;IN001&quot;, &quot;IN111&quot;)) filter(mu, grepl(&quot;Miami&quot;, muname)) filter(mu, muacres &gt; 0) # Select columns select(mu, areasymbol, musym) # Slice rows slice(mu, 1:5) 1.14.3 Transforming Data This allows you to create new columns by convert, compute, or combine data within existing columns. mu &lt;- mutate(mu, # convert to hectares muhectares = muacres * 0.4047, # convert muname to TRUE or FALSE if Miami is present using pattern matching miami = grepl(&quot;Miami&quot;, muname), # compute % minor component n_minor = n_component - n_majcompflag, # combine columns key = paste(areasymbol, musym) ) 1.14.4 Sorting Data Sorting allows you to rearrange your data. Beware R has several similar functions (e.g. sort and order) for sorting data only work with specific datatypes. The tidyverse function arrange is designed to work with data frames. # sort ascending arrange(mu, areasymbol, muname) # sort descending arrange(mu, desc(areasymbol), desc(muname)) 1.14.5 Piping Data Another particularly useful feature provided by the magrittr package and used in the tidyverse is the use of pipe (%&gt;%). Base R also has a native pipe operator (|&gt;). Using the RStudio keyboard shortcut Ctrl + Shift + M inserts the pipe you have selected as default in Global Options &gt; Code. f(x,y) becomes x %&gt;% f(y) The “pipe” is something that occurs in many programming languages and computing contexts. It allows output from one expression to be passed as input to the first argument of the next function. This allows sequences of commands to be read from right to left b(or top to bottom) rather than from the inside out. # non-piping example 1 mu_sub &lt;- filter(mu, areasymbol == &quot;IN001&quot;) mu_sub &lt;- mutate(mu_sub, pct_100less = pct_component &lt; 100) # non-piping example 2 mu_sub &lt;- mutate(filter(mu, areasymbol == &quot;IN001&quot;), pct_100less = pct_component &lt; 100) # piping mu_sub &lt;- mu %&gt;% filter(areasymbol == &quot;IN001&quot;) %&gt;% mutate(pct_100less = pct_component &lt; 100) 1.14.6 Merging/Joining or Combining Data ** Joining** When working with tabular data you often have 2 or more tables you need to join. There are several ways to join tables. Which direction to join and which columns to join will determine how you achieve the join. # inner join leg_mu &lt;- inner_join(leg, mu, by = c(&quot;liid&quot;, &quot;areasymbol&quot;)) # left join leg_mu &lt;- left_join(leg, mu, by = c(&quot;liid&quot;)) # right_join leg_mu &lt;- right_join(leg, mu, by = &quot;liid&quot;) ** Combining** If your tables have the same structure (e.g. columns), or length and order you may simply combine them. For example, if you have two different mapunit tables. # combine rows rbind(mu, mu) rbind(mu, leg) # won&#39;t work # combine columns cbind(mu, mu) # beware combine tables with duplicate column names cbind(mu, areasymbol_2 = mu$areasymbol) cbind(mu, leg) # won&#39;t work 1.14.7 Aggregating or Grouping Data Because soil data has multiple dimensions (e.g. properties and depths) and levels of organization (e.g. many to one relationships), it is often necessary to aggregate it. For example, when we wish to make a map we often need to aggregate over components and then map units. Depending on the data type this aggregation may involve taking a weighted average or selecting the dominant condition. The group_by function defines the groups over which we wish to summarize the data. mu_agg &lt;- mu %&gt;% group_by(grpname, areasymbol) %&gt;% summarize(sum_muacres = sum(muacres), n_musym = length(musym) ) 1.14.8 Reshaping Data Typically data is stored in what is known as a wide format, where each column contains a different variable (e.g. depth, clay, sand, rocks). However, sometimes it is necessary to reshape or pivot to a long format, where each variable/column is compressed into 2 new rows. One new column contains the old column names, while another new column contains the values from the old columns. This is particularly useful when combining multiple variables into a single plot. library(tidyr) # Simplify mu example dataset mu2 &lt;- mu %&gt;% select(grpname, areasymbol, musym, muacres, n_component, pct_hydric) %&gt;% slice(1:5) print(mu2) # Pivot long mu2_long &lt;- pivot_longer(mu2, cols = c(muacres, n_component, pct_hydric)) print(mu2_long) # Pivot wide mu2_wide &lt;- pivot_wider(mu2_long, names_from = name) print(mu2_wide) 1.14.9 Exporting Data To export data from R, use the command write.csv() or write.dbf() functions. Since we have already set our working directory, R automatically saves our file into the working directory. write.csv(mu_agg, file = &quot;mu_agg.csv&quot;) library(foreign) write.dbf(as.data.frame(mu_agg), file = &quot;mu_agg.dbf&quot;) 1.15 Exercise 2 Create a new R script file. To get information from the NASIS legend table for the state of Wisconsin use the soilDB function get_legend_from_NASISWebReport() for mlraoffice = \"%\" and areasymbol = \"WI% Filter the legend table for rows where the ssastatus == \"Update needed\" to find the soil survey areas that need update. Inspect the result to find the areasymbol values. Load the mapunit table, using soilDB get_mapunit_from_NASISWebReport() using the area symbols you identified in step 3. Calculate the acreage of hydric soils for each mapunit by multiplying muacres by pct_hydric. Note: pct_hydric is a percentage, not a proportion. Aggregate the total acreage of hydric soils each soil survey area using dplyr functions group_by() and summarize(). Join the aggregated mapunit table from Step 6 to the legend table from Step 3 using dplyr left_join(). Calculate the proportion of the total soil survey area acres (areaacres) that are hydric soils. Answer the following questions: What soil survey areas need update in Wisconsin? What proportion of those soil survey areas are hydric soils? Bonus: How does your joined result in Step 7 differ if you replace dplyr left_join() with inner_join()? Why? Save your R script and forward to your instructor. 1.16 Review Given what you now know about R, try to answer the following questions: Can you think of a situation where an existing hypothesis or conventional wisdom was not repeatable? What are packages? What is GitHub? Where can you get help? What is a data frame? What are 3 ways you can manipulate a data frame? 1.17 Additional Reading (Introduction) Introductory R Books R for Data Science RStudio Cheatsheets Quick-R Advanced DSM R Books Predictive Soil Mapping with R Using R for Digital Soil Mapping (not free) Soil Spectral Inference with R (not free) GSP SOC Cookbook GSP SAS Manual Soil Science R Applications aqp and soilDB tutorials ISRIC World Soil Information Example Training Courses ISRIC World Soil Information YouTube Channel OpenGeoHub Courses OpenGeoHub YouTube Channel David Rossiter’s Cornell Homepage Pierre Roudier Soil Sciences and Statistics Review Articles Arkely, R., 1976. Statistical Methods in Soil Classification Research. Advances in Agronomy 28:37-70. https://www.sciencedirect.com/science/article/pii/S0065211308605520 Mausbach, M., and L. Wilding, 1991. Spatial Variability of Soils and Landforms. Soil Science Society of America, Madison. https://dl.sciencesocieties.org/publications/books/tocs/sssaspecialpubl/spatialvariabil Wilding, L., Smeck, N., and G. Hall, 1983. Spatial Variability and Pedology. In : L. Widling, N. Smeck, and G. Hall (Eds). Pedogenesis and Soil Taxonomy I. Conceps and Interactions. Elseiver, Amsterdam, pp. 83-116. https://www.sciencedirect.com/science/article/pii/S0166248108705993 References "],["data.html", "Chapter 2 The Data We Use 2.1 Objectives (Tabular Data Structures) 2.2 The Structure of Soil Data 2.3 Challenges with Pedon Data 2.4 The SoilProfileCollection 2.5 Using the soilDB Package 2.6 Working with Data in R 2.7 fetchNASIS data checks 2.8 Extended Data Functions 2.9 Custom Queries to Local NASIS Database", " Chapter 2 The Data We Use 2.1 Objectives (Tabular Data Structures) Learn more about R and how to inspect objects and data types Use the soilDB package to load NASIS pedon data into R Understand the structure of data stored in a SoilProfileCollection (SPC) Learn about the checks run on data loaded by the fetchNASIS() function Learn ways to logically filter and subset data in R Learn how functions can be used to bundle operations Review additional data that is accessible via extended data functions Introduce soilReports R package 2.2 The Structure of Soil Data What if you could extract, organize, and visualize data from NASIS and many other commonly used soil database sources with a couple of lines of code? The aqp (Algorithms for Quantitative Pedology) and soilDB packages enable data to be fetched from various sources and cast into a SoilProfileCollection(SPC) object. Tabular and spatial data objects fetched via soilDB and aqp methods can simplify the process of working with commonly used soil data. 2.2.1 Package References SoilProfileCollection Object Introduction Tutorials on the AQP website Package ‘aqp’ manual Package ‘soilDB’ manual Package ‘sharpshootR’ manual The manual pages for soilDB and aqp are accessible (click index at the bottom of the Help tab in RStudio) by entering the following into the R console: # load the libaries library(soilDB) library(aqp) # links to lots of great examples help(soilDB) help(aqp) 2.2.2 soilDB functions for tabular data soilDB functions are the quickest way to get up and running: fetchNASIS() Gets and re-packages data from a local NASIS database. NASIS pedon/horizon data NASIS DMU/MU/component data fetchVegdata() Gets Vegetation Plot and related/child tables into a list from a local NASIS database. fetchNASISLabData() Gets KSSL laboratory pedon/horizon layer data from a local NASIS database. fetchNASISWebReport() fetchKSSL() Gets KSSL data from the SoilWeb system via BBOX, MLRA, or series name query. KSSL Data Demo Water Retention Curve Development from KSSL Data fetchOSD() Fetches a limited subset of horizon- and site-level attributes for named soil series from the SoilWeb system. Querying Soil Series Data OSDquery() Full-text searching of OSD sections. Querying Soil Series Data fetchSCAN() Queries soil and climate data from USDA-NRCS SCAN Stations. A Unified Interface to SCAN/SNOTEL Data fetchHenry() Downloads data from the Henry Mount Soil Climate Database. Henry Mount Soil Climate Database Tutorial fetchSDA() Fetches legend/mapunit/component/horizon data from Soil Data Access. SDA_query() Can be used to access SSURGO, STATSGO (spatial and tabular), and Lab DataMart snapshots Submits queries to the Soil Data Access system. Soil Data Access Tutorial SDA and Spatial Data SDA and Interpretations The following are still functional but not actively maintained: fetchPedonPC() Fetches commonly used site and horizon data from a PedonPC v.5 database. fetchRaCA() Gets Rapid Carbon Assessment (RaCA) data by State, geographic bounding-box, RaCA site ID, or series query from the SoilWeb system. RaCA Data Demo 2.2.3 Importance of Pedon Data The importance of pedon data for present and future work cannot be overstated. These data represent decades of on-the-ground observations of the soil resource for a given area. As difficult as it may be to take the time to enter legacy pedon data, it is vitally important that we capture this resource and get these data into NASIS as an archive of point observations. 2.2.4 Some Issues With Pedon Data Making and documenting observations of soil requires hard work. Digging is difficult, and writing soil descriptions is time consuming! Our confidence in observations typically weakens with the depth of the material described. If we acknowledge this, which we must, then how do we deal with it in pedon data? Use a cutoff depth, for example 100 cm, can be used to truncate observations to a zone of greater confidence. Show the relative confidence of the data with depth. 2.3 Challenges with Pedon Data Consistency Missing data Confidence in the observations Uncertainty with depth Description style differences Depth described, horizonation usage styles Legacy data vintage Decadal span of data Taxonomy updates, horizon nomenclature changes Location confidence Origin of the location information Datum used for data collection Accuracy for GPS values at the time of data collection 2.3.1 Meeting the Challenges Graphical display of the data and summary outputs (slice-wise aggregation) Generalized Horizon Labels (GHL). Derive an aggregate soil profile and summarize soil properties for groups of similar soils. More on that process can be seen in the following tutorial: GHL Aggregation Presentation and GHL Aggregation Tutorial. For more information regarding difficult pedon data, see the following tutorial in the “aqp” package: Dealing with Troublesome data. 2.4 The SoilProfileCollection The SoilProfileCollection class (SPC) provided by the aqp package is a specialized structure for soil data analysis. It simplifies the process of working with collections of data associated with soil profiles, e.g., site-level data, horizon-level data, spatial data, diagnostic horizon data, metadata, etc. A SoilProfileCollection is similar to the NASIS Site/Pedon “object” in that it provides generalizations, specific routines and rules about data tables and their relationships. In many ways the SPC is more adaptable than the NASIS Pedon concept because it is more general. However, the SPC is not as expressive as the complex hierarchy of objects in NASIS, which are more aligned with data archival vs. analysis. 2.4.1 SoilProfileCollection methods Many “familiar” methods are defined for the SoilProfileCollection object. Some are unique, and others operate like more common functions of vector and data.frame objects, such as nrow() (“how many horizons?”) or length() (“how many sites/pedons?”). Perhaps most importantly, when you access the site() data or the horizons() data of a SoilProfileCollection, you get a data.frame object that you can use like any other you might use or make in R. 2.4.1.1 Promoting a data.frame to SoilProfileCollection The SoilProfileCollection object is a collection of 1-D profile descriptions, of the type conventionally described on a Form 232, or on tabular data returned from laboratory. The object is “horizon data forward” in that you start with that, and can create site-level attributes by normalization, joins, calculations and more. Most of the time if you are using your NASIS data, or an official database, there are defined ways of getting the data “into” an SPC. For example, fetchOSD returns a SoilProfileCollection that has been assembled from horizon and site level attributes gleaned from the OSDs text, Soil Classification database, and other sources. In the pre-course, we had you set up a process so you could connect to your local NASIS instance to “fetch” data and have methods like fetchNASIS put things together for you. This input to make a SoilProfileCollection can be represented most simply as a data.frame with unique site or profile ID and depth combinations for each horizon or layer–for example, a subset of the phorizon or chorizon table in NASIS. A simple demonstration of “tabular horizon data” is the sp4 data set bundled with aqp: some serpentine soil profiles stored in a data.frame in the aqp package (after McGahan et al., 2009). library(aqp) # Load sample serpentine soil data (McGahan et al., 2009) data(sp4, package = &quot;aqp&quot;) # this is a data.frame # same as if loaded from CSV file etc. class(sp4) ## [1] &quot;data.frame&quot; # inspect the first couple of rows head(sp4) ## id name top bottom K Mg Ca CEC_7 ex_Ca_to_Mg sand silt clay CF ## 1 colusa A 0 3 0.3 25.7 9.0 23.0 0.35 46 33 21 0.12 ## 2 colusa ABt 3 8 0.2 23.7 5.6 21.4 0.23 42 31 27 0.27 ## 3 colusa Bt1 8 30 0.1 23.2 1.9 23.7 0.08 40 28 32 0.27 ## 4 colusa Bt2 30 42 0.1 44.3 0.3 43.0 0.01 27 18 55 0.16 ## 5 glenn A 0 9 0.2 21.9 4.4 18.8 0.20 54 20 25 0.55 ## 6 glenn Bt 9 34 0.3 18.9 4.5 27.5 0.20 49 18 34 0.84 To convert this horizon data into a SoilProfileCollection, we need to identify three parameters: idname, top, and bottom. These parameters refer to the columns of unique profile IDs, top depths and bottom depths, respectively. There are a couple of important constraints and considerations: records (rows) represent horizons profiles are uniquely identified by a column (user pedon ID, pedon record ID, etc.) profiles IDs cannot contain missing values (NA) horizon top and bottom depths are identified by columns ideally there are no gaps, overlap, or missing top/bottom depths (more on that later) Use a formula to specify column names in the data.frame, in this case \"id\", \"top\" and \"bottom\". # profile ID ~ top depth + bottom depth depths(sp4) &lt;- id ~ top + bottom # note new class class(sp4) ## [1] &quot;SoilProfileCollection&quot; ## attr(,&quot;package&quot;) ## [1] &quot;aqp&quot; # compact summary sp4 ## SoilProfileCollection with 10 profiles and 30 horizons ## profile ID: id | horizon ID: hzID ## Depth range: 16 - 49 cm ## ## ----- Horizons (6 / 30 rows | 10 / 14 columns) ----- ## id hzID top bottom name K Mg Ca CEC_7 ex_Ca_to_Mg ## colusa 1 0 3 A 0.3 25.7 9.0 23.0 0.35 ## colusa 2 3 8 ABt 0.2 23.7 5.6 21.4 0.23 ## colusa 3 8 30 Bt1 0.1 23.2 1.9 23.7 0.08 ## colusa 4 30 42 Bt2 0.1 44.3 0.3 43.0 0.01 ## glenn 5 0 9 A 0.2 21.9 4.4 18.8 0.20 ## glenn 6 9 34 Bt 0.3 18.9 4.5 27.5 0.20 ## [... more horizons ...] ## ## ----- Sites (6 / 10 rows | 1 / 1 columns) ----- ## id ## colusa ## glenn ## kings ## mariposa ## mendocino ## napa ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] 2.4.1.2 Extracting Site and Horizon Data The SoilProfileCollection is an S4 R object. S4 objects have slots. Of primary importance, are the slots for site-level and horizon-level data. You can extract values from these slots using the site() and horizons() functions. These create data.frame objects that are separate from the SoilProfileCollection. # extract site data from SPC into new data.frame &#39;s&#39; # note that it only contains profile IDs s &lt;- site(sp4) str(s) ## &#39;data.frame&#39;: 10 obs. of 1 variable: ## $ id: chr &quot;colusa&quot; &quot;glenn&quot; &quot;kings&quot; &quot;mariposa&quot; ... # extract horizon data from SPC into new data.frame &#39;h&#39; h &lt;- horizons(sp4) str(h) ## &#39;data.frame&#39;: 30 obs. of 14 variables: ## $ id : chr &quot;colusa&quot; &quot;colusa&quot; &quot;colusa&quot; &quot;colusa&quot; ... ## $ name : chr &quot;A&quot; &quot;ABt&quot; &quot;Bt1&quot; &quot;Bt2&quot; ... ## $ top : int 0 3 8 30 0 9 0 4 13 0 ... ## $ bottom : int 3 8 30 42 9 34 4 13 40 3 ... ## $ K : num 0.3 0.2 0.1 0.1 0.2 0.3 0.2 0.6 0.8 0.6 ... ## $ Mg : num 25.7 23.7 23.2 44.3 21.9 18.9 12.1 12.1 17.7 28.3 ... ## $ Ca : num 9 5.6 1.9 0.3 4.4 4.5 1.4 7 4.4 5.8 ... ## $ CEC_7 : num 23 21.4 23.7 43 18.8 27.5 23.7 18 20 29.3 ... ## $ ex_Ca_to_Mg: num 0.35 0.23 0.08 0.01 0.2 0.2 0.58 0.51 0.25 0.2 ... ## $ sand : int 46 42 40 27 54 49 43 36 27 42 ... ## $ silt : int 33 31 28 18 20 18 55 49 45 26 ... ## $ clay : int 21 27 32 55 25 34 3 15 27 32 ... ## $ CF : num 0.12 0.27 0.27 0.16 0.55 0.84 0.5 0.75 0.67 0.25 ... ## $ hzID : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... 2.4.1.3 Methods like data.frame The base R functions for accessing and setting data.frame columns by name such as $ and [[ work for SoilProfileCollection objects, too. Review data.frame methods: [[ and $: single columns in data.frame, by name x[['variable']] x$variable [: combinations of rows and columns, by name or index x[i, ]: specified rows, all columns x[, j]: all rows, specified columns x[i, j]: specified rows, specified columns See Chapter 1 and the Chapter 2 Appendix for additional details and examples. 2.4.1.3.1 Column Access by Name: $ and [[ # sp4 is a SoilProfileCollection sp4$clay ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 sp4[[&#39;clay&#39;]] ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 # horizon data.frame h$clay ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 h[[&#39;clay&#39;]] ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 # use $&lt;- or [[&lt;- to set proportional clay content sp4$clay &lt;- sp4[[&#39;clay&#39;]] / 100 # undo what we did above; back to percentage sp4[[&#39;clay&#39;]] &lt;- sp4$clay * 100 # create new site variable (&quot;numberone&quot; recycled for all sites) site(sp4)$newvar1 &lt;- &quot;numberone&quot; # create new horizon variable (&quot;numbertwo&quot; recycled for all horizons) horizons(sp4)$newvar2 &lt;- &quot;numbertwo&quot; 2.4.1.3.2 Row Access: [ The SoilProfileCollection also has [ (“single bracket”), but with a different interpretation from the [i, j] indexing of data.frame objects. In a data.frame you have object[row, column, drop=TRUE]; the result is a data.frame (or a vector with default drop=TRUE). In a SoilProfileCollection you have object[site, horizon]; the result is a SoilProfileCollection. # i-index: first 2 profiles, all horizons sp4[1:2, ] ## SoilProfileCollection with 2 profiles and 6 horizons ## profile ID: id | horizon ID: hzID ## Depth range: 34 - 42 cm ## ## ----- Horizons (6 / 6 rows | 10 / 15 columns) ----- ## id hzID top bottom name K Mg Ca CEC_7 ex_Ca_to_Mg ## colusa 1 0 3 A 0.3 25.7 9.0 23.0 0.35 ## colusa 2 3 8 ABt 0.2 23.7 5.6 21.4 0.23 ## colusa 3 8 30 Bt1 0.1 23.2 1.9 23.7 0.08 ## colusa 4 30 42 Bt2 0.1 44.3 0.3 43.0 0.01 ## glenn 5 0 9 A 0.2 21.9 4.4 18.8 0.20 ## glenn 6 9 34 Bt 0.3 18.9 4.5 27.5 0.20 ## ## ----- Sites (2 / 2 rows | 2 / 2 columns) ----- ## id newvar1 ## colusa numberone ## glenn numberone ## ## Spatial Data: ## [EMPTY] # j-index: all profiles; first 2 horizons of each profile sp4[, 1:2] ## SoilProfileCollection with 10 profiles and 20 horizons ## profile ID: id | horizon ID: hzID ## Depth range: 5 - 40 cm ## ## ----- Horizons (6 / 20 rows | 10 / 15 columns) ----- ## id hzID top bottom name K Mg Ca CEC_7 ex_Ca_to_Mg ## colusa 1 0 3 A 0.3 25.7 9.0 23.0 0.35 ## colusa 2 3 8 ABt 0.2 23.7 5.6 21.4 0.23 ## glenn 5 0 9 A 0.2 21.9 4.4 18.8 0.20 ## glenn 6 9 34 Bt 0.3 18.9 4.5 27.5 0.20 ## kings 7 0 4 A 0.2 12.1 1.4 23.7 0.58 ## kings 8 4 13 Bt1 0.6 12.1 7.0 18.0 0.51 ## [... more horizons ...] ## ## ----- Sites (6 / 10 rows | 2 / 2 columns) ----- ## id newvar1 ## colusa numberone ## glenn numberone ## kings numberone ## mariposa numberone ## mendocino numberone ## napa numberone ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] When you use the [ function, everything in the SoilProfileCollection is subset simultaneously depending on the constraints specified by the indices. # First profile, first 2 horizons horizons(sp4[1, 1:2]) ## id name top bottom K Mg Ca CEC_7 ex_Ca_to_Mg sand silt clay CF hzID newvar2 ## 1 colusa A 0 3 0.3 25.7 9.0 23.0 0.35 46 33 21 0.12 1 numbertwo ## 2 colusa ABt 3 8 0.2 23.7 5.6 21.4 0.23 42 31 27 0.27 2 numbertwo All slots in the collection have a relationship to the site or i-index. When you remove sites (profiles), all associated records (e.g. spatial, diagnostics, horizons, etc.) in the object are removed. Similarly, when all horizons are removed (say, you request the 6th j-index from a profile that has only 5 layers), the site index and all associated data are removed from the collection. 2.4.2 Exercise: Assemble a SoilProfileCollection from several CSV files Link to exercise R code 2.5 Using the soilDB Package The soilDB package for R provides functions for accessing data stored in NASIS, KSSL, SDA, SoilWeb, SoilGrids and other sources. These high-level ‘fetch’ functions bundle or wrap lower-level get functions which access internal database interfaces to NASIS and other data sources. The ODBC connection to NASIS that you set up during the pre-course is an example of this internal database interface. Basic data checks are run within fetch functions. These checks ensure the basic integrity of the data as it is queried and moved from its existing structure into an SPC. There are times when it is useful to use the lower-level get functions individually. They generally return single data.frame or list of data.frame. You can set up scripts to make custom queries against these or other sources on your own – there is an example at the end of this section. For now, we will start with the fetch functions and others that will get you a large variety of data you can use for soil and ecological site analyses. 2.5.1 Open Database Connectivity (ODBC) Connection to NASIS After setting up an ODBC connection, you can use R to access data from a selected set defined in your local NASIS database. How to Create an ODBC Connection to local NASIS database for R. Does NASIS need to be open and running to query data using soilDB? No, fetchNASIS() works whether the NASIS application is running or not. You just need to make sure that the data you want has been loaded into your selected set. 2.5.2 fetchNASIS() The fetchNASIS() convenience function extracts data from a NASIS selected set via Structured Query Language (SQL). Note that the import process in fetchNASIS(), and the other methods, is not comprehensive. It does not pull every column for every table related to pedon data out of NASIS. Instead, it pulls essential / commonly used pedon and horizon data. Higher level functions like fetchNASIS() bundle a series of lower-level queries to get specific parts of the Pedon or Component data structures. Much of the nested complexity of NASIS is simplified in the resulting object. You may need to make more detailed queries and joins to resolve specific questions. Many-to-one relationships are “flattened” where possible by fetchNASIS(). This aggregates the data from various tables into one “site” record with related horizon records, per profile. You can see the child tables that are aggregated using the get_extended_data_from_NASIS() method, which returns a named list of child table sources that can be joined to the SoilProfileCollection made with fetchNASIS() using the internal record IDs. 2.5.2.1 fetchNASIS arguments fetchNASIS() has a number of different arguments: from = ‘pedons’ or ‘components’ or ‘pedon_report’ This option allows you to select which data you want to load from NASIS. Choosing either ‘pedons’ or ‘components’ will load data from your local database. If ‘pedon_report’ is specified then it will load data from the text file generated by the NASIS report ‘fetchNASIS’ (run offline). This is useful for loading more than 20,000 pedons at one time, such for an entire Soil Survey Region. url = string specifying the (temporary) URL for the NASIS pedon_report output generated by the fetchNASIS NASIS Report that can be run “Offline Against National Database” EXAMPLE OUTPUT (MT663) SS = TRUE/FALSE The Selected Set (SS) option allows you to choose whether you want the data to load from your current selected set in NASIS or from the local database tables. The default is set to TRUE so if unspecified fetchNASIS() will always load from the data in the selected set. stringAsFactors = TRUE/FALSE This option allows you to select whether to convert strings into factors or not. The default is set to FALSE, which will handle strings as character formats. Manually set this option to TRUE if you wish to handle character strings as factors. rmHzErrors = TRUE/FALSE Setting this value to TRUE (the default) enables checks for horizon depth consistency. Consider setting this argument to FALSE if you aren’t concerned about horizon-depth errors or if you know that your selected set contains many combination horizons (e.g., consisting of E/Bt horizons or similar two-part horizons described individually for the same depth range). Note that any pedons flagged as having horizon-depth errors (rmHzErrors = TRUE) are omitted from the data returned by fetchNASIS(). nullFragsAreZero = TRUE/FALSE Setting this value to TRUE (the default) converts null entries for rock fragment volumes to 0. This is typically the right assumption because rock fragment data are typically populated only when observed. If you know that your data contain a combination of omitted information (e.g. no rock fragment volumes are populated) then consider setting this argument to FALSE. soilColorState = ‘moist’ or ‘dry’ Select dry or moist colors to be converted and placed into a horizon-level attribute called soil_color. The default is set to ‘moist’ unless specified. Moist and dry colors are also stored in moist_soil_color and dry_soil_color. lab = TRUE/FALSE This option allows for loading the data associated with horizons that may be in the phlabresults table. The default is set to FALSE, which will not load records from the phlabresults table. For more information on the data checks and adjusting the default options to fetchNASIS() function, see the following resource: Tips on Getting Data from NASIS into R. 2.5.3 The gopheridge Dataset The gopheridge sample data set is a sample R object returned from fetchNASIS() in a self-contained .rda file stored in soilDB. Open RStudio, and set up the environment by loading packages and the gopheridge sample dataset. library(aqp) library(soilDB) # load example dataset data(gopheridge, package = &quot;soilDB&quot;) # what kind of object is this? class(gopheridge) ## [1] &quot;SoilProfileCollection&quot; ## attr(,&quot;package&quot;) ## [1] &quot;aqp&quot; # what does the internal structure look like? str(gopheridge, 2) ## Formal class &#39;SoilProfileCollection&#39; [package &quot;aqp&quot;] with 9 slots ## ..@ idcol : chr &quot;peiid&quot; ## ..@ hzidcol : chr &quot;phiid&quot; ## ..@ depthcols : chr [1:2] &quot;hzdept&quot; &quot;hzdepb&quot; ## ..@ metadata :List of 7 ## ..@ horizons :&#39;data.frame&#39;: 317 obs. of 73 variables: ## ..@ site :&#39;data.frame&#39;: 52 obs. of 137 variables: ## ..@ sp :Formal class &#39;SpatialPoints&#39; [package &quot;sp&quot;] with 3 slots ## ..@ diagnostic :&#39;data.frame&#39;: 164 obs. of 4 variables: ## ..@ restrictions:&#39;data.frame&#39;: 56 obs. of 8 variables: # the fields at the site and horizon levels within the SPC siteNames(gopheridge) ## [1] &quot;peiid&quot; &quot;siteiid&quot; ## [3] &quot;pedon_id&quot; &quot;site_id&quot; ## [5] &quot;obs_date&quot; &quot;utmzone&quot; ## [7] &quot;utmeasting&quot; &quot;utmnorthing&quot; ## [9] &quot;x&quot; &quot;y&quot; ## [11] &quot;horizdatnm&quot; &quot;x_std&quot; ## [13] &quot;y_std&quot; &quot;gpspositionalerror&quot; ## [15] &quot;describer&quot; &quot;pedonpurpose&quot; ## [17] &quot;pedontype&quot; &quot;pedlabsampnum&quot; ## [19] &quot;labdatadescflag&quot; &quot;tsectstopnum&quot; ## [21] &quot;tsectinterval&quot; &quot;utransectid&quot; ## [23] &quot;tsectkind&quot; &quot;tsectselmeth&quot; ## [25] &quot;elev_field&quot; &quot;slope_field&quot; ## [27] &quot;aspect_field&quot; &quot;plantassocnm&quot; ## [29] &quot;earthcovkind1&quot; &quot;earthcovkind2&quot; ## [31] &quot;erocl&quot; &quot;bedrckdepth&quot; ## [33] &quot;bedrckkind&quot; &quot;bedrckhardness&quot; ## [35] &quot;hillslopeprof&quot; &quot;geomslopeseg&quot; ## [37] &quot;shapeacross&quot; &quot;shapedown&quot; ## [39] &quot;slopecomplex&quot; &quot;drainagecl&quot; ## [41] &quot;flodfreqcl&quot; &quot;floddurcl&quot; ## [43] &quot;flodmonthbeg&quot; &quot;pondfreqcl&quot; ## [45] &quot;ponddurcl&quot; &quot;pondmonthbeg&quot; ## [47] &quot;geomposhill&quot; &quot;geomposmntn&quot; ## [49] &quot;geompostrce&quot; &quot;geomposflats&quot; ## [51] &quot;swaterdepth&quot; &quot;surface_fine_gravel&quot; ## [53] &quot;surface_gravel&quot; &quot;surface_cobbles&quot; ## [55] &quot;surface_stones&quot; &quot;surface_boulders&quot; ## [57] &quot;surface_channers&quot; &quot;surface_flagstones&quot; ## [59] &quot;surface_parafine_gravel&quot; &quot;surface_paragravel&quot; ## [61] &quot;surface_paracobbles&quot; &quot;surface_parastones&quot; ## [63] &quot;surface_paraboulders&quot; &quot;surface_parachanners&quot; ## [65] &quot;surface_paraflagstones&quot; &quot;surface_unspecified&quot; ## [67] &quot;surface_total_frags_pct_nopf&quot; &quot;surface_total_frags_pct&quot; ## [69] &quot;slope_shape&quot; &quot;surface_fgravel&quot; ## [71] &quot;classdate&quot; &quot;classifier&quot; ## [73] &quot;classtype&quot; &quot;taxonname&quot; ## [75] &quot;localphase&quot; &quot;taxonkind&quot; ## [77] &quot;seriesstatus&quot; &quot;taxclname&quot; ## [79] &quot;taxpartsize&quot; &quot;taxorder&quot; ## [81] &quot;taxsuborder&quot; &quot;taxgrtgroup&quot; ## [83] &quot;taxsubgrp&quot; &quot;soiltaxedition&quot; ## [85] &quot;osdtypelocflag&quot; &quot;taxmoistcl&quot; ## [87] &quot;taxtempregime&quot; &quot;taxfamother&quot; ## [89] &quot;taxreaction&quot; &quot;taxfamhahatmatcl&quot; ## [91] &quot;psctopdepth&quot; &quot;pscbotdepth&quot; ## [93] &quot;selection_method&quot; &quot;ecositeid&quot; ## [95] &quot;ecositenm&quot; &quot;ecositecorrdate&quot; ## [97] &quot;es_classifier&quot; &quot;es_selection_method&quot; ## [99] &quot;ochric.epipedon&quot; &quot;argillic.horizon&quot; ## [101] &quot;paralithic.contact&quot; &quot;lithic.contact&quot; ## [103] &quot;mollic.epipedon&quot; &quot;slickensides&quot; ## [105] &quot;calcic.horizon&quot; &quot;duripan&quot; ## [107] &quot;cambic.horizon&quot; &quot;umbric.epipedon&quot; ## [109] &quot;anthropic.epipedon&quot; &quot;histic.epipedon&quot; ## [111] &quot;paralithic.materials&quot; &quot;lithologic.discontinuity&quot; ## [113] &quot;andic.soil.properties&quot; &quot;densic.contact&quot; ## [115] &quot;abrupt.textural.change&quot; &quot;aquic.conditions&quot; ## [117] &quot;redox.depletions.with.chroma.2.or.less&quot; &quot;redox.concentrations&quot; ## [119] &quot;reduced.matrix&quot; &quot;densic.materials&quot; ## [121] &quot;albic.horizon&quot; &quot;spodic.horizon&quot; ## [123] &quot;fibric.soil.materials&quot; &quot;hemic.soil.materials&quot; ## [125] &quot;sapric.soil.materials&quot; &quot;volcanic.glass&quot; ## [127] &quot;folistic.epipedon&quot; &quot;strongly.contrasting.particle.size.class&quot; ## [129] &quot;human.transported.material&quot; &quot;albic.materials&quot; ## [131] &quot;secondary.carbonates&quot; &quot;landform_string&quot; ## [133] &quot;landscape_string&quot; &quot;microfeature_string&quot; ## [135] &quot;geomicrorelief_string&quot; &quot;pmkind&quot; ## [137] &quot;pmorigin&quot; horizonNames(gopheridge) ## [1] &quot;phiid&quot; &quot;peiid&quot; &quot;hzname&quot; &quot;genhz&quot; &quot;hzdept&quot; ## [6] &quot;hzdepb&quot; &quot;bounddistinct&quot; &quot;boundtopo&quot; &quot;clay&quot; &quot;silt&quot; ## [11] &quot;sand&quot; &quot;fragvoltot&quot; &quot;texture&quot; &quot;texcl&quot; &quot;lieutex&quot; ## [16] &quot;phfield&quot; &quot;effclass&quot; &quot;labsampnum&quot; &quot;rupresblkdry&quot; &quot;rupresblkmst&quot; ## [21] &quot;rupresblkcem&quot; &quot;stickiness&quot; &quot;plasticity&quot; &quot;ksatpedon&quot; &quot;texture_class&quot; ## [26] &quot;hzID&quot; &quot;d_r&quot; &quot;d_g&quot; &quot;d_b&quot; &quot;d_hue&quot; ## [31] &quot;d_value&quot; &quot;d_chroma&quot; &quot;d_sigma&quot; &quot;m_r&quot; &quot;m_g&quot; ## [36] &quot;m_b&quot; &quot;m_hue&quot; &quot;m_value&quot; &quot;m_chroma&quot; &quot;m_sigma&quot; ## [41] &quot;moist_soil_color&quot; &quot;dry_soil_color&quot; &quot;soil_color&quot; &quot;fine_gravel&quot; &quot;gravel&quot; ## [46] &quot;cobbles&quot; &quot;stones&quot; &quot;boulders&quot; &quot;channers&quot; &quot;flagstones&quot; ## [51] &quot;parafine_gravel&quot; &quot;paragravel&quot; &quot;paracobbles&quot; &quot;parastones&quot; &quot;paraboulders&quot; ## [56] &quot;parachanners&quot; &quot;paraflagstones&quot; &quot;unspecified&quot; &quot;total_frags_pct_nopf&quot; &quot;total_frags_pct&quot; ## [61] &quot;art_fgr&quot; &quot;art_gr&quot; &quot;art_cb&quot; &quot;art_st&quot; &quot;art_by&quot; ## [66] &quot;art_ch&quot; &quot;art_fl&quot; &quot;art_unspecified&quot; &quot;total_art_pct&quot; &quot;huartvol_cohesive&quot; ## [71] &quot;huartvol_penetrable&quot; &quot;huartvol_innocuous&quot; &quot;huartvol_persistent&quot; 2.5.3.1 Make profile sketches The plotSPC() or plot() function applied to a SoilProfileCollection object generates sketches based on horizon depths, designations, and colors. The SoilProfileCollection Reference contains many examples demonstrating way in which these sketches can be customized. The fetchNASIS() function automatically converts moist Munsell colors into R-style colors, available in the soil_color horizon level attribute. An approximate color mixture is used when multiple colors per horizon are present. See ?plotSPC for a detailed list of arguments and examples. par(mar = c(1, 1, 1, 1)) # omitting pedon IDs and horizon designations plotSPC(gopheridge, print.id = FALSE, name = NA, width = 0.3) title(&#39;Pedons from the `gopheridge` sample dataset&#39;, line = -0.5) Additional examples / documentation related to soil profile sketches: * live-coding examples from class * OSD Dendrogram tutorial * Visualization of Horizon Boundaries tutorial * SoilProfileCollection Reference * Competing Series tutorial * Pair-Wise Distances by Generalized Horizon Labels tutorial 2.5.3.2 Pedon Data Checks When you load pedons using the fetchNASIS() function, the following data checks are performed: Presence of multiple map datums. Results reported to the user and the data are not modified. Inconsistent horizon boundaries. Pedons with inconsistent horizon boundaries are not loaded unless rmHzErrors = FALSE. In most cases, this occurs when the bottom depth of a horizon is not the same as the upper depth of the next lower horizon. ## hzname top bot ## 1 A 0 30 ## 2 Bt1 38 56 ## 3 Bt2 56 121 ## 4 Bk 121 135 ## 5 R 135 NA Note the issue above. The bottom depth of the A horizon and the upper depth of the Bt1 horizon should be the same: either 30 or 38 cm. The correct depth needs to be determined and fixed in the database. Missing lower horizon depths. Offending horizons are fixed by replacing the missing bottom depth with the top depth plus 2 cm. In the case of the profile shown above, a bottom depth of 137 cm would be inserted where the depth is missing. Sites missing pedon records. Data without corresponding horizons are not loaded. 2.5.3.3 Find Pedons with Errors If errors in the pedon data are detected when loading data using fetchNASIS(), the following “get” commands can trace them back to the corresponding records in NASIS. These access an option that is stored in a special object called an Environment associated with the soilDB package – they generally contain vectors of IDs. get('sites.missing.pedons', envir = soilDB.env) Returns user site ID for sites missing pedons get('dup.pedon.ids', envir = soilDB.env) Returns user pedon ID for sites with duplicate pedon ID get('bad.pedon.ids', envir = soilDB.env) Returns user pedon ID for pedons with inconsistent horizon depths get('bad.horizons', envir = soilDB.env) Returns a data.frame of horizon-level information for pedons with inconsistent horizon depths get(&#39;sites.missing.pedons&#39;, envir = soilDB.env) get(&#39;dup.pedon.ids&#39;, envir = soilDB.env) get(&#39;bad.pedon.ids&#39;, envir = soilDB.env) get(&#39;bad.horizons&#39;, envir = soilDB.env) These get() calls access variables stored in the package environment soilDB.env. The variables only exist if there are “problems” / values found by the data checks – and if you fix the errors in the NASIS database and the checks don’t find any errors then nothing will be returned by these functions. 2.5.4 Further Reading / Reference The SoilProfileCollection Reference contains examples based on the sp4 sample dataset. This is a good exercise for the class and reference for future work. The Querying Soil Series Data document contains a more detailed explanation of how the data behind fetchOSD were created and how to interpret its results. The Competing Soil Series document contains a number of examples related to getting OSD morphology using fetchOSD, laboratory data using fetchKSSL, and summary via profile sketches and slice-wise aggregation. Several examples pertaining to SDA, soil texture, and soil profile sketches are explored in this Random Walk 001. The “SPC Plotting Ideas” document outlines some advanced techniques for arranging and annotating soil profile sketches. 2.6 Working with Data in R 2.6.1 Summaries Now that you’ve loaded some data, you can look at additional ways to summarize and interact with data elements. 2.6.1.1 table() The base R table() function is very useful for quick summary operations. It returns a named vector with the amount of each unique level of the a given vector. The numeric vector of “counts” is commonly combined with other functions such as sort(), order(), prop.table(), is.na() or !is.na() (is not NA) to identify abundance, proportions, or missing data (NA). # load required packages library(aqp) library(soilDB) data(&quot;gopheridge&quot;, package = &quot;soilDB&quot;) # for these examples, we use the gopheridge dataset as our &quot;selected set&quot; pedons &lt;- gopheridge # NOTE: you can use fetchNASIS to load your own data, like this: # pedons &lt;- fetchNASIS() # summarize which soil taxa we have loaded table(pedons$taxonname) ## ## Gopheridge ## 52 # sort results in descending order sort(table(pedons$taxonname), decreasing = TRUE) ## Gopheridge ## 52 # could do the same thing for taxonomic subgroups # or any column of the SPC at the site or horizon levels table(pedons$taxsubgrp) ## ## mollic haploxeralfs typic haploxerepts ultic haploxeralfs ultic haploxerolls ## 1 6 44 1 sort(table(pedons$taxsubgrp), decreasing = TRUE) ## ## ultic haploxeralfs typic haploxerepts mollic haploxeralfs ultic haploxerolls ## 44 6 1 1 2.6.2 Missing Values table(pedons$taxsubgrp, useNA = &quot;ifany&quot;) ## ## mollic haploxeralfs typic haploxerepts ultic haploxeralfs ultic haploxerolls ## 1 6 44 1 # is.na(...) table(is.na(pedons$taxsubgrp)) ## ## FALSE ## 52 # is NOT NA !is.na(...) table(!is.na(pedons$taxsubgrp)) ## ## TRUE ## 52 # it can also be applied to horizon level columns in the SPC sort(table(pedons$texture), decreasing=TRUE) ## ## BR L GR-L GRV-L CBV-L SPM GRX-L SIL GRV-CL CBV-CL GR-SIL CBX-L ## 58 36 33 24 18 14 12 12 9 8 7 6 ## GRX-CL CBX-CL GRV-SIL CL GRV-SCL GRX-C MPM SL CB-L GR-CL GRX-SCL PGR-C ## 5 4 4 3 3 3 3 3 2 2 2 2 ## PGRX-L SICL STV-CL STV-L STX-C STX-L C CB-C CB-CL CB-SCL CB-SIL CBV-SIL ## 2 2 2 2 2 2 1 1 1 1 1 1 ## CBX-SCL CN-L CN-SICL CNX-L CNX-SICL FLV-L GR-C GR-SIC GRV-SICL GRX-SIC GRX-SIL PCB-SICL ## 1 1 1 1 1 1 1 1 1 1 1 1 ## PCBV-SICL PCN-C PCNX-CL PGRV-C PGRV-CL PGRX-SCL PGRX-SIL ST-L STV-C STX-CL STX-SICL ## 1 1 1 1 1 1 1 1 1 1 1 2.6.3 Logical Operators Logical operators act on vectors for the purposes of comparison. == “EQUAL TO” != “NOT EQUAL TO” &lt; LESS than LESS than or equal to &lt;= &gt; GREATER than GREATER than or equal to &gt;= %in% Equivalent to IN () in SQL; same logic as match() but returns a boolean, not integer Example: pedons$taxpartsize %in% c('loamy-skeletal', 'sandy-skeletal') Returns a vector of TRUE/FALSE equal in length to left-hand side &amp; logical AND | logical OR 2.6.4 Pattern Matching The following examples use the grep() function to pattern match within the data, create an index of the SoilProfileCollection for records that match the specified pattern within that column, and then use that index to filter to specific sites and their corresponding profiles. Patterns are specified using regular expression (REGEX) syntax. This process can be applied to many different columns in the SPC based on how you need to filter the data. This example pattern matches on the tax_subgroup column, but another useful application might be to pattern match on geomorphology or parent material. Say we want to see what the variation of particle size classes are within a specific subgroup? We can use grep() to create a row index, then apply that index to the SoilProfileCollection. # create a numeric index for pedons with taxsubgroup containing &#39;typic&#39; idx &lt;- grep(&#39;typic&#39;, pedons$taxsubgrp) idx ## [1] 11 12 13 14 26 50 # use square bracket notation to subset &#39;typic&#39; soils in `subset1` object subset1 &lt;- pedons[idx,] # or use the index directly to summarize taxpartsize for &#39;typic&#39; soils sort(table(pedons$taxpartsize[idx]), decreasing = TRUE) ## ## loamy-skeletal clayey-skeletal ## 5 1 Note: grep() below has an invert argument (default FALSE). This option is very useful for excluding the results of the pattern matching process by inverting whatever the result is. grepl() is the logical version of grep(), so you can invert it using the logical NOT operator: !. Another method is to create an index using which() function. which() takes any logical vector (or expression), and it returns the indices (positions) where that expression returns TRUE. The use of which becomes more important when there are missing values (NA) in an expression. Do a graphical check to see the “typic” profiles are selected. Plot them in R using the SoilProfileCollection “plot” method (e.g., specialized version of the generic plot() function). # adjust margins par(mar=c(1,0,0,1)) # plot the first 10 profiles of subset1 plot(subset1[1:10, ], label = &#39;taxsubgrp&#39;, max.depth = 60) title(&#39;Pedons with the word &quot;typic&quot; at subgroup-level of Soil Taxonomy&#39;, line=-2) For more information on using regular expressions in grep() for pattern matching operations, see: Regular-expression-syntax. Quick check: Compare or run these commands with some code, and review the documentation, to answer the questions. True or False: grepl() returns a numeric vector True or False: which(grepl('typic', pedons$taxsubgrp)) is the same as grep('typic', pedons$taxsubgrp). 2.6.4.1 REGEX Quick Start . One character, any character * Zero-or-more Quantifier (of previous token) + One-or-more Quantifier (of previous token) {n} quantifier where n is the the number of a match “repeats” (of previous token) [A-Z!] ONE capital letter, or an exclamation mark [0-9]{2} TWO numbers (using { quantifier) | is equivalent to OR: Example: grep('loamy|sandy', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) “loamy OR sandy” ^ Anchor to beginning of string / line: Example: grep('^sandy', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) “STARTS WITH sandy” $ Anchor to end of string / line: Example: grep('skeletal$', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) “ENDS WITH skeletal” \\\\b Anchor to word boundary: Example: grep('\\\\bmesic', c(\"mesic\",\"thermic\",\"isomesic\")) “WORD STARTS WITH mesic” (e.g. not “isomesic”) 2.6.4.2 Resources for Regular Expressions https://regex101.com/ &amp; https://regexr.com/ - Online regular expression testers http://www.regular-expressions.info/quickstart.html - One-page regular expression quick start guide 2.6.5 Filtering A variety of methods are available to subset or “filter” R data objects, from a simple data.frame or vector, to something more complex like a Spatial object or a SoilProfileCollection. You can index many R objects using numeric or logical expressions as above. There are also methods that make this process a little easier. The base R method for this is subset() and it works on data.frame objects. It is nice because you can specify column names without explicitly referencing the data set, since subset uses non-standard evaluation of expressions passed as arguments. 2.6.5.1 Filtering with aqp::subset We use the SoilProfileCollection subset method, where we first specify a data (pedons) object then we can write expressions for the columns that exist in that object. Here, we combine two logical expressions to find taxsubgrp containing \"alfs\" (Alfisols) with obsdate before January 1st, 2010. subset2 &lt;- subset(pedons, grepl(&quot;alfs&quot;, taxsubgrp) &amp; obs_date &lt; as.POSIXlt(&quot;2010-01-01&quot;)) # check taxonomic range of particle size classes in the data # overwhelmingly these are described as loamy-skeletal ultic haploxeralfs sort(table(subset2$taxsubgrp), decreasing = TRUE) ## ## ultic haploxeralfs mollic haploxeralfs ## 28 1 sort(table(subset2$taxpartsize), decreasing = TRUE) ## ## loamy-skeletal clayey-skeletal fine fine-loamy ## 25 2 1 1 # check year described and taxpartsize table(subset2$taxpartsize, substr(subset2$obs_date, 0, 4)) ## ## 2007 2008 2009 ## clayey-skeletal 1 0 1 ## fine 1 0 0 ## fine-loamy 1 0 0 ## loamy-skeletal 19 1 5 # a double equal sign &#39;==&#39; is used for exact character or numeric criteria subset3 &lt;- subset(subset2, taxpartsize == &#39;loamy-skeletal&#39;) table(subset3$taxpartsize) ## ## loamy-skeletal ## 25 par(mar = c(0, 0, 2, 1)) plotSPC(subset3[1:12, ], print.id = FALSE) title(&#39;Loamy-skeletal Ultic Haploxeralfs&#39;) 2.6.6 Dates and Times Dates and times use special object types in R. The Unix time, also known as “Posix time,” is a system for describing a point in time. Unix epoch is a whole number value that is the number of seconds elapsed since the 00:00:00 UTC on 1 January 1970 minus leap seconds. We can use logical comparison operators on dates and times if their string representation such as \"01/01/1970\" is converted to a common base R UNIX time representation known as POSIXlt or POSIXct. This conversion accounts for important things such as timezone using your computer’s locale–which is important to keep in mind. When converting to POSIX time several unambiguous (year-month-day) date time formats can be detected. For instance, if you want to convert a date in the common month-day-year format, you need to specify the format argument: as.POSIXlt(&quot;01/01/1970&quot;, format = &quot;%m/%d/%Y&quot;) By default the timezone will match your current timezone. Dates without times are treated as being at midnight UTC. You can customize the timezone with tz argument: as.POSIXlt(&quot;01/01/1970&quot;, tz = &quot;UTC&quot;, format = &quot;%m/%d/%Y&quot;) POSIXlt and POSIXct objects can be formatted with the format() function. strptime() can be used to parse character vectors into date/times. R also has the Date class which can be used for formatting calendar dates. You can convert POSIXlt/POSIXct objects to Date with as.Date() 2.7 fetchNASIS data checks fetchNASIS does a lot of the hard work for you by pulling out a variety of child tables and joining them to site/horizon tables This results in a SoilProfileCollection object that has many of the NASIS data columns. In order to do this, a variety of processes are run. Output is generated that you should look at. We will walk through the most common parts. Analysis of Site Coordinate Data for pedons from MT647 multiple horizontal datums present, consider using WGS84 coordinates (x_std, y_std) Mixing of multiple colors (for “default” moist and dry colors for many:1 case) mixing dry colors ... [33 of 4108 horizons] mixing moist colors ... [60 of 4532 horizons] Quality control and limited “filling” of fragments, horizon depth data, site data, tables replacing missing lower horizon depths with top depth + 1cm ... [19 horizons] -&gt; QC: horizon errors detected: Use `get(&#39;bad.pedon.ids&#39;, envir=soilDB.env)` for pedon record IDs (peiid) Use `get(&#39;bad.horizons&#39;, envir=soilDB.env)` for horizon designations -&gt; QC: pedons missing bottom hz depths: Use `get(&#39;missing.bottom.depths&#39;, envir=soilDB.env)` for pedon record IDs (peiid) Notes about default settings and handling of NULL (missing data elements or records) NOTE: some records are missing surface fragment cover NOTE: some records are missing rock fragment volume NOTE: all records are missing artifact volume 2.7.1 Inspecting Results Here we inspect occurrence of andic soil properties in MT647. We will download this “selected set” from the course website as an .rda file to save you the effort of crafting your selected set just for this example. example.data.dir &lt;- &quot;C:/workspace2&quot; example.data.path &lt;- file.path(example.data.dir, &quot;mt647.rda&quot;) if(!dir.exists(example.data.dir)) dir.create(example.data.dir, recursive = TRUE) download.file(&quot;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/book/02/mt647.rda&quot;, destfile = example.data.path) Downloading and installing the above .rda is equivalent to doing NASIS query “_PedonPC_Plus_DataDump_select” (MLRA04 Bozeman) for to fill your selected set for User Site ID: %MT647%, NASIS Site: MLRA04%, NASIS Group: 4-MIS Pedons, followed by mt647 &lt;- fetchNASIS(). 2.7.1.1 Load Example Data To load the sample object data into R, just use load() and the path to the .rda file (example.data.path or \"C:/workspace2/mt647.rda\") # load the sample data example.data.dir &lt;- &quot;C:/workspace2&quot; load(file.path(example.data.dir, &quot;mt647.rda&quot;)) length(mt647) ## [1] 481 table(site(mt647)$andic.soil.properties, useNA = &quot;ifany&quot;) ## ## FALSE TRUE &lt;NA&gt; ## 2 83 396 # get just the profiles with andic.soil.properties == TRUE subset(mt647, andic.soil.properties) ## SoilProfileCollection with 83 profiles and 446 horizons ## profile ID: peiid | horizon ID: phiid ## Depth range: 20 - 305 cm ## ## ----- Horizons (6 / 446 rows | 10 / 73 columns) ----- ## peiid phiid hzdept hzdepb hzname texture genhz bounddistinct boundtopo clay ## 828140 4005861 0 2 Oe &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA ## 828140 4005863 2 13 E CBV-L &lt;NA&gt; clear smooth 5 ## 828140 4005859 13 23 Bs1 CBV-L &lt;NA&gt; clear smooth 8 ## 828140 4005862 23 33 Bs2 CBV-L &lt;NA&gt; gradual smooth 8 ## 828140 4005864 33 69 BC GRX-FSL &lt;NA&gt; gradual smooth 10 ## 828140 4005860 69 152 2C GRX-FSL &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10 ## [... more horizons ...] ## ## ----- Sites (6 / 83 rows | 10 / 105 columns) ----- ## siteiid peiid pedon_id site_id obs_date utmzone utmeasting utmnorthing x y ## 845415 828140 P23-134 93MT6470134 1993-08-12 12 294091.0 5092141 -113.6569 45.95195 ## 845429 828152 P92-052 92MT6470052 1992-07-30 12 274420.9 5123423 -113.9253 46.22694 ## 845430 828153 P92-046 92MT6470046 1992-07-23 12 282680.7 5161752 -113.8361 46.57417 ## 845432 828155 P93-053 93MT6470053 1993-08-10 12 259526.3 5079142 -114.0958 45.82389 ## 845434 828157 P91-103 91MT6470103 1991-09-26 12 272221.3 5127708 -113.9558 46.26472 ## 845451 828176 P89-011 89MT6470011 1989-08-02 12 276988.7 5110875 -113.8861 46.11500 ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] We can compare this to what we see in the NASIS Pedon Diagnostic Features table: Any profiles that have have logic errors detected are stored in soilDB.env bad.pedon.ids variable after you run fetchNASIS. If this variable does not exist either you have not run fetchNASIS() in the current session or there are no errors # these are the troublesome user pedon IDs get(&#39;bad.pedon.ids&#39;, envir=soilDB.env) ## [1] &quot;P93-037&quot; &quot;P90-008&quot; &quot;P90-004&quot; &quot;P90-009&quot; &quot;P93-058&quot; &quot;P93-059&quot; &quot;P90-025&quot; &quot;P90-002&quot; &quot;P90-012&quot; &quot;P92-001&quot; &quot;P92-085&quot; ## [12] &quot;P91-105&quot; &quot;P90-019&quot; &quot;P75-006&quot; &quot;P90-010&quot; &quot;P90-015&quot; &quot;P91-094&quot; &quot;P92-029&quot; &quot;P92-076&quot; &quot;P93-026&quot; &quot;P93-035&quot; &quot;P93-063&quot; ## [23] &quot;P93-064&quot; &quot;P93-041&quot; &quot;P93-043&quot; &quot;P93-044&quot; &quot;P93-083&quot; &quot;P93-112&quot; &quot;P93-113&quot; &quot;P93-124&quot; &quot;P93-001&quot; &quot;P96-007&quot; &quot;P91-025&quot; ## [34] &quot;P93-078&quot; &quot;P92-044&quot; &quot;P91-112&quot; &quot;P92-038&quot; &quot;P90-018&quot; &quot;P93-057&quot; &quot;P93-084&quot; &quot;P91-059&quot; &quot;P90-016&quot; &quot;P92-063&quot; &quot;P92-048&quot; ## [45] &quot;P93-052&quot; &quot;F01-230&quot; &quot;F95-420&quot; &quot;F95-114&quot; &quot;F96-205&quot; # by default, rmHzErrors removes the &quot;bad&quot; illogical pedons any(mt647$pedon_id %in% get(&#39;bad.pedon.ids&#39;, envir=soilDB.env)) ## [1] FALSE When fetchNASIS(..., rmHzErrors = TRUE) (the default), any horizons that were omitted from the SoilProfileCollection will be stored in the bad.horizons variable in the soilDB.env environment. head(get(&#39;bad.horizons&#39;, envir=soilDB.env)) ## peiid phiid pedon_id hzname hzdept hzdepb ## 67 868038 4270407 F01-230 Bw NA NA ## 68 868038 4270406 F01-230 E NA NA ## 95 868072 4270514 F95-114 &lt;NA&gt; NA NA ## 99 868048 4270437 F95-420 &lt;NA&gt; NA NA ## 111 868074 4270521 F96-205 C NA NA ## 112 868074 4270519 F96-205 B NA NA 2.7.1.2 Logic Checks for the SoilProfileCollection The aqp package has several functions that do logic checks on SoilProfileCollection objects. The main method that does this in aqp is checkHzDepthLogic which returns a data.frame of results of running four logic tests on the horizon data from each profile. Checks for: bottom depths less than top depth / bad top depth order (\"depthLogic\") bottom depths equal to top depth (\"sameDepth\") overlaps and gaps (\"overlapOrGap\") missing depths (\"missingDepth\") logic_tests &lt;- checkHzDepthLogic(mt647err) # look at first few (look OK; valid == TRUE) head(logic_tests) ## peiid valid depthLogic sameDepth missingDepth overlapOrGap ## 1 828138 TRUE FALSE FALSE FALSE FALSE ## 2 828139 TRUE FALSE FALSE FALSE FALSE ## 3 828140 TRUE FALSE FALSE FALSE FALSE ## 4 828141 TRUE FALSE FALSE FALSE FALSE ## 5 828142 TRUE FALSE FALSE FALSE FALSE ## 6 828143 TRUE FALSE FALSE FALSE FALSE # these all have overlapOrGap errors head(logic_tests[!logic_tests$valid, ]) ## peiid valid depthLogic sameDepth missingDepth overlapOrGap ## 11 828148 FALSE FALSE FALSE FALSE TRUE ## 23 828160 FALSE FALSE FALSE FALSE TRUE ## 25 828162 FALSE FALSE FALSE FALSE TRUE ## 34 828171 FALSE FALSE FALSE FALSE TRUE ## 35 828172 FALSE FALSE FALSE FALSE TRUE ## 36 828173 FALSE FALSE FALSE FALSE TRUE # join the logic test data into the site table site(mt647err) &lt;- logic_tests Use the $valid vector in result to filter out profiles with missing depths (logic_tests$valid == FALSE) bad.profiles &lt;- subset(mt647err, !valid) bad.profiles ## SoilProfileCollection with 49 profiles and 338 horizons ## profile ID: peiid | horizon ID: phiid ## Depth range: 15 - 152 cm ## ## ----- Horizons (6 / 338 rows | 10 / 73 columns) ----- ## peiid phiid hzdept hzdepb hzname texture genhz bounddistinct boundtopo clay ## 828148 4005908 0 3 Oe &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA ## 828148 4005907 3 5 Oi &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA ## 828148 4005905 5 15 E CB-FSL &lt;NA&gt; clear smooth 5 ## 828148 4005911 15 23 Bs CB-SIL &lt;NA&gt; abrupt smooth 5 ## 828148 4005909 23 25 2E CBV-FSL &lt;NA&gt; clear smooth 8 ## 828148 4005910 25 58 2B CBV-FSL &lt;NA&gt; gradual smooth 10 ## [... more horizons ...] ## ## ----- Sites (6 / 49 rows | 10 / 110 columns) ----- ## peiid siteiid pedon_id site_id obs_date utmzone utmeasting utmnorthing x y ## 828148 845423 P93-037 93MT6470037 1993-07-28 12 272869.5 5126695 -113.9469 46.25583 ## 828160 845435 P90-008 90MT6470008 1990-07-12 12 274656.5 5129224 -113.9250 46.27917 ## 828162 845437 P90-004 90MT6470004 1990-07-16 12 273885.5 5128079 -113.9344 46.26861 ## 828171 845446 P90-009 90MT6470009 1990-07-24 12 274813.1 5129404 -113.9230 46.28083 ## 828172 845447 P93-058 93MT6470058 1993-08-19 12 265401.3 5110256 -114.0356 46.10555 ## 828173 845448 P93-059 93MT6470059 1993-08-19 12 265446.7 5110316 -114.0350 46.10611 ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] And also filter out the valid ones (logic_tests$valid == TRUE) good.profiles &lt;- subset(mt647err, logic_tests$valid) good.profiles ## SoilProfileCollection with 481 profiles and 2536 horizons ## profile ID: peiid | horizon ID: phiid ## Depth range: 14 - 1552 cm ## ## ----- Horizons (6 / 2536 rows | 10 / 73 columns) ----- ## peiid phiid hzdept hzdepb hzname texture genhz bounddistinct boundtopo clay ## 828138 4005848 0 5 O &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA ## 828138 4005852 5 18 E GR-L &lt;NA&gt; clear smooth 12 ## 828138 4005850 18 38 Bw1 GRV-L &lt;NA&gt; gradual wavy 14 ## 828138 4005849 38 51 Bw2 CBV-FSL &lt;NA&gt; gradual wavy 12 ## 828138 4005853 51 71 BC CBV-SL &lt;NA&gt; clear wavy 12 ## 828138 4005851 71 81 R &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA ## [... more horizons ...] ## ## ----- Sites (6 / 481 rows | 10 / 110 columns) ----- ## peiid siteiid pedon_id site_id obs_date utmzone utmeasting utmnorthing x y ## 828138 845413 P91-043 91MT6470043 1991-07-11 12 268288.0 5068425 -113.9781 45.73056 ## 828139 845414 P91-029 91MT6470029 1991-06-27 12 280623.2 5147302 -113.8561 46.44361 ## 828140 845415 P23-134 93MT6470134 1993-08-12 12 294091.0 5092141 -113.6569 45.95195 ## 828141 845416 P93-025 93MT6470025 1993-07-08 12 279148.0 5151960 -113.8775 46.48500 ## 828142 845417 P93-075 93MT6470075 1993-09-14 12 261086.6 5080380 -114.0764 45.83556 ## 828143 845418 P93-074 93MT6470074 1993-09-14 12 261036.4 5080197 -114.0769 45.83389 ## [... more sites ...] ## ## Spatial Data: ## [EMPTY] 2.8 Extended Data Functions Additional data related to both site and horizon information can be fetched using the get_extended_data_from_NASIS() function. This function returns a named list() with several tables that fetchNASIS draws from–in addition to the typical Site/Pedon/Component/Horizon tables. There are a variety of calculated fields that are included in the default fetchNASIS result based on the extended data–you can make use of these to simplify queries and aggregations for a variety of analyses… And then follow up with more detailed data as needed. 2.8.1 Elements of get_extended_data_from_NASIS() Ecological Site History (\"ecositehistory\") Diagnostic Features (\"diagnostic\") Diagnostic Feature TRUE/FALSE Summary (\"diagHzBoolean\") Restrictions (\"restriction\") Fragment and Texture Summaries Horizon Fragments (\"frag_summary\") Horizon Artifacts (\"art_summary\") Surface Fragments (\"surf_frag_summary\") Texture Class Modifiers (\"texmodifier\") Geomorphic Table Summaries (\"geomorph\") Parent Material Summaries (\"pm\") Taxonomic History (\"taxhistory\") Site Text Notes w/ Photo links(\"photo\") Horizon Structure (\"struct\") Horizon Designation (\"hzdesgn\") 2.8.2 Load Example Data Below is a summary of additional information that can be readily brought into R from your NASIS selected set via the get_extended_data_from_NASIS() function. To download the sample 2015MT663% data from the course page with R: example.data.dir &lt;- &quot;C:/workspace2&quot; example.data.path &lt;- file.path(example.data.dir, &quot;mt663.rda&quot;) if (!dir.exists(example.data.dir)) dir.create(example.data.dir, recursive = TRUE) download.file(&quot;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/book/02/mt663.rda&quot;, destfile = example.data.path) Before continuing, imagine opening the NASIS client, populating your selected set with 2015MT663% using a query like “NSSC Pangaea – POINT-Pedon/Site by User Pedon ID” Load the data like we did above. # load the sample data example.data.dir &lt;- &quot;C:/workspace2&quot; load(file.path(example.data.dir, &quot;mt663.rda&quot;)) ## fetch extended site and horizon data from local NASIS # mt663ext &lt;- get_extended_data_from_NASIS_db() We could use the get_extended_data_from_NASIS_db() function if 2015MT663% or other data were in the selected set, but we will use the mt663ext data we loaded from the .rda file. The column names are the names of variables that you could join to your site or horizon data by various means. Generally these variable names, with a few exceptions, mirror the NASIS 7 data model names. # site and pedon related extended data # list all dataframes in the extended data str(mt663ext, 1) ## List of 14 ## $ ecositehistory :&#39;data.frame&#39;: 0 obs. of 5 variables: ## $ diagnostic :&#39;data.frame&#39;: 292 obs. of 4 variables: ## $ diagHzBoolean :&#39;data.frame&#39;: 115 obs. of 20 variables: ## $ restriction :&#39;data.frame&#39;: 11 obs. of 8 variables: ## $ frag_summary :&#39;data.frame&#39;: 561 obs. of 18 variables: ## $ art_summary :&#39;data.frame&#39;: 561 obs. of 14 variables: ## $ surf_frag_summary:&#39;data.frame&#39;: 134 obs. of 10 variables: ## $ texmodifier :&#39;data.frame&#39;: 622 obs. of 5 variables: ## $ geomorph :&#39;data.frame&#39;: 241 obs. of 9 variables: ## $ taxhistory :&#39;data.frame&#39;: 115 obs. of 21 variables: ## $ photo :&#39;data.frame&#39;: 793 obs. of 4 variables: ## $ pm :&#39;data.frame&#39;: 179 obs. of 10 variables: ## $ struct :&#39;data.frame&#39;: 444 obs. of 6 variables: ## $ hzdesgn :&#39;data.frame&#39;: 561 obs. of 19 variables: # vegetation data summary colnames(mt663ext$ecositehistory) ## [1] &quot;siteiid&quot; &quot;ecositeid&quot; &quot;ecositenm&quot; &quot;ecositecorrdate&quot; &quot;es_classifier&quot; # diagnostic features colnames(mt663ext$diagnostic) ## [1] &quot;peiid&quot; &quot;featkind&quot; &quot;featdept&quot; &quot;featdepb&quot; # surface rock fragments colnames(mt663ext$surf_frag_summary) ## [1] &quot;peiid&quot; &quot;surface_fgravel&quot; &quot;surface_gravel&quot; &quot;surface_cobbles&quot; &quot;surface_stones&quot; ## [6] &quot;surface_boulders&quot; &quot;surface_channers&quot; &quot;surface_flagstones&quot; &quot;surface_paragravel&quot; &quot;surface_paracobbles&quot; # geomorphic description colnames(mt663ext$geomorph) ## [1] &quot;peiid&quot; &quot;geomicrorelief&quot; &quot;geommicelev&quot; &quot;geomfmod&quot; &quot;geomfname&quot; &quot;geomfeatid&quot; ## [7] &quot;existsonfeat&quot; &quot;geomfiidref&quot; &quot;geomftname&quot; # taxonomic history data colnames(mt663ext$taxhistory) ## [1] &quot;peiid&quot; &quot;classdate&quot; &quot;classifier&quot; &quot;classtype&quot; &quot;taxonname&quot; &quot;localphase&quot; ## [7] &quot;taxonkind&quot; &quot;seriesstatus&quot; &quot;taxclname&quot; &quot;taxpartsize&quot; &quot;taxorder&quot; &quot;taxsuborder&quot; ## [13] &quot;taxgrtgroup&quot; &quot;taxsubgrp&quot; &quot;soiltaxedition&quot; &quot;osdtypelocflag&quot; &quot;taxmoistcl&quot; &quot;taxtempregime&quot; ## [19] &quot;taxfamother&quot; &quot;psctopdepth&quot; &quot;pscbotdepth&quot; # linked photo stored in site textnotes colnames(mt663ext$photo) ## [1] &quot;siteiid&quot; &quot;recdate&quot; &quot;textcat&quot; &quot;imagepath&quot; # site parent materials colnames(mt663ext$pm) ## [1] &quot;siteiid&quot; &quot;seqnum&quot; &quot;pmorder&quot; &quot;pmdept&quot; &quot;pmdepb&quot; &quot;pmmodifier&quot; &quot;pmgenmod&quot; ## [8] &quot;pmkind&quot; &quot;pmorigin&quot; &quot;pmweathering&quot; ### ### horizon related extended data ### # rock fragments colnames(mt663ext$frag_summary) ## [1] &quot;phiid&quot; &quot;fine_gravel&quot; &quot;gravel&quot; &quot;cobbles&quot; &quot;stones&quot; ## [6] &quot;boulders&quot; &quot;channers&quot; &quot;flagstones&quot; &quot;parafine_gravel&quot; &quot;paragravel&quot; ## [11] &quot;paracobbles&quot; &quot;parastones&quot; &quot;paraboulders&quot; &quot;parachanners&quot; &quot;paraflagstones&quot; ## [16] &quot;unspecified&quot; &quot;total_frags_pct_nopf&quot; &quot;total_frags_pct&quot; # soil texture modifers colnames(mt663ext$texmodifier) ## [1] &quot;peiid&quot; &quot;phiid&quot; &quot;phtiid&quot; &quot;seqnum&quot; &quot;texmod&quot; # soil structure data colnames(mt663ext$struct) ## [1] &quot;phiid&quot; &quot;structgrade&quot; &quot;structsize&quot; &quot;structtype&quot; &quot;structid&quot; &quot;structpartsto&quot; 2.8.3 Visualizing Common Landforms The following code generates a simple graphical summary of the 10 most commonly occurring \"landform_string\" (a calculated field in fetchNASIS()) to inspect which are the most common. # load data from a NASIS selected set (or sample object) pedons &lt;- mt663 # create &#39;lf&#39; object of landform factors sorted in descending order lf &lt;- sort(table(pedons$landform_string), decreasing = TRUE) # plot top 10 or length, whichever is shorter Hmisc::dotchart2(lf[1:pmin(10, length(lf))], col = &#39;black&#39;, xlim = c(0, max(lf)), cex.labels = 0.75) For a challenge and to further inspect your own data try the above code with some other summaries of geomorphic data produced by fetchNASIS(). You can swap landform_string for: landscape_string (landscape), hillslopeprof (2D), geomposmntn, geomposhill, geompostrce, geomposflats (3D), slope_shape, shapeacross, shapedown (slope shape across/down), microfeature_string (microfeature), or geomicrorelief_string (site observation microrelief). 2.8.4 Diagnostic Features 2.8.4.1 Boolean Diagnostic Features in fetchNASIS If diagnostic features are populated in the Pedon Diagnostic Features table in NASIS, then Boolean (TRUE or FALSE) fields are created for each diagnostic feature type found in the data brought in by fetchNASIS. These fields can be used to model presence / absence of a diagnostic soil feature by extracting the site data from the SoilProfileCollection with site(). 2.8.4.2 Thickness from Diagnostic Features Table The following is an example of how you could use the diagnostic features (if populated!) from the extended data to determine the thickness of a diagnostic feature of interest. # get diagnostic features associated with pedons loaded from selected set d &lt;- diagnostic_hz(mt663) # summary of the diagnostic features in your data! unique(d$featkind) ## [1] &quot;ochric epipedon&quot; &quot;cambic horizon&quot; &quot;lithic contact&quot; &quot;mollic epipedon&quot; ## [5] &quot;argillic horizon&quot; &quot;redox concentrations&quot; &quot;andic soil properties&quot; &quot;secondary carbonates&quot; ## [9] &quot;sapric soil materials&quot; &quot;aquic conditions&quot; &quot;reduced matrix&quot; &quot;albic horizon&quot; ## [13] &quot;spodic horizon&quot; &quot;glossic horizon&quot; &quot;spodic materials&quot; &quot;lithologic discontinuity&quot; ## [17] &quot;densic materials&quot; &quot;umbric epipedon&quot; &quot;albic materials&quot; NA # tabulate sort(table(droplevels(factor(d$featkind))), decreasing = TRUE) ## ## ochric epipedon cambic horizon argillic horizon mollic epipedon ## 61 54 43 42 ## andic soil properties lithic contact secondary carbonates umbric epipedon ## 30 20 7 7 ## albic horizon spodic horizon glossic horizon reduced matrix ## 6 4 3 3 ## sapric soil materials lithologic discontinuity albic materials aquic conditions ## 3 2 1 1 ## densic materials redox concentrations spodic materials ## 1 1 1 # subset argillic horizons d &lt;- d[d$featkind == &#39;argillic horizon&#39;, ] # create a new column and subtract the upper from the lower depth d$argillic_thickness_cm &lt;- d$featdepb - d$featdept # create another new column with the upper depth to the diagnostic feature d$depth_to_argillic_cm &lt;- d$featdept # omit NA values d &lt;- na.omit(d) # subset to pedon records IDs and calculated thickness d &lt;- d[, c(&#39;peiid&#39;, &#39;argillic_thickness_cm&#39;, &#39;depth_to_argillic_cm&#39;)] head(d) ## peiid argillic_thickness_cm depth_to_argillic_cm ## 7 1092610 56 30 ## 24 1092617 38 34 ## 26 1092618 29 23 ## 28 1092619 38 32 ## 30 1092620 29 24 ## 33 1092621 23 19 # left-join with existing site data site(mt663) &lt;- d # plot as histogram par(mar = c(4.5, 4.5, 1, 1)) # note additional arguments to adjust figure labels hist( mt663$argillic_thickness_cm, xlab = &#39;Thickness of argillic (cm)&#39;, main = &#39;&#39;, las = 1 ) hist( mt663$depth_to_argillic_cm, xlab = &#39;Depth to argillic top depth (cm)&#39;, main = &#39;&#39;, las = 1 ) Quick check: What can you do with the boolean diagnostic feature data stored in the site table of a fetchNASIS SoilProfileCollection? 2.8.4.3 Diagnostic Feature Diagrams # work up diagnostic plot based on the mt663 dataset loaded above library(aqp) library(soilDB) library(sharpshootR) # can limit which diagnostic features to show by setting &#39;v&#39; manually v &lt;- c(&#39;ochric.epipedon&#39;, &#39;mollic.epipedon&#39;, &#39;andic.soil.properties&#39;, &#39;argillic.horizon&#39;, &#39;cambic.horizon&#39;, &#39;lithic.contact&#39;) # the default concatenated landform_string may have multiple levels # depending on how the geomorphic tables were populated # these are concatenated using the ampersand (&amp;) character # so take the first string split using ampersand as a delimiter mt663$first_landform &lt;- sapply(strsplit(mt663$landform_string, &quot;&amp;&quot;), function(x) x[[1]]) # plot with diagnostic features ordered according to co-occurrence # v: site-level attributes to consider # k: number of clusters to identify diagnosticPropertyPlot( mt663[1:30, ], v = v, k = 5, grid.label = &#39;site_id&#39;, dend.label = &#39;first_landform&#39;, sort.vars = TRUE ) 2.8.5 Comparing Horizon Data with Diagnostic “Intervals” Here is a demo of using Soil Data Access and the soilDB function fetchSDA to inspect the horizon data / properties in portions of the profile corresponding to diagnostic features: Glacierpoint glom() demo - finding sandy “cambic” horizons 2.8.5.1 Follow along with your own data Use the following script to generate a diagnostic-feature diagram for the pedon data you’ve loaded from your NASIS selected set. Select a series of diagnostic properties or automatically pull diagnostic feature columns. library(aqp) library(soilDB) library(sharpshootR) # Load data f &lt;- fetchNASIS(from = &#39;pedons&#39;) # ... May need to use subset() to reduce the number of pedons! # get all diagnostic feature columns from site data # by pattern matching on &#39;[.]&#39; in the site attribute names # this is not a generic rule, but works here idx &lt;- grep(&#39;[.]&#39;, siteNames(f)) v &lt;- siteNames(f)[idx] # inspect v v # insert diagnostics of interest from the possible list in &#39;v&#39; v &lt;- c(&#39;ochric.epipedon&#39;, &#39;cambic.horizon&#39;, &#39;argillic.horizon&#39;, &#39;paralithic.contact&#39;, &#39;lithic.contact&#39;) # generate diagnostic property diagram diagnosticPropertyPlot( f, v = v, k = 5, grid.label = &#39;site_id&#39;, dend.label = &#39;taxonname&#39; ) For more information on generating diagnostic feature diagrams, see the following tutorial: Diagnostic Feature Property Plots. 2.9 Custom Queries to Local NASIS Database fetchNASIS and related convenience functions are wrappers around commonly used chunks of SQL (Structured Query Language). Queries of the NASIS local database can be written in T-SQL which is the dialect of SQL used to communicate with Microsoft SQL Server. This is the connection that you set for the pre-course. The following example will return all records in your selected set sitesoiltemp table, along with a couple of fields from the site, siteobs, and pedon tables. This is a convenient way to collect all of the field-based soil temperature data associated with the pedons in your selected set for further analysis. You can use the CA792 (Sequoia-Kings Canyon National Park) pedons as an example. Use a query that searches user pedon ID for the following pattern %ca792% to download and populate a selected set in the NASIS client. library(soilDB) # write query as a character string q &lt;- &quot;SELECT siteiid as siteiid, peiid, usiteid as site_id, upedonid as pedon_id, obsdate as obs_date, soitemp, soitempdep FROM site_View_1 INNER JOIN siteobs_View_1 ON site_View_1.siteiid = siteobs_View_1.siteiidref LEFT OUTER JOIN sitesoiltemp_View_1 ON siteobs_View_1.siteobsiid = sitesoiltemp_View_1.siteobsiidref LEFT OUTER JOIN pedon_View_1 ON siteobs_View_1.siteobsiid = pedon_View_1.siteobsiidref ORDER BY obs_date, siteiid;&quot; # setup connection local NASIS channel &lt;- dbConnectNASIS() # exec query d &lt;- dbQueryNASIS(channel, q) The functions dbConnectNASIS() (alias NASIS()) and dbQueryNASIS() allow you to create a connection to the NASIS local database and send queries to that connection, respectively. By default, dbQueryNASIS() will close your connection after completing the query; you can change this by setting close=FALSE. # check results str(d) # remove records missing values d &lt;- na.omit(d) # tabulate unique soil depths table(d$soitempdep) # extract doy of year d$doy &lt;- as.integer(format(d$obs_date, &quot;%j&quot;)) # when where measurements collected? hist( d$doy, xlim = c(1, 366), las = 1, main = &#39;Soil Temperature Measurements&#39;, xlab = &#39;Day of Year&#39; ) # soil temperature by day of year plot( soitemp ~ doy, data = d, main = &#39;Soil Temperature Measurements (CA792)\\nNASIS &quot;Site Soil Temperature&quot; table&#39;, type = &#39;p&#39;, pch = 21, bg = &#39;royalblue&#39;, xlim = c(1, 366), ylim = c(-1, 26), xlab = &#39;Day of Year&#39;, ylab = &#39;Soil Temperature at 50cm (deg C)&#39;, las = 1 ) # vernal equinox, summer solstice, autumnal equinox, winter solstice x &lt;- as.Date(c(&#39;2022-03-20&#39;, &#39;2022-06-21&#39;, &#39;2022-09-23&#39;, &#39;2022-12-21&#39;)) # convert dates -&gt; Julian date, or day-of-year doy &lt;- as.integer(format(x, &quot;%j&quot;)) # add vertical lines abline(v = doy, lty = 3, col = grey(0.45)) # annotate # pos argument: left-center offset text( x = doy, y = -1, labels = c(&#39;vernal equinox&#39;, &#39;summer solstice&#39;, &#39;autumnal equinox&#39;, &#39;winter solstice&#39;), pos = 2, cex = 0.75, font = 3 ) # box/whisker plot showing distribution of day-of-year # when measurements (pedon descriptions) were collected boxplot(d$doy, at = 26, horizontal = TRUE, add = TRUE, lty = 1, width = 1, col = &#39;white&#39;, axes = FALSE) Alternatively, use a circular histogram. Days of the year progress clockwise from 0. library(circular) d.circ &lt;- circular(d$doy, type = &#39;angles&#39;, units = &#39;degrees&#39;, rotation = &#39;clock&#39;, zero = pi/2) par(mar = c(0, 0, 0, 0)) plot.circular(d.circ, stack = TRUE, bg = &#39;royalblue&#39;, pch = 21, sep = 0.04, shrink = 1.5) "],["eda.html", "Chapter 3 Exploratory Data Analysis 3.1 Objectives (Exploratory Data Analysis) 3.2 Statistics 3.3 Data Inspection 3.4 Exercise 1: fetch and inspect 3.5 Descriptive Statistics 3.6 Exercise 2: Compute Descriptive Statistics 3.7 Graphical Methods 3.8 Exercise 3: Graphical Methods 3.9 Transformations 3.10 The Shiny Package 3.11 soilReports 3.12 Additional Reading (Exploratory Data Analysis)", " Chapter 3 Exploratory Data Analysis Before embarking on developing statistical models and generating predictions, it is essential to understand your data. This is typically done using conventional numerical and graphical methods. John Tukey ((Tukey 1977)) advocated the practice of exploratory data analysis (EDA) as a critical part of the scientific process. “No catalog of techniques can convey a willingness to look for what can be seen, whether or not anticipated. Yet this is at the heart of exploratory data analysis. The graph paper and transparencies are there, not as a technique, but rather as a recognition that the picture examining eye is the best finder we have of the wholly unanticipated.” Fortunately, we can dispense with the graph paper and transparencies and use software that makes routine work of developing the ‘pictures’ (i.e., graphical output) and descriptive statistics needed to explore our data. NASIS has an abundance of tabular soils data captured within it, dating back in some cases as far as 1975. These records capture a large amount of raw information about our soil series and map unit component concepts. In many cases, the data are not as clear to interpret or detailed as we would like. This chapter will demonstrate ways to characterize a variety of different data types. 3.1 Objectives (Exploratory Data Analysis) Review methods for estimating Low, RV, and High values Review different methods for visualizing soil data Review data transformations 3.2 Statistics Descriptive statistics include: Mean - arithmetic average Median - middle value Mode - most frequent value Standard Deviation - variation around the mean Interquartile Range - range encompasses 50% of the values Kurtosis - peakedness of the data distribution Skewness - symmetry of the data distribution Graphical methods include: Histogram - a bar plot where each bar represents the frequency of observations for a given range of values Density estimation - an estimation of the frequency distribution based on the sample data Quantile-quantile plot - a plot of the actual data values against a normal distribution Box plots - a visual representation of median, quartiles, symmetry, skewness, and outliers Scatter plots - a graphical display of one variable plotted on the x axis and another on the y axis Radial plots - plots formatted for the representation of circular data 3.3 Data Inspection Before you start an EDA, you should inspect your data and correct all typos and blatant errors. EDA can then be used to identify additional errors such as outliers and help you determine the appropriate statistical analyses. For this chapter we’ll use the loafercreek dataset from the CA630 Soil Survey Area. library(dplyr) # Load from the the loakercreek dataset data(&quot;loafercreek&quot;, package = &quot;soilDB&quot;) # Extract the horizon table h &lt;- aqp::horizons(loafercreek) # Construct generalized horizon designations n &lt;- c(&quot;A&quot;, &quot;BAt&quot;, &quot;Bt1&quot;, &quot;Bt2&quot;, &quot;Cr&quot;, &quot;R&quot;) # REGEX rules p &lt;- c(&quot;A&quot;, &quot;BA|AB&quot;, &quot;Bt|Bw&quot;, &quot;Bt3|Bt4|2B|C&quot;, &quot;Cr&quot;, &quot;R&quot;) # Compute genhz labels and add to loafercreek dataset h$genhz &lt;- aqp::generalize.hz(h$hzname, n, p) # Examine genhz vs hznames (wide format) table(h$genhz, h$hzname) ## ## 2BC 2BCt 2Bt1 2Bt2 2Bt3 2Bt4 2Bt5 2CB 2CBt 2Cr 2Crt 2R A A1 A2 AB ABt Ad Ap B BA BAt BC BCt Bt Bt1 Bt2 Bt3 ## A 0 0 0 0 0 0 0 0 0 0 0 0 97 7 7 0 0 1 1 0 0 0 0 0 0 0 0 0 ## BAt 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 31 8 0 0 0 0 0 0 ## Bt1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 8 93 88 0 ## Bt2 1 1 3 8 8 6 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 4 16 0 0 0 47 ## Cr 0 0 0 0 0 0 0 0 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## R 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## not-used 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## ## Bt4 Bw Bw1 Bw2 Bw3 C CBt Cd Cr Cr/R Crt H1 Oi R Rt ## A 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## BAt 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Bt1 0 10 2 2 1 0 0 0 0 0 0 0 0 0 0 ## Bt2 8 0 0 0 0 6 6 1 0 0 0 0 0 0 0 ## Cr 0 0 0 0 0 0 0 0 49 0 20 0 0 0 0 ## R 0 0 0 0 0 0 0 0 0 1 0 0 0 40 1 ## not-used 0 0 0 0 0 0 0 0 0 0 0 1 24 0 0 # Examine matching pairs (long format) h %&gt;% group_by(genhz, hzname) %&gt;% count() ## # A tibble: 43 × 3 ## # Groups: genhz, hzname [43] ## genhz hzname n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 A A 97 ## 2 A A1 7 ## 3 A A2 7 ## 4 A Ad 1 ## 5 A Ap 1 ## 6 BAt AB 1 ## 7 BAt BA 31 ## 8 BAt BAt 8 ## 9 Bt1 ABt 2 ## 10 Bt1 Bt 8 ## # … with 33 more rows As noted in Chapter 1, a visual examination of the raw data is possible by clicking on the dataset in the environment tab, or via commandline: View(h) This view is fine for a small dataset, but can be cumbersome for larger ones. The summary() function can be used to quickly summarize a dataset however, even for our small example dataset, the output can be voluminous. Therefore in the interest of saving space we’ll only look at a sample of columns. h %&gt;% select(genhz, clay, total_frags_pct, phfield, effclass) %&gt;% summary() ## genhz clay total_frags_pct phfield effclass ## A :113 Min. :10.00 Min. : 0.00 Min. :4.90 Length:626 ## BAt : 40 1st Qu.:18.00 1st Qu.: 0.00 1st Qu.:6.00 Class :character ## Bt1 :206 Median :22.00 Median : 5.00 Median :6.30 Mode :character ## Bt2 :118 Mean :23.63 Mean :13.88 Mean :6.18 ## Cr : 75 3rd Qu.:28.00 3rd Qu.:20.00 3rd Qu.:6.50 ## R : 48 Max. :60.00 Max. :95.00 Max. :7.00 ## not-used: 26 NA&#39;s :167 NA&#39;s :381 The summary() function is known as a generic R function. It will return a preprogrammed summary for any R object. Because h is a data frame, we get a summary of each column. Factors will be summarized by their frequency (i.e., number of observations), while numeric or integer variables will print out a five number summary, and characters simply print their length. The number of missing observations for any variable will also be printed if they are present. If any of these metrics look unfamiliar to you, don’t worry we’ll cover them shortly. When you do have missing data and the function you want to run will not run with missing values, the following options are available: Exclude all rows or columns that contain missing values using the function na.exclude(), such as h2 &lt;- na.exclude(h). However this can be wasteful because it removes all rows (e.g., horizons), regardless if the row only has 1 missing value. Instead it’s sometimes best to create a temporary copy of the variable in question and then remove the missing variables, such as clay &lt;- na.exclude(h$clay). Replace missing values with another value, such as zero, a global constant, or the mean or median value for that column, such as h$clay &lt;- ifelse(is.na(h$clay), 0, h$clay) # or h[is.na(h$clay), ] &lt;- 0. Read the help file for the function you’re attempting to use. Many functions have additional arguments for dealing with missing values, such as na.rm. A quick check for typos would be to examine the list of levels for a factor or character, such as: # just for factors levels(h$genhz) ## [1] &quot;A&quot; &quot;BAt&quot; &quot;Bt1&quot; &quot;Bt2&quot; &quot;Cr&quot; &quot;R&quot; &quot;not-used&quot; # for characters and factors sort(unique(h$hzname)) ## [1] &quot;2BC&quot; &quot;2BCt&quot; &quot;2Bt1&quot; &quot;2Bt2&quot; &quot;2Bt3&quot; &quot;2Bt4&quot; &quot;2Bt5&quot; &quot;2CB&quot; &quot;2CBt&quot; &quot;2Cr&quot; &quot;2Crt&quot; &quot;2R&quot; &quot;A&quot; &quot;A1&quot; &quot;A2&quot; &quot;AB&quot; ## [17] &quot;ABt&quot; &quot;Ad&quot; &quot;Ap&quot; &quot;B&quot; &quot;BA&quot; &quot;BAt&quot; &quot;BC&quot; &quot;BCt&quot; &quot;Bt&quot; &quot;Bt1&quot; &quot;Bt2&quot; &quot;Bt3&quot; &quot;Bt4&quot; &quot;Bw&quot; &quot;Bw1&quot; &quot;Bw2&quot; ## [33] &quot;Bw3&quot; &quot;C&quot; &quot;CBt&quot; &quot;Cd&quot; &quot;Cr&quot; &quot;Cr/R&quot; &quot;Crt&quot; &quot;H1&quot; &quot;Oi&quot; &quot;R&quot; &quot;Rt&quot; If the unique() function returned typos such as “BT” or “B t”, you could either fix your original dataset or you could make an adjustment in R, such as: h$hzname &lt;- ifelse(h$hzname == &quot;BT&quot;, &quot;Bt&quot;, h$hzname) Typo errors such as these are a common problem with old pedon data in NASIS. 3.4 Exercise 1: fetch and inspect Create a new R script Load the gopheridge dataset found within the soilDB package or use your own data (highly encouraged) and inspect the dataset Apply the generalized horizon rules below or develop your own, see the following job-aid Summarize the hzdept, genhz, texture_class, sand, and fine gravel. Save your R script, and forward to your instructor. # gopheridge rules n &lt;- c(&#39;A&#39;, &#39;Bt1&#39;, &#39;Bt2&#39;, &#39;Bt3&#39;,&#39;Cr&#39;,&#39;R&#39;) p &lt;- c(&#39;^A|BA$&#39;, &#39;Bt1|Bw&#39;,&#39;Bt$|Bt2&#39;, &#39;Bt3|CBt$|BCt&#39;,&#39;Cr&#39;,&#39;R&#39;) 3.5 Descriptive Statistics Table 3.1: Short Description of Descriptive Statistics and R Functions Parameter NASIS Description R function Mean RV ? arithmetic average mean() Median RV middle value, 50% quantile median() Mode RV most frequent value sort(table(), decreasing = TRUE)[1] Standard Deviation L &amp; H ? variation around mean sd() Quantiles L &amp; H percent rank of values, such that all values are &lt;= p quantile() 3.5.1 Measures of Central Tendency These measures are used to determine the mid-point of the range of observed values. In NASIS speak this should ideally be equivalent to the representative value (RV) for numeric and integer data. The mean and median are the most commonly used measures for our purposes. Mean - is the arithmetic average all are familiar with, formally expressed as: \\(\\bar{x} =\\frac{\\sum_{i=1}^{n}x_i}{n}\\) which sums ( \\(\\sum\\) ) all the X values in the sample and divides by the number (n) of samples. It is assumed that all references in this document refer to samples rather than a population. The mean clay content from the loafercreek dataset may be determined: mean(h$clay, na.rm = TRUE) ## [1] 23.62767 Median is the middle measurement of a sample set, and as such is a more robust estimate of central tendency than the mean. This is known as the middle or 50th quantile, meaning there are an equal number of samples with values less than and greater than the median. For example, assuming there are 21 samples, sorted in ascending order, the median would be the 11th sample. The median from the sample dataset may be determined: median(h$clay, na.rm = TRUE) ## [1] 22 Mode - is the most frequent measurement in the sample. The use of mode is typically reserved for factors, which we will discuss shortly. One issue with using the mode for numeric data is that the data need to be rounded to the level of desired precision. R does not include a function for calculating the mode, but we can calculate it using the following example. # sort and select the 1st value, which will be the mode sort(table(round(h$clay)), decreasing = TRUE)[1] ## 25 ## 42 Frequencies To summarize factors and characters we can examine their frequency or number of observations. This is accomplished using the table() or summary() functions. table(h$genhz) ## ## A BAt Bt1 Bt2 Cr R not-used ## 113 40 206 118 75 48 26 # or summary(h$genhz) ## A BAt Bt1 Bt2 Cr R not-used ## 113 40 206 118 75 48 26 This gives us a count of the number of observations for each horizon. If we want to see the comparison between two different factors or characters, we can include two variables. table(h$genhz, h$texcl) ## ## c cl l scl sic sicl sil sl ## A 0 0 78 0 0 0 27 6 ## BAt 0 2 31 0 0 1 4 1 ## Bt1 2 44 127 4 1 5 20 1 ## Bt2 16 54 29 5 1 3 5 0 ## Cr 1 0 0 0 0 0 0 0 ## R 0 0 0 0 0 0 0 0 ## not-used 0 1 0 0 0 0 0 0 # or h %&gt;% count(genhz, texcl) ## genhz texcl n ## 1 A l 78 ## 2 A sil 27 ## 3 A sl 6 ## 4 A &lt;NA&gt; 2 ## 5 BAt cl 2 ## 6 BAt l 31 ## 7 BAt sicl 1 ## 8 BAt sil 4 ## 9 BAt sl 1 ## 10 BAt &lt;NA&gt; 1 ## 11 Bt1 c 2 ## 12 Bt1 cl 44 ## 13 Bt1 l 127 ## 14 Bt1 scl 4 ## 15 Bt1 sic 1 ## 16 Bt1 sicl 5 ## 17 Bt1 sil 20 ## 18 Bt1 sl 1 ## 19 Bt1 &lt;NA&gt; 2 ## 20 Bt2 c 16 ## 21 Bt2 cl 54 ## 22 Bt2 l 29 ## 23 Bt2 scl 5 ## 24 Bt2 sic 1 ## 25 Bt2 sicl 3 ## 26 Bt2 sil 5 ## 27 Bt2 &lt;NA&gt; 5 ## 28 Cr c 1 ## 29 Cr &lt;NA&gt; 74 ## 30 R &lt;NA&gt; 48 ## 31 not-used cl 1 ## 32 not-used &lt;NA&gt; 25 We can also add margin totals to the table or convert the table frequencies to proportions. # append the table with row and column sums addmargins(table(h$genhz, h$texcl)) ## ## c cl l scl sic sicl sil sl Sum ## A 0 0 78 0 0 0 27 6 111 ## BAt 0 2 31 0 0 1 4 1 39 ## Bt1 2 44 127 4 1 5 20 1 204 ## Bt2 16 54 29 5 1 3 5 0 113 ## Cr 1 0 0 0 0 0 0 0 1 ## R 0 0 0 0 0 0 0 0 0 ## not-used 0 1 0 0 0 0 0 0 1 ## Sum 19 101 265 9 2 9 56 8 469 # calculate the proportions relative to the rows, margin = 1 calculates for rows, margin = 2 calculates for columns, margin = NULL calculates for total observations table(h$genhz, h$texture_class) %&gt;% prop.table(margin = 1) %&gt;% round(2) * 100 ## ## br c cb cl gr l pg scl sic sicl sil sl spm ## A 0 0 0 0 0 70 0 0 0 0 24 5 0 ## BAt 0 0 0 5 0 79 0 0 0 3 10 3 0 ## Bt1 0 1 0 22 0 62 0 2 0 2 10 0 0 ## Bt2 0 14 1 46 2 25 1 4 1 3 4 0 0 ## Cr 97 2 0 0 2 0 0 0 0 0 0 0 0 ## R 100 0 0 0 0 0 0 0 0 0 0 0 0 ## not-used 0 0 0 4 0 0 0 0 0 0 0 0 96 To determine the mean by a group or category, use the group_by and summarize functions: h %&gt;% group_by(genhz) %&gt;% summarize(clay_avg = mean(clay, na.rm = TRUE), clay_med = median(clay, na.rm = TRUE) ) ## # A tibble: 7 × 3 ## genhz clay_avg clay_med ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 16.2 16 ## 2 BAt 19.5 19 ## 3 Bt1 24.0 24 ## 4 Bt2 31.2 30 ## 5 Cr 15 15 ## 6 R NaN NA ## 7 not-used NaN NA 3.5.2 Measures of Dispersion These are measures used to determine the spread of values around the mid-point. This is useful to determine if the samples are spread widely across the range of observations or concentrated near the mid-point. In NASIS speak these values might equate to the low (L) and high (H) values for numeric and integer data. Variance is a positive value indicating deviation from the mean: \\(s^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} {n - 1}\\) This is the square of the sum of the deviations from the mean, divided by the number of samples minus 1. It is commonly referred to as the sum of squares. As the deviation increases, the variance increases. Conversely, if there is no deviation, the variance will equal 0. As a squared value, variance is always positive. Variance is an important component for many statistical analyses including the most commonly referred to measure of dispersion, the standard deviation. Variance for the sample dataset is: var(h$clay, na.rm=TRUE) ## [1] 64.27607 Standard Deviation is the square root of the variance: \\(s = \\sqrt\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} {n - 1}\\) The units of the standard deviation are the same as the units measured. From the formula you can see that the standard deviation is simply the square root of the variance. Standard deviation for the sample dataset is: sd(h$clay, na.rm = TRUE) ## [1] 8.017236 # or sqrt(var(h$clay, na.rm = TRUE)) ## [1] 8.017236 Coefficient of Variation (CV) is a relative (i.e., unitless) measure of standard deviation: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\) CV is calculated by dividing the standard deviation by the mean and multiplying by 100. Since standard deviation varies in magnitude with the value of the mean, the CV is useful for comparing relative variation amongst different datasets. However Webster (2001) discourages using CV to compare different variables. Webster (2001) also stresses that CV is reserved for variables that have an absolute 0, like clay content. CV may be calculated for the sample dataset as: # remove NA values and create a new variable clay &lt;- na.exclude(h$clay) sd(clay) / mean(clay) * 100 ## [1] 33.93156 Quantiles (a.k.a. Percentiles) - the percentile is the value that cuts off the first nth percent of the data values when sorted in ascending order. The default for the quantile() function returns the min, 25th percentile, median or 50th percentile, 75th percentile, and max, known as the five number summary originally proposed by Tukey. Other probabilities however can be used. At present the 5th, 50th, and 95th are being proposed for determining the range in characteristics (RIC) for a given soil property. quantile(h$clay, na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 10 18 22 28 60 # or quantile(clay, probs = c(0.05, 0.5, 0.95), na.rm = TRUE) ## 5% 50% 95% ## 13.0 22.0 38.1 Thus, for the five number summary 25% of the observations fall between each of the intervals. Quantiles are a useful metric because they are largely unaffected by the distribution of the data, and have a simple interpretation. Range is the difference between the highest and lowest measurement of a group. Using the sample data it may be determined as: range(clay) ## [1] 10 60 which returns the minimum and maximum values observed, or: diff(range(clay)) ## [1] 50 # or max(clay) - min(clay) ## [1] 50 Interquartile Range (IQR) is the range from the upper (75%) quartile to the lower (25%) quartile. This represents 50% of the observations occurring in the mid-range of a sample. IQR is a robust measure of dispersion, unaffected by the distribution of data. In soil survey lingo you could consider the IQR to estimate the central concept of a soil property. IQR may be calculated for the sample dataset as: IQR(clay) ## [1] 10 # or diff(quantile(clay, p = c(0.25, 0.75))) ## 75% ## 10 3.5.3 Correlation A correlation matrix is a table of the calculated correlation coefficients of all variables. This provides a quantitative measure to guide the decision making process. The following will produce a correlation matrix for the sp4 dataset: h$hzdepm &lt;- (h$hzdepb + h$hzdept) / 2 # Compute the middle horizon depth h %&gt;% select(hzdepm, clay, sand, total_frags_pct, phfield) %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% round(2) ## hzdepm clay sand total_frags_pct phfield ## hzdepm 1.00 0.59 -0.08 0.50 -0.03 ## clay 0.59 1.00 -0.17 0.28 0.13 ## sand -0.08 -0.17 1.00 -0.05 0.12 ## total_frags_pct 0.50 0.28 -0.05 1.00 -0.16 ## phfield -0.03 0.13 0.12 -0.16 1.00 As seen in the output, variables are perfectly correlated with themselves and have a correlation coefficient of 1.0. Negative values indicate a negative relationship between variables. What is considered highly correlated? A good rule of thumb is anything with a value of 0.7 or greater is considered highly correlated. 3.6 Exercise 2: Compute Descriptive Statistics Add to your existing R script from Exercise 1. Aggregate by genhz and calculate several descriptive statistics for hzdept, gravel and phfield. Cross-tabulate geomposhill and argillic.horizon from the site table, as a percentage. Compute a correlation matrix between hzdept, gravel and phfield. Save your R script, and forward to your instructor. 3.7 Graphical Methods Now that we’ve checked for missing values and typos and made corrections, we can graphically examine the sample data distribution of our data. Frequency distributions are useful because they can help us visualize the center (e.g., RV) and spread or dispersion (e.g., low and high) of our data. Typically in introductory statistics the normal (i.e., Gaussian) distribution is emphasized. Table 3.2: Short Description of Graphical Methods Plot Types Description Bar a plot where each bar represents the frequency of observations for a ‘group’ Histogram a plot where each bar represents the frequency of observations for a ‘given range of values’ Density an estimation of the frequency distribution based on the sample data Quantile-Quantile a plot of the actual data values against a normal distribution Box-Whisker a visual representation of median, quartiles, symmetry, skewness, and outliers Scatter &amp; Line a graphical display of one variable plotted on the x axis and another on the y axis Table 3.3: Comparison of R’s 3 Graphing Systems and their Equivalent Functions for Plotting Plot Types Base R lattice ggplot geoms Bar barplot() barchart() geom_bar() Histogram hist() histogram() geom_histogram() Density plot(density()) densityplot() geom_density() Quantile-Quantile qqnorm() qq() geom_qq() Box-Whisker boxplot() bwplot() geom_boxplot() Scatter &amp; Line plot() xyplot geom_point() 3.7.1 Distributions 3.7.2 Bar Plot A bar plot is a graphical display of the frequency (i.e. number of observations (count or n)), such as soil texture, that fall within a given class. It is a graphical alternative to to the table() function. library(ggplot2) # bar plot ggplot(h, aes(x = texcl)) + geom_bar() 3.7.3 Histogram A histogram is similar to a bar plot, except that instead of summarizing categorical data, it categorizes a continuous variable like clay content into non-overlappying intervals for the sake of display. The number of intervals can be specified by the user, or can be automatically determined using an algorithm, such as nclass.Sturges(). Since histograms are dependent on the number of bins, for small datasets they’re not the best method of determining the shape of a distribution. ggplot(h, aes(x = clay)) + geom_histogram(bins = nclass.Sturges(h$clay)) 3.7.4 Density Curve A density estimation, also known as a Kernel density plot, generally provides a better visualization of the shape of the distribution in comparison to the histogram. Compared to the histogram where the y-axis represents the number or percent (i.e., frequency) of observations, the y-axis for the density plot represents the probability of observing any given value, such that the area under the curve equals one. One curious feature of the density curve is the hint of two peaks (i.e. bimodal distribution?). Given that our sample includes a mixture of surface and subsurface horizons, we may have two different populations. However, considering how much the two distributions overlap, it seems impractical to separate them in this instance. ggplot(h, aes(x = clay)) + geom_density() 3.7.5 Box plots Box plots are a graphical representation of the five number summary, depicting quartiles (i.e. the 25%, 50%, and 75% quantiles), minimum, maximum and outliers (if present). Boxplots convey the shape of the data distribution, the presence of extreme values, and the ability to compare with other variables using the same scale, providing an excellent tool for screening data, determining thresholds for variables and developing working hypotheses. The parts of the boxplot are shown in the figure below. The “box” of the boxplot is defined as the 1st quartile and the 3rd quartile. The median, or 2nd quartile, is the dark line in the box. The whiskers (typically) show data that is 1.5 * IQR above and below the 3rd and 1st quartile. Any data point that is beyond a whisker is considered an outlier. That is not to say the outlier points are in error, just that they are extreme compared to the rest of the data set. However, you may want to evaluate these points to ensure that they are correct. ggplot(h, aes(x = genhz, y = clay)) + geom_boxplot() The above box plot shows a steady increase in clay content with depth. Notice the outliers in the box plots, identified by the individual circles. 3.7.6 Quantile comparison plots (QQplot) A QQ plot is a plot of the actual data values against a normal distribution (which has a mean of 0 and standard deviation of 1). # QQ Plot for Clay ggplot(h, aes(sample = clay)) + geom_qq() + geom_qq_line() # QQ Plot for Frags ggplot(h, aes(sample = total_frags_pct)) + geom_qq() + geom_qq_line() If the data set is perfectly symmetric (i.e. normal), the data points will form a straight line. Overall this plot shows that our clay example is more or less symmetric. However the second plot shows that our rock fragments are far from evenly distributed. A more detailed explanation of QQ plots may be found on Wikipedia: https://en.wikipedia.org/wiki/QQ_plot 3.7.7 The ‘Normal’ distribution What is a normal distribution and why should you care? Many statistical methods are based on the properties of a normal distribution. Applying certain methods to data that are not normally distributed can give misleading or incorrect results. Most methods that assume normality are robust enough for all data except the very abnormal. This section is not meant to be a recipe for decision making, but more an extension of tools available to help you examine your data and proceed accordingly. The impact of normality is most commonly seen for parameters used by pedologists for documenting the ranges of a variable (i.e., Low, RV and High values). Often a rule-of thumb similar to: “two standard deviations” is used to define the low and high values of a variable. This is fine if the data are normally distributed. However, if the data are skewed, using the standard deviation as a parameter does not provide useful information of the data distribution. The quantitative measures of Kurtosis (peakedness) and Skewness (symmetry) can be used to assist in accessing normality and can be found in the fBasics package, but [Webster (2001)) cautions against using significance tests for assessing normality. The preceding sections and chapters will demonstrate various methods to cope with alternative distributions. A Gaussian distribution is often referred to as “Bell Curve”, and has the following properties: Gaussian distributions are symmetric around their mean The mean, median, and mode of a Gaussian distribution are equal The area under the curve is equal to 1.0 Gaussian distributions are denser in the center and less dense in the tails Gaussian distributions are defined by two parameters, the mean and the standard deviation 68% of the area under the curve is within one standard deviation of the mean Approximately 95% of the area of a Gaussian distribution is within two standard deviations of the mean Viewing a histogram or density plot of your data provides a quick visual reference for determining normality. Distributions are typically normal, Bimodal or Skewed: Examples of different types of distributions Occasionally distributions are Uniform, or nearly so: With the loafercreek dataset the mean and median for clay were only slightly different, so we can safely assume that we have a normal distribution. However many soil variables often have a non-normal distribution. For example, let’s look at graphical examination of the mean vs. median for clay and rock fragments: The solid lines represent the breakpoint for the mean and standard deviations. The dashed lines represents the median and quantiles. The median is a more robust measure of central tendency compared to the mean. In order for the mean to be a useful measure, the data distribution must be approximately normal. The further the data departs from normality, the less meaningful the mean becomes. The median always represents the same thing independent of the data distribution, namely, 50% of the samples are below and 50% are above the median. The example for clay again indicates that distribution is approximately normal. However for rock fragments, we commonly see a long tailed distribution (e.g., skewed). Using the mean in this instance would overestimate the rock fragments. Although in this instance the difference between the mean and median is only 9 percent. 3.7.8 Scatterplots and Line Plots Plotting points of one ratio or interval variable against another is a scatter plot. Plots can be produced for a single or multiple pairs of variables. Many independent variables are often under consideration in soil survey work. This is especially common when GIS is used, which offers the potential to correlate soil attributes with a large variety of raster datasets. The purpose of a scatterplot is to see how one variable relates to another. With modeling in general the goal is parsimony (i.e., simple). The goal is to determine the fewest number of variables required to explain or describe a relationship. If two variables explain the same thing, i.e., they are highly correlated, only one variable is needed. The scatterplot provides a perfect visual reference for this. Create a basic scatter plot using the loafercreek dataset. # scatter plot ggplot(h, aes(x = clay, y = hzdepm)) + geom_point() + ylim(100, 0) # line plot ggplot(h, aes(y = clay, x = hzdepm, group = peiid)) + geom_line() + coord_flip() + xlim(100, 0) This plots clay on the X axis and depth on the X axis. As shown in the scatterplot above, there is a moderate correlation between these variables. The function below produces a scatterplot matrix for all the numeric variables in the dataset. This is a good command to use for determining rough linear correlations for continuous variables. library(GGally) h %&gt;% select(hzdepm, clay, phfield, total_frags_pct) %&gt;% ggpairs() 3.7.9 3rd Dimension - Color, Shape, Size, Layers, etc… 3.7.9.1 Color and Groups # scatter plot ggplot(h, aes(x = clay, y = hzdepm, color = genhz)) + geom_point(size = 3) + ylim(100, 0) # density plot ggplot(h, aes(x = clay, color = genhz)) + geom_density(size = 2) # bar plot ggplot(h, aes(x = genhz, fill = texture_class)) + geom_bar() # box plot ggplot(h, aes(x = genhz, y = clay)) + geom_boxplot() # heat map (pseudo bar plot) s &lt;- aqp::site(loafercreek) ggplot(s, aes(x = landform_string, y = pmkind)) + geom_tile(alpha = 0.2) 3.7.9.2 Facets - box plots library(tidyr) # convert to long format df &lt;- h %&gt;% select(peiid, genhz, hzdepm, clay, phfield, total_frags_pct) %&gt;% pivot_longer(cols = c(&quot;clay&quot;, &quot;phfield&quot;, &quot;total_frags_pct&quot;)) ggplot(df, aes(x = genhz, y = value)) + geom_boxplot() + xlab(&quot;genhz&quot;) + facet_wrap(~ name, scales = &quot;free_y&quot;) 3.7.9.3 Facets - depth plots data(loafercreek, package = &quot;soilDB&quot;) s &lt;- aqp::slab(loafercreek, fm = ~ clay + phfield + total_frags_pct, slab.structure = 0:100, slab.fun = function(x) quantile(x, c(0.1, 0.5, 0.9), na.rm = TRUE)) ggplot(s, aes(x = top, y = X50.)) + # plot median geom_line() + # plot 10th &amp; 90th quantiles geom_ribbon(aes(ymin = X10., ymax = X90., x = top), alpha = 0.2) + # invert depths xlim(c(100, 0)) + # flip axis coord_flip() + facet_wrap(~ variable, scales = &quot;free_x&quot;) 3.8 Exercise 3: Graphical Methods Add to your existing R script from Exercise 2. Create a faceted boxplot of genhz vs gravel and phfield. Create a facted depth plot for gravel and phfield Save your R script, and forward to your instructor. 3.9 Transformations Slope aspect and pH are two common variables warranting special consideration for pedologists. 3.9.1 pH There is a recurring debate as to the best way to average pH since is it a log transformed variable. Remember, pHs of 6 and 5 correspond to hydrogen ion concentrations of 0.000001 and 0.00001 respectively. The actual average is 5.26; -log10((0.000001 + 0.00001) / 2). If no conversions are made for pH, the mean and sd in the summary are considered the geometric mean and sd, not the arithmetic. The wider the pH range, the greater the difference between the geometric and arithmetic mean. The difference between the arithmetic average of 5.26 and the geometric average of 5.5 is small. Boyd, Tucker, and Viriyatum (2011) examined the issue in detail, and suggests that regardless of the method is used it should be documented. If you have a table with pH values and wish to calculate the arithmetic mean using R, this example will work: # arithmetic mean log10(mean(1/10^-h$phfield, na.rm = TRUE)) ## [1] 6.371026 # geometric mean mean(h$phfield, na.rm = TRUE) ## [1] 6.18 3.9.2 Circular data Slope aspect - requires the use of circular statistics for numerical summaries, or graphical interpretation using circular plots. For example, if soil map units being summarized have a uniform distribution of slope aspects ranging from 335 degrees to 25 degrees, the Zonal Statistics tool in ArcGIS would return a mean of 180. The most intuitive means available for evaluating and describing slope aspect are circular plots available with the circular package in R and the radial plot option in the TEUI Toolkit. The circular package in R will also calculate circular statistics like mean, median, quartiles etc. library(circular) # Extract the site table s &lt;- aqp::site(loafercreek) aspect &lt;- s$aspect_field aspect &lt;- circular(aspect, template=&quot;geographic&quot;, units=&quot;degrees&quot;, modulo=&quot;2pi&quot;) summary(aspect) ## n Min. 1st Qu. Median Mean 3rd Qu. Max. Rho NA&#39;s ## 101.0000 12.0000 255.0000 195.0000 199.5000 115.0000 20.0000 0.1772 5.0000 The numeric output is fine, but a following graphic is more revealing, which shows the dominant Southwest slope aspect. rose.diag(aspect, bins = 8, col=&quot;grey&quot;) 3.9.3 Texture Class and Fine-earth Separates Many pedon descriptions include soil texture class and modifiers, but lack numeric estimates such as clay content and rock fragments percentage. This lack of “continuous” numeric data makes it difficult to analyze and estimate certain properties precisely. While numeric data on textural separates may be missing, it can still be estimated by the class ranges and averages. NASIS has many calculations used to estimate missing values. To facilitate this process in R, several new functions have recently been added to the aqp package. These new aqp functions are intended to impute missing values or check existing values. The ssc_to_texcl() function uses the same logic as the particle size estimator calculation in NASIS to classify sand and clay into texture class. The results are stored in data(soiltexture) and used by texcl_to_ssc() as a lookup table to convert texture class to sand, silt and clay. The function texcl_to_ssc() replicates the functionality described by Levi (2017). Unlike the other functions, texture_to_taxpartsize() is intended to be computed on weighted averages within the family particle size control section. Below is a demonstration of these new aqp R functions. library(aqp) library(soiltexture) # example of texcl_to_ssc(texcl) texcl &lt;- c(&quot;s&quot;, &quot;ls&quot;, &quot;l&quot;, &quot;cl&quot;, &quot;c&quot;) test &lt;- texcl_to_ssc(texcl) head(cbind(texcl, test)) # example of ssc_to_texcl() ssc &lt;- data.frame(CLAY = c(55, 33, 18, 6, 3), SAND = c(20, 33, 42, 82, 93), SILT = c(25, 34, 40, 12, 4) ) texcl &lt;- ssc_to_texcl(sand = ssc$SAND, clay = ssc$CLAY ) ssc_texcl &lt;- cbind(ssc, texcl) head(ssc_texcl) # plot on a textural triangle TT.plot(class.sys = &quot;USDA-NCSS.TT&quot;, tri.data = ssc_texcl, pch = 16, col = &quot;blue&quot; ) # example of texmod_to_fragvoltol() frags &lt;- c(&quot;gr&quot;, &quot;grv&quot;, &quot;grx&quot;, &quot;pgr&quot;, &quot;pgrv&quot;, &quot;pgrx&quot;) test &lt;- texmod_to_fragvoltot(frags)[1:4] head(test) # example of texture_to_taxpartsize() tex &lt;- data.frame(texcl = c(&quot;c&quot;, &quot;cl&quot;, &quot;l&quot;, &quot;ls&quot;, &quot;s&quot;), clay = c(55, 33, 18, 6, 3), sand = c(20, 33, 42, 82, 93), fragvoltot = c(35, 15, 34, 60, 91) ) tex$fpsc &lt;- texture_to_taxpartsize( texcl = tex$texcl, clay = tex$clay, sand = tex$sand, fragvoltot = tex$fragvoltot ) head(tex) 3.10 The Shiny Package Shiny is an R package which combines R programming with the interactivity of the web. install.packages(&quot;shiny&quot;) Methods for Use Online Locally The shiny package, created by RStudio, enables users to not only use interactive applications created by others, but to build them as well. 3.10.1 Online Easiest Method Click a Link: https://gallery.shinyapps.io/lake_erie_fisheries_stock_assessment_app/ Open a web browser Navigate to a URL The ability to use a shiny app online is one of the most useful features of the package. All of the R code is executed on a remote computer which sends the results over a live http connection. Very little is required of the user in order to obtain results. 3.10.2 Locally No Internet required once configured Install R and RStudio (done) Install Required packages (app dependent) Download, open in RStudio and click “Run App” The online method may be easy for the user, but it does require a significant amount of additional effort for the creator. We won’t get into those details here! The local method, however simply requires the creator to share a single app.R file. It is the user which needs to put forth the additional effort. 3.10.3 Web App Demonstration Online: https://usda.shinyapps.io/r11_app Local: https://github.com/ncss-tech/vitrusa/raw/master/r11_smp_app/app.R Online apps such as the Region 11 Web App are useful tools, available for all to use during soil survey, ecological site development, or other evaluations. The Region 11 app is however limited to data which is already available online, such as SDA (Soil Data Access) and NASIS (National Soil Information System) Web Reports. It is also relient on the successful operation of those data systems. If the NASIS Web Reports or SDA is down for maintanence, the app fails. Local apps have the ability to leverage local data systems more easily like NASIS or other proprietary data. 3.10.4 Troubleshooting Errors Reload the app and try again. (refresh browser, or click stop, and run app again in RStudio) When the app throws an error, it stops. All other tabs/reports will no longer function until the app is reloaded. Read the getting started section on the home page. This is a quick summary of tips to avoid issues, and will be updated as needed. Check to see if SDA and NASIS Web Reports are operational, if they aren’t working, then the app won’t work either. Double check your query inputs. (typos, wildcards, null data, and too much data, can be common issues) 5 minutes of inactivity will cause the connection to drop, be sure you are actively using the app. Run the app locally - the online app does not show console activity, which can be a big help when identifying problems. Check the app issues page to see if the issue you are experiencing is already documented. (Polite but not required) Contact the app author (john.hammerly@usda.gov) When you run into trouble, there are a few steps you can take on your own to get things working again. This list may help you get your issue resolved. If not, contact (john.hammerly@usda.gov) for assistance. 3.10.5 Shiny App Embedding Shiny apps are extremely versatile, they can be embedded into presentations, Markdown, or HTML Those same formats can also be embedded in to a shiny app. This is a very simple example of a shiny app which consists of an interactive dropdown menu which controls what region is displayed in the bar chart. Let’s take a look at the code. 3.10.5.1 Shiny App Code shinyApp( # Use a fluid Bootstrap layout ui = fluidPage( # Give the page a title titlePanel(&quot;Telephones by region&quot;), # Generate a row with a sidebar sidebarLayout( # Define the sidebar with one input sidebarPanel( selectInput(&quot;region&quot;, &quot;Region:&quot;, choices = colnames(datasets::WorldPhones)), hr(), helpText(&quot;Data from AT&amp;T (1961) The World&#39;s Telephones.&quot;) ), # Create a spot for the barplot mainPanel( plotOutput(&quot;phonePlot&quot;) ) ) ), # Define a server function for the Shiny app server = function(input, output) { # Fill in the spot we created for a plot output$phonePlot &lt;- renderPlot({ # Render a barplot barplot( datasets::WorldPhones[, input$region] * 1000, main = input$region, ylab = &quot;Number of Telephones&quot;, xlab = &quot;Year&quot; ) }) } ) Shiny apps consist of a ui and a server. The ui is the part of the shiny app the user sees, the user interface. In the ui, a user can choose or enter inputs for processing and viewing the results. The server takes the inputs, performs some data processing and rendering of those inputs and generates the outputs for the ui to display. 3.10.6 Questions What new features in RStudio are available for you to use once the shiny package is installed? The Region 11 Web App uses which two data sources for reports? If an error occurs while using the Region 11 Web App, what should you do? Poll: A shiny app consists of: 3.10.7 Examples 3.10.7.1 NASIS Reports The NASIS Reports button is a link to a master list of NASIS Web reports for Regions 10 and 11. 3.10.7.2 Water Table The query method option allows you to choose between MUKEY, NATSYM, MUNAME. It also has a radio button for switching between flooding and ponding. Plots and Data Tables are on separate sub-menu items. 3.10.7.3 Organic Matter Same options as the Water Table Tab except no radio button. The query method option allows you to choose between MUKEY, NATSYM, MUNAME. Plots and Data Tables are on separate sub-menu items. 3.10.7.4 Project Report The project report can accept multiple projects. Use the semicolon (;) as a separator. You can also save a copy of the report by clicking the link below the submit button. 3.10.7.5 Project Extent Type in Year, type or select office and type project name for the Project extent query. Zoom and pan to view extent. Use the layers button to change the basemap or toggle layers. Click the link below the submit button to download a .zip containing the extent as a ESRI shapefile. 3.10.7.6 Long Range Plan Enter an office symbol to generate a long range plan report. 3.10.7.7 Interpretations Enter the national mapunit symbol to plot all available interpretations for the mapunit from SDA. 3.10.8 Work on your Own 3.10.8.1 Project Report Use the project report to generate a report on a project in your own area. Save the results and explain the results of pH plots for one of your components. 3.10.8.2 Project Extent Map an extent of a project. How many layers are available to choose from as a basemap? How many layers can be toggled on or off? 3.10.8.3 Long Range Plan Choose an office to generate a long range plan. What is the highest acreage project for 2025? ## Soil Reports One of the strengths of NASIS is that it that has many queries and reports to access the complex data. This makes it easy for the average user to load their data, process it and run numerous reports. 3.11 soilReports The soilReports R package is essentially just a collection of R Markdown (.Rmd) documents. R Markdown is a plain text markup format for creating reproducible, dynamic reports with R. These .Rmd files can be used to generate HTML, PDF, Word, Markdown documents with a variety of forms, templates and applications. Example report output can be found at the following link: https://github.com/ncss-tech/soilReports#example-output. Detailed instructions are provided for each report: https://github.com/ncss-tech/soilReports#choose-an-available-report Install the soilReports package. This package is updated regularly and should be installed from GitHub. # Install the soilReports package from GitHub remotes::install_github(&quot;ncss-tech/soilReports&quot;, dependencies = FALSE, build = FALSE) To view the list of available reports first load the package then use the listReports() function. # Load the soilReports package library(soilReports) # List reports listReports() ## name version ## 1 national/DT-report 1.0 ## 2 region11/component_summary_by_project 0.1 ## 3 region11/lab_summary_by_taxonname 1.0 ## 4 region11/mupolygon_summary_by_project 0.1 ## 5 region11/pedon_summary_by_taxonname 1 ## 6 region2/dmu-diff 0.7 ## 7 region2/dmu-summary 0.3 ## 8 region2/edit-soil-features 0.2.1 ## 9 region2/gdb-acreage 1.0 ## 10 region2/mlra-comparison-dynamic 0.1 ## 11 region2/mlra-comparison 1.0 ## 12 region2/mu-comparison-dashboard 0.0.0 ## 13 region2/mu-comparison 4.0.1 ## 14 region2/mu-summary 1 ## 15 region2/pedon-summary 0.9 ## 16 region2/QA-summary 0.6 ## 17 region2/shiny-pedon-summary 1.0 ## 18 region2/spatial-pedons 1.0 ## 19 templates/minimal 1.0 ## description ## 1 Create interactive data tables from CSV files ## 2 summarize component data for an MLRA project ## 3 summarize lab data from NASIS Lab Layer table ## 4 summarize mupolygon layer from a geodatabase ## 5 summarize field pedons from NASIS pedon table ## 6 Differences between select DMU ## 7 DMU Summary Report ## 8 Generate summaries of NASIS components for EDIT Soil Features sections ## 9 Geodatabase Mapunit Acreage Summary Report ## 10 compare MLRA/LRU-scale delineations, based on mu-comparison report ## 11 compare MLRA using pre-made, raster sample databases ## 12 interactively subset and summarize SSURGO data for input to `region2/mu-comparison` report ## 13 compare stack of raster data, sampled from polygons associated with 1-8 map units ## 14 summarize raster data for a large collection of map unit polygons ## 15 Generate summaries from pedons (NASIS) and associated GIS data. ## 16 QA Summary Report ## 17 Interactively subset and summarize NASIS pedon data from one or more map units ## 18 Visualize NASIS pedons in interactive HTML report with overlays of SSURGO, STATSGO or custom polygons ## 19 A minimal soilReports template 3.11.1 Extending soilReports Each report in soilReports has a “manifest” that describes any dependencies, configuration files or inputs for your R Markdown report document. If you can identify these things it is easy to convert your own R-based analyses to the soilReports format. The .Rmd file allows R code and text with Markdown markup to be mingled in the same document and then “executed” like an R script. 3.11.2 Exercise: Run Shiny Pedon Summary The region2/shiny-pedon-summary report is an interactive Shiny-based report that uses flexdashboard to help the user subset and summarize NASIS pedons from a graphical interface. You can try a ShinyApps.io demo here The “Shiny Pedon Summary” allows one to rapidly generate reports from a large set of pedons in their NASIS selected set. The left INPUT sidebar has numerous options for subsetting pedon data. Specifically, you can change REGEX patterns for mapunit symbol, taxon name, local phase, and User Pedon ID. Also, you can use the drop down boxes to filter on taxon kind or compare different “modal”/RV pedons. Example: Analyzing the Loafercreek Taxadjuncts Create an instance of the region2/shiny-pedon-summary report with soilReports: # create new instance of reports library(soilReports) # set path for shiny-pedon-summary report instance shinyped.path &lt;- &quot;C:/workspace2/chapter3/shiny-pedon&quot; # create directories (if needed) if(!dir.exists(shinyped.path)) dir.create(shinyped.path, recursive = TRUE) # get report dependencies reportSetup(&#39;region2/shiny-pedon-summary&#39;) # copy report contents to target path copyReport(&#39;region2/shiny-pedon-summary&#39;, outputDir = shinyped.path, overwrite = TRUE) Update the config.R file You can update the config.R file in “C:/workspace2/chapter3/shiny-pedon” (or wherever you installed the report) to use the soilDB datasets loafercreek and gopheridge by setting demo_mode &lt;- TRUE. This is the simplest way to demonstrate how this report works. Alternately, when demo_mode &lt;- FALSE, pedons will be loaded from your NASIS selected set. config.R also allows you to specify a shapefile for overlaying the points on – to determine mapunit symbol – as well as several raster data sources whose values will be extracted at point locations and summarized. The demo dataset does not use either of these by default, due to large file sizes. Furthermore, a default (very general) set of REGEX generalized horizon patterns is provided to assign generalized horizon labels for provisional grouping. These provided patterns are unlikely to cover ALL cases, and always need to be modified for final correlation. That said, they do a decent job of making a first-pass correlation for diverse types of soils. The default config.R settings use the general patterns: use_regex_ghz &lt;- TRUE. You are welcome to modify the defaults. If you want to use the values you have populated in NASIS Pedon Horizon Component Layer ID, set use_regex_ghz &lt;- FALSE. Run the report via shiny.Rmd This report uses the Shiny flexdashboard interface. Open up shiny.Rmd and click the “Run Document” button to start the report. This will load the pedon and spatial data specified in config.R. NOTE: If a Viewer Window does not pop-up right away, click the gear icon to the right of the “Run Document” button. Be sure the option “Preview in Window” is checked, then click “Run Document” again. All of the subset parameters are in the left-hand sidebar. Play around with all of these options – the graphs and plots in the tabs to the right will automatically update as you make changes. When you like what you have, you can export a non-interactive HTML file for your records. To do this, first, set the “Report name:” box to something informative – this will be your report output file name. Then, scroll down to the bottom of the INPUT sidebar and click “Export Report” button. Check the “output” folder (subdirectory of where you installed the report) for your results. 3.11.3 Exercise: Run Mapunit Comparison Another popular report in soilReports is the region2/mu-comparison report. This report uses constant density sampling (sharpshootR::constantDensitySampling()) to extract numeric and categorical values from multiple raster data sources that overlap a set of user-supplied polygons. In this example, we clip a small portion of SSURGO polygons from the CA630 soil survey area extent. We then select a small set of mapunit symbols (5012, 3046, 7083, 7085, 7088) that occur within the clipping extent. These mapunits have soil forming factors we expect to contrast with one another in several ways. You can inspect other mapunit symbols by changing mu.set in config.R. Download the demo data: # set up ch4 path and path for report mucomp.path &lt;- &quot;C:/workspace2/chapter3/mucomp&quot; # create any directories that may be missing if(!dir.exists(mucomp.path)) { dir.create(mucomp.path, recursive = TRUE) } mucomp.zip &lt;- file.path(mucomp.path, &#39;mucomp-data.zip&#39;) # download raster data, SSURGO clip from CA630, and sample script for clipping your own raster data download.file(&#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_3-mucomp-data/mucomp-data.zip&#39;, mucomp.zip) unzip(mucomp.zip, exdir = mucomp.path, overwrite = TRUE) Create an instance of the region2/mu-comparison report with soilReports: # create new instance of reports library(soilReports) # get report dependencies reportSetup(&#39;region2/mu-comparison&#39;) # create report instance copyReport(&#39;region2/mu-comparison&#39;, outputDir = mucomp.path, overwrite = TRUE) If you want, you can now set up the default config.R that is created by copyReport() to work with your own data. OR you can use the “sample” config.R file (called new_config.R) in the ZIP file downloaded above. Run this code to replace the default config.R with the sample data config.R: # copy config file containing relative paths to rasters downloaded above file.copy(from = file.path(mucomp.path, &quot;new_config.R&quot;), to = file.path(mucomp.path, &quot;config.R&quot;), overwrite = TRUE) Open report.Rmd in the C:/workspace2/chapter3/mucomp folder and click the “Knit” button at the top of the RStudio source pane to run the report. Inspect the report output HTML file, as well as the spatial and tabular data output in the output folder. Question: What differences can you see between the five mapunit symbols that were analyzed? 3.11.4 Exercise: Run Lab Summary By Taxon Name Soil Report For another exercise you can use the region11/lab_summary_by_taxonname report report to summarize laboratory data for a soil series. This report requires you to get some data from the Pedon “NCSS Lab” tables in NASIS. 3.11.4.1 Requirements Data are properly populated, otherwise the report may fail. Common examples include: Horizon depths don’t lineup Either the Pedon or Site tables isn’t loaded ODBC connection to NASIS is set up Beware each report has a unique configuration file that may need to be edited. 3.11.4.2 Instructions Load your NASIS selected set. Run a query such as “POINT - Pedon/Site/NCSSlabdata by upedonid and Taxon Name” from the Region 11 report folder to load your selected set. Be sure to target both the site, pedon and lab layer tables. Remove from your selected set the pedons and sites you wish to exclude from the report. Copy the lab summary to your working directory. copyReport(reportName = &quot;region11/lab_summary_by_taxonname&quot;, outputDir = &quot;C:/workspace2/lab_sum&quot;) Examine the report folder contents. The report is titled report.Rmd. Notice there are several other support files. The parameters for the report are contained in the config.R file. Check or create a genhz_rules file for a soil series. In order to aggregate the pedons by horizon designation, a genhz_rules file (e.g., Miami_rules.R) is needed. See above. If none exists see the following job aid on how to create one, Assigning Generalized Horizon Labels. Pay special attention to how caret ^ and dollar $ symbols are used in REGEX. They function as anchors for the beginning and end of the string, respectively. A ^ placed before an A horizon, ^A, will match any horizon designation that starts with A, such as Ap, Ap1, but not something merely containing A, such as BA. Placing a $ after a Bt horizon, Bt$, will match any horizon designation that ends with Bt, such as 2Bt or 3Bt, but not something with a vertical subdivision, such as Bt2. Wrapping pattern with both ^ and $ symbols will result only in exact matches – i.e. that start and end with the contents between ^ and $. For example ^[AC]$, will only match A or C, not Ap, Ap2, or Cg. Execute the report. Command-line approach # Set report parameters series &lt;- &quot;Miami&quot; genhz_rules &lt;- &quot;C:/workspace2/lab_sum/Miami_rules.R&quot; # report file path report_path &lt;- &quot;C:/workspace2/lab_sum/report.Rmd&quot; # Run the report render(input = report_path, output_dir = &quot;C:/workspace2&quot;, output_file = &quot;C:/workspace2/lab_sum.html&quot;, envir = new.env(), params = list(series = series, genhz_rules = genhz_rules ) ) Manual approach Open the report.Rmd, hit the Knit drop down arrow, and select Knit with Parameters. Save the report. The report is automatically saved upon creation in the same folder as the R report. However, it is given the same generic name as the R report (i.e., “C:/workspace/lab_sum/report.html”), and will be overwritten the next time the report is run. Therefore, if you wish to save the report, rename the .html file to a name of your choosing and/or convert it to a PDF. Also, beware when opening the .html file with Internet Explorer – be sure to click on “Allow blocked content” if prompted. Otherwise, Internet Explorer will alter the formatting of tables etc. within the document. Sample pedon report Brief summary of steps: Load your selected set with the pedon and site table for an existing GHL file, or make your own (highly encouraged) Run the lab_summary_by_taxonname.Rmd report on a soil series of your choice. Show your work and submit the results to your mentor. 3.12 Additional Reading (Exploratory Data Analysis) Healy, K., 2018. Data Visualization: a practical introduction. Princeton University Press. http://socviz.co/ Helsel, D.R., Hirsch, R.M., Ryberg, K.R., Archfield, S.A., and Gilroy, E.J., 2020, Statistical methods in water resources: U.S. Geological Survey Techniques and Methods, book 4, chap. A3, 458 p., https://doi.org/10.3133/tm4a3. [Supersedes USGS Techniques of Water-Resources Investigations, book 4, chap. A3, version 1.1.] Kabacoff, R.I., 2018. Data Visualization in R. https://rkabacoff.github.io/datavis/ Peng, R. D., 2016. Exploratory Data Analysis with R. Leanpub. https://bookdown.org/rdpeng/exdata/ Wilke, C.O., 2019. Fundamentals of Data Visualization. O’Reily Media, Inc. https://clauswilke.com/dataviz/ References "],["spatial.html", "Chapter 4 Spatial Data in R 4.1 Objectives (Spatial Data) 4.2 Making Maps with R 4.3 Spatial Data Sources 4.4 Viewing Pedon Locations 4.5 Many Packages, Many Spatial Representations 4.6 Coordinate Reference Systems (CRS) 4.7 Raster data 4.8 Spatial Overlay Operations 4.9 Additional Reading (Spatial)", " Chapter 4 Spatial Data in R This chapter is a brief demonstration of possible ways to process spatial data in R. 4.1 Objectives (Spatial Data) Gain experience with creating, editing, and exporting spatial data objects in R. Learn about making maps with R Learn the basics of sf and sp representation of vector data Learn the basics of raster classes and functions Learn about some interfaces to NCSS spatial data sources Develop a strategy for navigating the many possible spatial data processing methods The next sections will require loading these libraries into the R session. # SPC and soil database interface library(aqp) library(soilDB) # &quot;Simple Feature&quot; (vector) data structures library(sf) # superseded by sf -- spatial object classes e.g. SpatialPoints/SpatialPolygons library(sp) # superseded by sf and terra -- bindings for PROJ &amp; GDAL libraries library(rgdal) # gridded data management / analysis library(terra) # superseded by terra library(raster) # interactive maps with leaflet library(mapview) There are many packages available for working with spatial data, however we only have time to cover introducing a few common libraries. A couple resources are linked here for 5 packages that provide different ways displaying spatial data graphically: tmap ggplot2, ggmap mapview mapdeck leaflet 4.2 Making Maps with R R has become a powerful tool for visualization and interaction with spatial data. There are many tools available for making maps with R! It is not all geostatistics and coordinate reference system transformations. There are powerful ways to automate your GIS workflow from beginning to end–from creating terrain derivatives from a source DEM, to high-quality, publication-ready maps and interactive HTML/JavaScript widgets. All of the details of this could fill several books! And it does! Creating Maps with R (Stephen Roecker) Making Maps with R (Nico Hahn). geocompr: Chapter 8 - Making maps with R 4.3 Spatial Data Sources Spatial data sources: “raster” and “vector” Raster data sources (grids/images): GeoTIFF, ERDAS, BIL, ASCII grid, WMS, … Vector data sources (points/lines/polygons): Shape File, ESRI File Geodatabase, KML, GeoJSON, GML, WFS, … Conventional data sources that can be upgraded to spatial data: NASIS/LIMS reports: typically point coordinates Web pages: GeoJSON, WKT, or point coordinates Excel file: typically point coordinates CSV files: typically point coordinates Photo EXIF information: typically point coordinates Here are some R-based interfaces to NCSS data sources via soilDB package. Functions that return tabular data which can be upgraded to spatial data: fetchKSSL(): KSSL “site” data contain x,y coordinates fetchNASIS(): NASIS “site” data contain x,y, coordinates Functions that return spatial data: fetchSDA_spatial(): polygon, bounding box and centroid data from SSURGO, STATSGO and the sapolygon (Soil Survey Area Polygon) from Soil Data Access (SDA) seriesExtent(): simplified series extent as polygons fetchHenry(): sensor / weather station locations as points SDA_query(): SSURGO data as points, lines, polygons (via SDA) SDA_spatialQuery(): use points or polygons as a “query” to SDA seriesExtent() and taxaExtent(): extent of series and taxonomic classes derived from SSURGO (SoilWeb) in vector and raster format (800m resolution). The vector output is identical to series extents reported by Series Extent Explorer mukey.wcs() and ISSR800.wcs() provide an interface to gSSURGO (mukey), gNATSGO (mukey), and the ISSR-800 (gridded soil property) data. 4.4 Viewing Pedon Locations ( Introducing the sf package with mapview) 4.4.1 Plotting Geographic Data Plotting the data as an R graphic can give you some idea of how data look spatially and whether their distribution is what you expect. Typos are relatively common when coordinates are manually entered. Viewing the data spatially is a quick way to see if any points plot far outside of the geographic area of interest and therefore clearly have an error. # plot the locations of the gopheridge pedons with R # # Steps: # 1) create and inspect an sf data.frame object # 2) plot the data with mapview # load libraries library(aqp) library(soilDB) library(sf) library(mapview) # this creates sample gopheridge object in your environment data(&quot;gopheridge&quot;, package = &quot;soilDB&quot;) # replace gopheridge object with fetchNASIS() (your data) # gopheridge &lt;- fetchNASIS() # create simple features POINT geometry data.frame # st_as_sf(): convert data.frame to spatial simple features, with points in $geometry # st_crs(): set EPSG:4326 Coordinate Reference System (CRS) as Well-Known Text (WKT) gopher.locations &lt;- st_as_sf( site(gopheridge), coords = c(&#39;x_std&#39;,&#39;y_std&#39;), crs = st_crs(4326) ) # create interactive map with sfc_POINT object # use site_id in sf data.frame as labels mapview(gopher.locations, label = gopher.locations$site_id) 4.4.2 EXERCISE 1 (Spatial Intro) In this exercise, you will create an interactive map with the pedons in your selected set. Then you will export them to a shapefile. Modify the code snippets below to make an R plot and a shapefile of pedon data loaded from your NASIS selected set. You will plot pedon locations using the standard WGS84 longitude/latitude decimal degrees fields from Site table of NASIS. In some cases, these data might be incomplete; you need to handle this possibility. Missing values in coordinates for sf/Spatial objects are not allowed. In this exercise you will create a subset SoilProfileCollection for the pedons that are not missing spatial data (x_std and y_std). Make a new R script, load the aqp, soilDB, sf and mapview packages and some pedons via fetchNASIS() (or similar source). library(aqp) library(soilDB) library(sf) library(mapview) # get pedons from the selected set pedons &lt;- fetchNASIS(from = &#39;pedons&#39;) Use the base R subset() function to create a subset of your SoilProfileCollection using is.na() x_std and y_std variables contain WGS84 longitude and latitude in decimal degrees. This is the standard format for location information used in NASIS. # modify this code (replace ...) to create a subset pedons.sp &lt;- aqp::subset(pedons, ...) Create a sf data.frame from the site data in the SoilProfileCollection object pedons.sp using aqp::site(). Replace the ... in the following code. Promoting a data.frame to sf POINT geometry requires that the X and Y columns be specified. pedon.locations &lt;- sf::st_as_sf( ..., coords = c(&#39;x_std&#39;, &#39;y_std&#39;), crs = sf::st_crs(4326) #WGS84 GCS ) View your sf object pedon.locations interactively with mapview, and change the map.types argument to 'Esri.WorldImagery'. Use the pedon.locations column site_id as the mapview(label=...) argument. # plot an interactive map mapview(pedon.locations, legend = FALSE, map.types = &#39;OpenStreetMap&#39;) Create a subset sf data.frame with only the following “site data” columns: pedlabsampnum, pedon_id, taxonname, hillslopeprof, elev_field, slope_field, aspect_field, plantassocnm, bedrckdepth, bedrckkind, pmkind, pmorigin. Select the target columns with dplyr::select() (or another method) by replacing the ... in the following code. pedon.locations_sub &lt;- dplyr::select(pedon.locations, ...) # see also base::subset(x, select=...) Export the spatial information in pedon.locations_sub to a shape file (.shp) with sf::st_write() # write to SHP; output CRS is geographic coordinate system WGS84 sf::st_write(pedon.locations_sub, &quot;./NASIS-pedons.shp&quot;) For an example of exporting data to shapefile with the sp package, see this tutorial: Export Pedons to Shapefile with sp. 4.4.2.1 Export for Google Earth (.kml) Google Earth is a powerful viewer for point data. Geographic data is displayed in Google Earth using the Keyhole Markup Language (KML) format. Using the plotKML package, you can easily create a KML file to inspect and view in Google Earth. See the related material in this tutorial: Export Pedons to Google Earth. 4.5 Many Packages, Many Spatial Representations 4.5.1 The sf package Simple Features Access is a set of standards that specify a common storage and access model of geographic features. It is used mostly for two-dimensional geometries such as point, line, polygon, multi-point, multi-line, etc. This is one of many ways of modeling the geometry of shapes in the real world. This model happens to be widely adopted in the R ecosystem via the sf package, and very convenient for typical data encountered by soil survey operations. The sf package represents the latest and greatest in spatial data processing within the comfort of an R session. It provides a “main” object class sf to contain geometric data and associated tabular data in a familiar data.frame format. sf methods work on a variety of different levels of abstraction and manipulation of those geometries. Most of the sf package functions start with the prefix st_, such as: st_crs() (get/set coordinate reference system), st_transform() (project feature class to different coordinate reference system), st_bbox() (bounding box), st_buffer() (buffer). Many of these are “verbs” that are common GIS operations. 4.5.1.1 sf vignettes You can the following sf package vignettes for details, sample data sets and usage of sf objects. Simple Features for R Reading, Writing and Converting Simple Features Manipulating Simple Feature Geometries Manipulating Simple Features Plotting Simple Features Miscellaneous Spherical geometry in sf using s2geometry 4.5.2 The sp and rgdal Packages Please note that rgdal will be retired by the end of 2023, plan transition to sf/stars/terra functions using GDAL and PROJ at your earliest convenience. The data structures (“classes”) and functions provided by the sp package have served a foundational role in the handling of spatial data in R for years. Many of the following examples will reference names such as SpatialPoints, SpatialPointsDataFrame, and SpatialPolygonsDataFrame. These are specialized (S4) classes implemented by the sp package. Objects of these classes maintain linkages between all of the components of spatial data. For example, a point, line, or polygon feature will typically be associated with: coordinate geometry bounding box coordinate reference system attribute table 4.5.3 Converting sp and sf sp provides access to the same compiled code libraries (PROJ, GDAL, GEOS) as sf, but mostly via the interfaces in the separate rgdal package. For certain applications, such as some packages we demonstrate below, there are no sp “interfaces” to the methods – only sf, or vice-versa. In the future, all R spatial code using sp will have to migrate to sf/stars or terra functions. For now the different package object types are interchangeable, and you may find yourself having to do this for a variety of reasons. You can convert between object types as neede using sf::as_Spatial() or sf::st_as_sf. Check the documentation (?functionname) to figure out what object types different methods need as input; and check an input object’s class with class() or inherits(). 4.5.4 Importing / Exporting Vector Data Import a feature class from a ESRI File Geodatabase or shape file. If you have a .shp file, you can specify the whole path, including the file extension in the dsn argument, or just the folder. For a Geodatabase, you should specify the feature class using the layer argument. Note that a trailing “/” is omitted from the dsn (data source name) and the “.shp” suffix is omitted from the layer. 4.5.4.1 sf x &lt;- sf::st_read(dsn = &#39;E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb&#39;, layer = &#39;ca630_a&#39;) x &lt;- sf::read_sf(dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;) sf::st_write(x, dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;) sf::write_sf(x, dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;) 4.5.4.2 sp / rgdal Export object x to shapefile. x &lt;- rgdal::readOGR(dsn = &#39;E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb&#39;, layer = &#39;ca630_a&#39;) rgdal::writeOGR(x, dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;, driver = &#39;ESRI Shapefile&#39;) The st_read() / read_sf() / st_write() / write_sf() and readOGR(), writeOGR(), readGDAL(), writeGDAL() (used for SpatialGridDataFrame) functions have many arguments, so it is worth spending some time reviewing the associated manual pages. 4.5.5 Interactive mapping with mapview and leaflet The mapview and leaflet packages make it possible to display interactive maps of sf objects in RStudio viewer pane, or within an HTML document generated via R Markdown (e.g. this document). mapview package Basics Advanced Features See other “Articles” in this series, you can make complex, interactive maps using the mapview package. leaflet package leafem: ‘leaflet’ Extensions for ‘mapview’ The seriesExtent function in soilDB returns an sp object (SpatialPolygonsDataFrame) showing generalized extent polygons for a given soil series. # load required packages, just in case library(soilDB) library(sf) library(mapview) # series extents from SoilWeb (sp objects) pentz &lt;- seriesExtent(&#39;pentz&#39;) amador &lt;- seriesExtent(&#39;amador&#39;) # convert to sf objects (way of the future) pentz &lt;- st_as_sf(pentz) amador &lt;- st_as_sf(amador) # combine into a single object s &lt;- rbind(pentz, amador) # colors used in the map # add more colors as needed cols &lt;- c(&#39;royalblue&#39;, &#39;firebrick&#39;) # make a simple map, colors set by &#39;series&#39; column mapview(s, zcol = &#39;series&#39;, col.regions = cols, legend = TRUE) 4.5.5.1 EXERCISE 2: Map your favorite soil series extents The following code demonstrates how to fetch / convert / map soil series extents, using a vector of soil series names. Results appear in the RStudio “Viewer” pane. Be sure to try the “Export” and “show in window” (next to the broom icon) buttons. # load required packages, just in case library(soilDB) library(sf) library(mapview) # vector of series names, letter case does not matter # try several (2-9)! series.names &lt;- c(&#39;auberry&#39;, &#39;sierra&#39;, &#39;holland&#39;, &#39;cagwin&#39;) # iterate over series names, get extent # result is a list s &lt;- lapply(series.names, soilDB::seriesExtent) # iterate over series extents (sp objects) # convert to sf objects # result is a list s &lt;- lapply(s, st_as_sf) # flatten list -&gt; single sf object s &lt;- do.call(&#39;rbind&#39;, s) # colors used in the map # note trick used to dynamically set the number of colors cols &lt;- RColorBrewer::brewer.pal(n = length(series.names), name = &#39;Set1&#39;) # make a simple map, colors set by &#39;series&#39; column # click on polygons for details # try pop-out / export buttons mapview(s, zcol = &#39;series&#39;, col.regions = cols, legend = TRUE) 4.5.6 The terra Package The terra package package provides most of the commonly used grid and vector processing functionality that one might find in a conventional GIS. It provides high-level data structures and functions for the GDAL (Geospatial Data Abstraction Library). re-sampling / interpolation projection and warping (coordinate system transformations of gridded data) cropping, mosaicing, masking local and focal functions raster algebra contouring raster/vector conversions terrain analysis model-based prediction (more on this in Part 2) 4.5.6.1 Other approaches to raster data 4.5.6.1.1 `raster`` A more complete background on the capabilities of the raster package, and the replacement terra, are described in the Spatial Data Science with R online book. Introduction to the raster package vignette 4.5.6.1.2 stars There is also a package called stars (Spatiotemporal Arrays: Raster and Vector Datacubes) that is the sf-centric way of dealing with higher dimensional raster and vector “datacubes.” Data cubes have dimensions related to time, spectral band, and sensor. The stars data structures are often used for processing satellite data sources. 4.5.6.2 Importing / Exporting Rasters # use an example from the raster package f &lt;- system.file(&quot;external/test.grd&quot;, package = &quot;raster&quot;) # create a reference to this raster r &lt;- raster(f) # equivalent terra code r2 &lt;- terra::rast(f) # show RasterLayer details print(r) ## class : RasterLayer ## dimensions : 115, 80, 9200 (nrow, ncol, ncell) ## resolution : 40, 40 (x, y) ## extent : 178400, 181600, 329400, 334000 (xmin, xmax, ymin, ymax) ## crs : +proj=sterea +lat_0=52.1561605555556 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +datum=WGS84 +units=m +no_defs ## source : test.grd ## names : test ## values : 138.7071, 1736.058 (min, max) # show SpatRaster details print(r2) ## class : SpatRaster ## dimensions : 115, 80, 1 (nrow, ncol, nlyr) ## resolution : 40, 40 (x, y) ## extent : 178400, 181600, 329400, 334000 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=sterea +lat_0=52.1561605555556 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +datum=WGS84 +units=m +no_defs ## source : test.grd ## name : test ## min value : 138.7071 ## max value : 1736.0579 # default plot method plot(r) The R object only stores a reference to the data until they are needed to be loaded into memory. This allows for internal raster manipulation algorithms to intelligently deal with very large grids that may not fit in memory. 4.5.7 Converting Vector to Raster 4.5.7.1 terra::rasterize() 4.5.7.2 raster::rasterize() 4.5.7.3 fasterize::fasterize() 4.6 Coordinate Reference Systems (CRS) Spatial data aren’t all that useful without an accurate description of the Coordinate Reference System (CRS). This type of information is typically stored within the “.prj” component of a shapefile, or in the header of a GeoTIFF. Without a CRS it is not possible to perform coordinate transformations (e.g. conversion of geographic coordinates to projected coordinates), spatial overlay (e.g. intersection), or geometric calculations (e.g. distance or area). The “old” way (PROJ.4) of specifying coordinate reference systems is using character strings containing, for example: +proj or +init arguments. In general, this still “works,” so you may encounter it and need to know about it. But you also may encounter cases where CRS are specified using integers, strings of the form authority:code, or well-known text (WKT). Some common examples of coordinate system “EPSG” codes and their legacy “PROJ.4” strings. 4 “EPSG” stands for European Petroleum Survey Group. The “EPSG Geodetic Parameter Dataset” is a public registry of geodetic datums, spatial reference systems, Earth ellipsoids, coordinate transformations and related units of measurement. “OGC” refers to the Open Geospatial Consortium, which is an example of another important authority:code. “ESRI” (company that develops ArcGIS) also defines many CRS codes. “PROJ” is the software responsible for transforming coordinates from one CRS to another. The current version of PROJ available is 9, and in PROJ &gt; 6 major changes were made to the way that coordinate reference systems are defined and transformed led to the “PROJ.4” syntax falling out of favor. EPSG: 4326 / PROJ.4:+proj=longlat +datum=WGS84 - geographic, WGS84 datum (NASIS Standard) OGC:CRS84 - geographic, WGS84 datum (same as above but explicit longitude, latitude XY order) EPSG: 4269 / PROJ.4:+proj=longlat +datum=NAD83 - geographic, NAD83 datum EPSG: 4267 / PROJ.4:+proj=longlat +datum=NAD27 - geographic, NAD27 datum EPSG: 26910 / PROJ.4:+proj=utm +zone=10 +datum=NAD83 - projected (UTM zone 10), NAD83 datum EPSG: 5070 / PROJ.4: +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23.0 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs - Albers Equal Area CONUS (gSSURGO) More on the EPSG codes and specifics of CRS definitions: https://spatialreference.org/ref/epsg/ https://epsg.io/ While you may encounter PROJ.4 strings, these are no longer considered the preferred method of referencing Coordinate Reference Systems – and, in general, newer methods are “easier.” Well-known text (WKT) is a human- machine-readable standard format for geometry, so storing the Coordinate Reference System information in a similar format makes sense. This format is returned by the sf::st_crs method. For example: the WKT representation of EPSG:4326: st_crs(4326) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] This is using the OGC WKT CRS standard. Adoption of this standard caused some significant changes in packages in the R ecosystem. So you can get familiar, what follows are several examples of doing the same thing: setting the CRS of spatial objects with WGS84 longitude/latitude geographic coordinates. If you have another target coordinate system, it is just a matter of using the correct codes to identify it. 4.6.1 Assigning and Transforming Coordinate Systems Returning to the example from above, lets assign a CRS to our series extent s using different methods. s &lt;- seriesExtent(&#39;san joaquin&#39;) The following are equivalent sf versus sp vs. rgdal syntax. 4.6.1.1 sf Use st_crs&lt;- to set, or st_crs get CRS of sf objects. Supply the target EPSG code as an integer. # s is an sp object, we convert it to sf with st_as_sf s &lt;- st_as_sf(s) # the CRS of s is EPSG:4326 st_crs(s) == st_crs(4326) ## [1] TRUE # set CRS using st_crs&lt;- (replace with identical value) st_crs(s) &lt;- st_crs(4326) Transformation of points, lines, and polygons with sf requires an “origin” CRS be defined in the argument x. The “target” CRS is defined as an integer (EPSG code) in the crs argument or is the output of st_crs(). # transform to UTM zone 10 s.utm &lt;- st_transform(x = s, crs = 26910) # transform to GCS NAD27 s.nad27 &lt;- st_transform(x = s, crs = st_crs(4267)) 4.6.1.2 sp and rgdal You can do the same thing several different ways with sp objects. An equivalent EPSG, OGC and PROJ.4 can be set or get using proj4string&lt;-/proj4string and either a sp CRS object or a PROJ.4 string for Spatial objects. # s is an sf object (we converted it), convert back to Spatial* object s.sp &lt;- sf::as_Spatial(s) # these all create the same internal sp::CRS object proj4string(s.sp) &lt;- sp::CRS(&#39;EPSG:4326&#39;) # proj &gt;6; EPSG proj4string(s.sp) &lt;- sp::CRS(&#39;OGC:CRS84&#39;) # proj &gt;6; OGC proj4string(s.sp) &lt;- &#39;+init=epsg:4326&#39; # proj4 style +init string proj4string(s.sp) &lt;- &#39;+proj=longlat +datum=WGS84&#39; # proj4 style +proj string Here, we do the same transformations we did above only using sp: spTransform(). # transform to UTM zone 10 s.utm &lt;- spTransform(s.sp, CRS(&#39;+proj=utm +zone=10 +datum=NAD83&#39;)) # transform to GCS NAD27 s.nad27 &lt;- spTransform(s.sp, CRS(&#39;+proj=longlat +datum=NAD27&#39;)) 4.6.1.3 terra and raster Use crs&lt;- and crs for raster, terra or sp CRS objects. # r is a raster object; set CRS as the CRS of itself crs(r) &lt;- raster::crs(sp::CRS(r)) “Transforming” or “warping” a raster is a different from with a vector as it requires interpolation of pixels to a target resolution and CRS. The method provided by terra is project() and in raster it is projectRaster(). It works the same as the above “transform” methods in that you specify an object to transform, and the target reference system or a template for the object. t.wgs84 &lt;- terra::project(terra::rast(r), terra::crs(&quot;EPSG:4326&quot;)) r.wgs84 &lt;- projectRaster(r, CRS(&quot;EPSG:4326&quot;)) Note that the default warping of raster uses bilinear interpolation (method='bilinear'), which is appropriate for continuous variables. You also have the option of using nearest-neighbor (method='ngb') for categorical variables (class maps) where interpolation would not make sense. If we want to save this transformed raster to file, we can use something like this: writeRaster(r.wgs84, filename=&#39;r_wgs84.tif&#39;, options=c(&quot;COMPRESS=LZW&quot;)) 4.6.1.4 Related Links sf package website rspatial.org - Spatial Data Science with R Goodbye PROJ.4 strings! How to specify a coordinate reference system in R? 4.7 Raster data 4.7.1 In-memory versus disk-based processing Processing of raster data in memory is always faster than processing on disk, as long as there is sufficient memory. With the raster package, the initial file/disk-based reference can be converted to an in-memory RasterLayer with the readAll() function. You can achieve a similar effect in terra by doing set.values(object). # check: file is on disk inMemory(r) # load into memory, if possible r &lt;- readAll(r) # check: file is in memory inMemory(r) Exporting data requires consideration of the output format, datatype, encoding of NODATA, and other options such as compression. See the manual pages for writeRaster(), writeFormats(), and dataType() for details. For example, suppose you had a RasterLayer object that you wanted to save to disk as an internally-compressed GeoTIFF: # using previous example data set raster::writeRaster(r, filename = &#39;r.tif&#39;, options = c(&quot;COMPRESS=LZW&quot;)) With terra, “LZW” compression is used by default when writing GeoTiff files. Using the gdal argument e.g.: terra::writeRaster(..., gdal=) is equivalent to specifying option argument to raster::writeRaster(). # using previous example data set terra::writeRaster(t.wgs84, filename = &#39;t.wgs84.tif&#39;) The writeRaster() function interprets the given (and missing) arguments as: ‘.tif’ suffix interpreted as format=GTiff creation options of “LZW compression” passed to GeoTiff driver default datatype default NAflag 4.7.2 Object Properties RasterLayer and SpatRaster objects are similar to sf and sp objects in that they keep track of the linkages between data, coordinate reference system, and optional attribute tables. Getting and setting the contents of raster objects should be performed using functions such as: raster::NAvalue(r) / terra::NAflag(r): get / set the NODATA value raster::wkt(r) OR terra::crs(r): get / set the coordinate reference system res(r): get / set the resolution raster::extent(r) or terra::ext(r): get / set the extent raster::dataType(r) / terra::writeRaster(..., datatype=): get / set the data type … many more, see the raster and terra package manuals 4.7.3 Data Types Commonly used raster datatype include: “unsigned integer”, “signed integer”, and “floating point” of variable precision. INT1U: integers from 0 to 255 INT2U: integers from 0 to 65,534 INT2S: integers from -32,767 to 32,767 INT4S: integers from -2,147,483,647 to 2,147,483,647 FLT4S: floating point from -3.4e+38 to 3.4e+38 FLT8S: floating point from -1.7e+308 to 1.7e+308 It is wise to manually specify an output datatype that will “just fit” the required precision. For example, if you have generated a RasterLayer that warrants integer precision and ranges from 0 to 100, then the INT1U data type would provide enough precision to store all possible values and the NODATA value. Raster data stored as integers will always be smaller (sometimes 10-100x) than those stored as floating point, especially when internal compression is enabled. # integer grid with a range of 0-100 # maybe soil texture classes raster::writeRaster(r, filename = &#39;r.tif&#39;, datatype = &#39;INT1U&#39;) # floating point grid with very wide range # maybe DSM model output terra::writeRaster(t.wgs84, filename = &#39;r.tif&#39;, datatype = &#39;FLT4S&#39;) 4.7.3.1 Notes on Compression It is often a good idea to create internally-compressed raster data. The GeoTiff format can accommodate many different compression algorithms, including lossy (JPEG) compression. Usually, the default “LZW” or “DEFLATE” compression will result in significant savings, especially for data encoded as integers. For example, the CONUS gSSURGO map unit key grid at 30m resolution is about 55Gb (GeoTiff, no compression) vs. 2.4Gb after LZW compression. # reasonable compression using LZW is the default, compare to raster::writeRaster(r, filename=&#39;r.tif&#39;, options=c(&quot;COMPRESS=NONE&quot;)) # takes longer to write the file, but better compression terra::writeRaster(terra::writeRaster(t.wgs84, filename=&#39;r.tif&#39;, gdal=c(&quot;COMPRESS=DEFLATE&quot;, &quot;PREDICTOR=2&quot;, &quot;ZLEVEL=9&quot;)) See this article for some ideas on optimization of file read/write times and associated compressed file sizes. 4.8 Spatial Overlay Operations Spatial data are lot more useful when “combined” (overlay, intersect, spatial query, etc.) to generate something new. For simplicity, we will refer to this kind of operation as an “extraction”. The CRS of the two objects being overlaid must match. 4.8.1 Vector Data p &lt;- sf::st_as_sf(data.frame(x = -120, y = 37.5), coords = c(&quot;x&quot;, &quot;y&quot;), crs = 4326) In sf the functions used to do this are st_intersects() or st_intersection(). st_intersection(p, &lt;Simple Features&gt;) In sp objects, you do these operations with the sp::over() function. Access the associated vignette by pasting vignette(\"over\") in the console when the sp package is loaded. # hand make a SpatialPoints object # note that this is GCS p &lt;- SpatialPoints(coords = cbind(-120, 37.5), proj4string = CRS(&#39;+proj=longlat +datum=WGS84&#39;)) # spatial extraction of MLRA data requires a CRS transformation p.aea &lt;- spTransform(p, proj4string(mlra)) over(p.aea, mlra) 4.8.2 Spatial Data Extraction 4.8.2.1 Load Required Packages Load required packages into a fresh RStudio Session (Ctrl + Shift + F10) library(aqp) library(soilDB) library(sf) library(terra) 4.8.2.2 Download Example Data Run the following to create a path for the example data. Be sure to set a valid path to a local disk. # store path as a variable, in case you want to keep it somewhere else ch4.data.path &lt;- &#39;C:/workspace2/chapter-4&#39; # make a place to store chapter 2b example data dir.create(ch4.data.path, recursive = TRUE) # download polygon example data from github download.file( &#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_4-spatial-data/chapter-4-mu-polygons.zip&#39;, file.path(ch4.data.path, &#39;chapter-4-mu-polygons.zip&#39;) ) # download raster example data from github download.file( &#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_4-spatial-data/chapter-4-PRISM.zip&#39;, file.path(ch4.data.path, &#39;chapter-4-PRISM.zip&#39;) ) # unzip unzip( file.path(ch4.data.path, &#39;chapter-4-mu-polygons.zip&#39;), exdir = ch4.data.path, overwrite = TRUE ) unzip( file.path(ch4.data.path, &#39;chapter-4-PRISM.zip&#39;), exdir = ch4.data.path, overwrite = TRUE ) 4.8.2.3 Load Sample MLRA Data We will be using polygons associated with MLRA 15 and 18 as part of this demonstration. Import these data with sf::st_read(). # load MLRA polygons mlra &lt;- sf::st_read(file.path(ch4.data.path, &#39;mlra-18-15-AEA.shp&#39;)) We will load the sample raster data (PRISM derived) using raster::raster() # mean annual air temperature, Deg C maat &lt;- raster::raster(file.path(ch4.data.path, &#39;MAAT.tif&#39;)) # mean annual precipitation, mm map &lt;- raster::raster(file.path(ch4.data.path, &#39;MAP.tif&#39;)) # frost-free days ffd &lt;- raster::raster(file.path(ch4.data.path, &#39;FFD.tif&#39;)) # growing degree days gdd &lt;- raster::raster(file.path(ch4.data.path, &#39;GDD.tif&#39;)) # percent of annual PPT as rain rain_fraction &lt;- raster::raster(file.path(ch4.data.path, &#39;rain_fraction.tif&#39;)) # annual sum of monthly PPT - ET_p ppt_eff &lt;- raster::raster(file.path(ch4.data.path, &#39;effective_precipitation.tif&#39;)) Sometimes it is convenient to “stack” raster data that share a common grid size, extent, and coordinate reference system into a multilayer terra SpatRaster object. Calling terra::rast() on a list of SpatRaster is equivalent to making a RasterStack from several RasterLayers with raster::stack(). # create a raster stack (multiple rasters aligned) rs &lt;- raster::stack(maat, map, ffd, gdd, rain_fraction, ppt_eff) rs ## class : RasterStack ## dimensions : 762, 616, 469392, 6 (nrow, ncol, ncell, nlayers) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -123.2708, -118.1375, 34.44583, 40.79583 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=NAD83 +no_defs ## names : MAAT, MAP, FFD, GDD, rain_fraction, effective_precipitation ## min values : -4.073542, 114.000000, 35.000000, 76.000000, 12.000000, -825.589661 ## max values : 18.67642, 2958.00000, 365.00000, 3173.00000, 100.00000, 2782.39136 plot(rs) 4.8.3 Approach Typically, spatial queries of raster data by polygon features are performed in two ways: for each polygon, collect all pixels that overlap (exactextractr approach) for each polygon, collect a sample of pixels defined by sampling points The first method ensures that all data are included in the analysis, however, processing can be slow for multiple/detailed rasters, and the results may not fit into memory. The second method is more efficient (10-100x faster), requires less memory, and can remain statistically sound–as long as a reasonable sampling strategy is applied. Sampling may also help you avoid low-acreage “anomalies” in the raster product. More on sampling methods in the next chapter. The extract() function can perform several operations in one call, such as buffering (in projected units) with buffer argument. See the manual page for an extensive listing of optional arguments and what they do. Sampling and extraction with raster methods results in a matrix object. Sampling and extraction with terra the results in a SpatVector object. # sampling single RasterLayer -&gt; SpatRaster terra::spatSample(terra::rast(maat), size = 10) ## MAAT ## 1 NA ## 2 16.505342 ## 3 15.317194 ## 4 NA ## 5 15.689567 ## 6 14.430653 ## 7 17.491835 ## 8 5.144731 ## 9 16.246895 ## 10 15.949578 # sampling RasterStack -&gt; SpatRaster terra::spatSample(terra::rast(rs), size = 10) ## MAAT MAP FFD GDD rain_fraction effective_precipitation ## 1 NA NA NA NA NA NA ## 2 14.99419 1199 349 2140 99 410.8856 ## 3 12.18016 2254 194 1841 88 1523.3220 ## 4 15.80716 300 259 2615 100 -485.2193 ## 5 12.84000 980 229 1900 88 290.4952 ## 6 11.71534 121 165 2006 95 -582.3167 ## 7 NA NA NA NA NA NA ## 8 14.77218 511 359 2039 99 -204.8428 ## 9 NA NA NA NA NA NA ## 10 16.59895 292 310 2679 99 -622.5543 par(mfcol = c(1, 2), mar = c(1, 1, 3, 1)) # regular sampling + extraction of raster values x.regular &lt;- terra::spatSample( terra::rast(maat), method = &quot;regular&quot;, size = 100, as.points = TRUE ) x.regular ## class : SpatVector ## geometry : points ## dimensions : 96, 1 (geometries, attributes) ## extent : -123.2667, -118.1417, 34.64167, 40.6 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (with axis order normalized for visualization) ## names : MAAT ## type : &lt;num&gt; ## values : NA ## 16.41 ## 11 # see also raster::sampleRegular() plot(maat, axes = FALSE, legend = FALSE, main = &#39;Regular Sampling&#39;) points(x.regular) # random sample + extraction of raster values # note that NULL values are removed x.random &lt;- terra::spatSample( terra::rast(maat), size = 100, as.points = TRUE, na.rm = TRUE ) # see also raster::sampleRandom() plot(maat, axes = FALSE, legend = FALSE, main = &#39;Random Sampling with NA Removal&#39;) points(x.random) Note that the mean can be efficiently estimated, even with a relatively small number of samples. # all values: slow for large grids mean(raster::values(maat), na.rm = TRUE) # regular sampling: efficient, central tendency comparable to above mean(x.regular$MAAT, na.rm = TRUE) # this value will be pseudorandom # depends on number of samples, pattern of NA mean(x.random$MAAT, na.rm = TRUE) Just how much variation can we expect when collecting 100, randomly-located samples over such a large area? # 10 replications of samples of n=100 z &lt;- replicate(10, { mean(terra::spatSample(terra::rast(maat), size = 100, na.rm = TRUE)$MAAT, na.rm = TRUE) }) # 90% of the time the mean MAAT values were within: quantile(z, probs = c(0.05, 0.95)) Do the above routine 100 times: compute the mean MAAT from 100 randomly-located samples. Does it make a difference in your estimates? # MLRA polygons in native coordinate system plot(sf::st_geometry(mlra), main = &#39;MLRA 15 and 18&#39;) box() # MAAT raster plot(maat, main = &#39;PRISM Mean Annual Air Temperature (deg C)&#39;) # plot MAAT raster with MLRA polygons on top # this requires transforming to CRS of MAAT mlra.gcs &lt;- sf::st_transform(mlra, sf::st_crs(maat)) plot(maat, main = &#39;PRISM Mean Annual Air Temperature (deg C)&#39;) plot(sf::st_geometry(mlra.gcs), main = &#39;MLRA 15 and 18&#39;, add = TRUE) 4.8.3.1 Extracting Raster Data: KSSL Pedon Locations Extract PRISM data at the coordinates associated with KSSL pedons that have been correlated to the AUBURN series. We will use the fetchKSSL() function from the soilDB package to get KSSL data from the most recent snapshot. This example can be easily adapted to pedon data extracted from NASIS using fetchNASIS(). Get some KSSL data and upgrade the “site” data to a SpatialPointsDataFrame. # result is a SoilProfileCollection object auburn &lt;- soilDB::fetchKSSL(series = &#39;auburn&#39;) # extract site data s &lt;- sf::st_as_sf(aqp::site(auburn), coords = c(&quot;x&quot;, &quot;y&quot;), crs = 4326) Extract PRISM data (the RasterStack object we made earlier) at the Auburn KSSL locations and summarize. # convert sf object s to terra SpatVector, and pass to terra::extract() e &lt;- terra::extract(terra::rast(rs), terra::vect(s), df = TRUE) # summarize: remove first (ID) column using [, -1] j index summary(e[, -1]) ## MAAT MAP FFD GDD rain_fraction effective_precipitation ## Min. :15.52 Min. :448.0 Min. :278.0 Min. :2456 Min. :99 Min. :-409.0 ## 1st Qu.:16.24 1st Qu.:519.0 1st Qu.:305.5 1st Qu.:2586 1st Qu.:99 1st Qu.:-329.1 ## Median :16.45 Median :569.5 Median :316.0 Median :2608 Median :99 Median :-282.3 ## Mean :16.33 Mean :633.3 Mean :314.4 Mean :2588 Mean :99 Mean :-208.6 ## 3rd Qu.:16.60 3rd Qu.:661.0 3rd Qu.:329.5 3rd Qu.:2623 3rd Qu.:99 3rd Qu.:-188.4 ## Max. :16.65 Max. :947.0 Max. :334.0 Max. :2651 Max. :99 Max. : 128.4 Join the extracted PRISM data with the original SoilProfileCollection object. More information on SoilProfileCollection objects here. # combine site data (sf) with extracted raster values (data.frame), row-order is identical, result is sf res &lt;- cbind(s, e) # extract unique IDs and PRISM data; dplyr verbs work with sf data.frames res2 &lt;- dplyr::select(res, pedon_key, MAAT, MAP, FFD, GDD, rain_fraction, effective_precipitation) # join with original SoilProfileCollection object via pedon_key site(auburn) &lt;- res2 The extracted values are now part of the “auburn” SoilProfileCollection object via site(&lt;SoilProfileCollection&gt;) &lt;- data.frame LEFT JOIN method. Does there appear to be a relationship between soil morphology and “effective precipitation”? # create an ordering of pedons based on the extracted effective PPT new.order &lt;- order(auburn$effective_precipitation) # setup figure margins, 1x1 row*column layout par(mar = c(4.5, 0, 4, 0), mfcol = c(1, 1)) # plot profile sketches plotSPC(auburn, name = &#39;hzn_desgn&#39;, print.id = FALSE, color = &#39;clay&#39;, plot.order = new.order, cex.names = 0.75, max.depth = 70, width = 0.3, name.style = &#39;center-top&#39;, scaling.factor = 1, plot.depth.axis = FALSE, hz.depths = TRUE ) # add an axis with extracted raster values axis(side = 1, at = 1:length(auburn), labels = round(auburn$effective_precipitation[new.order]), cex.axis = 0.75) mtext(&#39;Annual Sum of Monthly (PPT - ET_p) (mm)&#39;, side = 1, line = 2.5) Note that negative values are associated with a net deficit in monthly precipitation vs. estimated ET. 4.8.3.2 Raster Summary By Polygon: Series Extent The seriesExtent() function from the soilDB package provides a simple interface to Series Extent Explorer data files. Note that these series extents have been generalized for rapid display at regional to continental scales. A more precise representation of “series extent” can be generated from SSURGO polygons and queried from SDA. Get an approximate extent for the Amador soil series from SEE. See the seriesExtent tutorial and manual page for additional options and related functions. # get (generalized) amador soil series extent from SoilWeb amador &lt;- soilDB::seriesExtent(s = &#39;amador&#39;) # convert to EPSG:5070 Albers Equal Area amador &lt;- sf::st_transform(sf::st_as_sf(amador), 5070) Generate 100 sampling points within the extent using a hexagonal grid. These point locations will be used to extract raster values from our SpatRaster of PRISM data. Note that using a “hexagonal” grid is not supported on geographic coordinates. samples &lt;- sf::st_sample(amador, size = 100, type = &#39;hexagonal&#39;) For comparison, extract a single point from each SSURGO map unit delineation that contains Amador as a major component. This will require a query to SDA for the set of matching map unit keys (mukey), followed by a second request to SDA for the geometry. The SDA_query function is used to send arbitrary queries written in SQL to SDA, the results may be a data.frame or list, depending on the complexity of the query. The fetchSDA_spatial function returns map unit geometry as either polygons, polygon envelopes, or a single point within each polygon as selected by mukey or nationalmusym. # result is a data.frame mukeys &lt;- soilDB::SDA_query(&quot;SELECT DISTINCT mukey FROM component WHERE compname = &#39;Amador&#39; AND majcompflag = &#39;Yes&#39;;&quot;) # result is a SpatialPointsDataFrame amador.pts &lt;- soilDB::fetchSDA_spatial( mukeys$mukey, by.col = &#39;mukey&#39;, method = &#39;point&#39;, chunk.size = 2 ) amador.pts &lt;- sf::st_as_sf(amador.pts) Graphically check both methods: # prepare samples and mapunit points for viewing on PRISM data hexagonal &lt;- sf::st_transform(samples, sf::st_crs(maat)) amador_gcs &lt;- sf::st_transform(amador, sf::st_crs(maat)) maatcrop &lt;- terra::crop(maat, amador_gcs) # adjust margins and setup plot device for two columns par(mar = c(1, 1, 3, 1), mfcol = c(1, 2)) # first figure plot(maatcrop, main = &#39;PRISM MAAT\\n100 Sampling Points from Extent&#39;, axes = FALSE) plot(sf::st_geometry(amador_gcs), add = TRUE) plot(hexagonal, cex = 0.25, add = T) plot(maatcrop, main = &#39;PRISM MAAT\\nPolygon Centroids&#39;, axes = FALSE) plot(sf::st_geometry(amador.pts), cex = 0.25, add = TRUE) Extract PRISM data (the RasterStack object we made earlier) at the sampling locations (100 regularly-spaced and from MU polygon centroids) and summarize. Note that CRS transformations are automatic (when possible), with a warning. # return the result as a data.frame object e &lt;- raster::extract(rs, sf::as_Spatial(hexagonal), df = TRUE) e.pts &lt;- raster::extract(rs, sf::as_Spatial(amador.pts), df = TRUE) # check out the extracted data summary(e[,-1]) ## MAAT MAP FFD GDD rain_fraction effective_precipitation ## Min. :16.42 Min. :334.0 Min. :313.0 Min. :2583 Min. : 99.00 Min. :-547.7 ## 1st Qu.:16.60 1st Qu.:440.2 1st Qu.:323.0 1st Qu.:2628 1st Qu.: 99.00 1st Qu.:-420.6 ## Median :16.65 Median :486.5 Median :330.0 Median :2644 Median : 99.00 Median :-359.2 ## Mean :16.65 Mean :479.8 Mean :328.4 Mean :2643 Mean : 99.03 Mean :-369.3 ## 3rd Qu.:16.69 3rd Qu.:526.2 3rd Qu.:334.0 3rd Qu.:2663 3rd Qu.: 99.00 3rd Qu.:-303.0 ## Max. :16.83 Max. :661.0 Max. :340.0 Max. :2711 Max. :100.00 Max. :-170.7 # all pair-wise correlations, note NAs -&gt; why? knitr::kable(cor(e[,-1]), digits = 2) MAAT MAP FFD GDD rain_fraction effective_precipitation MAAT 1.00 -0.47 0.12 0.71 0.02 -0.45 MAP -0.47 1.00 0.56 -0.87 -0.08 0.99 FFD 0.12 0.56 1.00 -0.50 0.03 0.60 GDD 0.71 -0.87 -0.50 1.00 0.02 -0.86 rain_fraction 0.02 -0.08 0.03 0.02 1.00 -0.08 effective_precipitation -0.45 0.99 0.60 -0.86 -0.08 1.00 # only 1 unique value, correlation is impossible to compute unique(e$rain_fraction) ## [1] 99 100 Quickly compare the two sets of samples. # compile results into a list maat.comparison &lt;- list(&#39;regular samples&#39; = e$MAAT, &#39;polygon centroids&#39; = e.pts$MAAT) # number of samples per method lapply(maat.comparison, length) ## $`regular samples` ## [1] 98 ## ## $`polygon centroids` ## [1] 395 # summary() applied by group lapply(maat.comparison, summary) ## $`regular samples` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.42 16.60 16.65 16.65 16.69 16.83 ## ## $`polygon centroids` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.18 16.61 16.65 16.66 16.72 17.10 # box-whisker plot par(mar = c(4.5, 8, 3, 1), mfcol = c(1, 1)) boxplot( maat.comparison, horizontal = TRUE, las = 1, xlab = &#39;MAAT (deg C)&#39;, varwidth = TRUE, boxwex = 0.5, main = &#39;MAAT Comparison&#39; ) Basic climate summaries from a standardized source (e.g. PRISM) might be a useful addition to an OSD. Think about how you could adapt this example to compare climate summaries derived from NASIS pedons to summaries derived from map unit polygons and generalized soil series extents. 4.8.3.3 Raster Summary By Polygon: MLRA The following example is a simplified version of what is available in the soilReports package, reports on the ncss-tech GitHub repository, or in the TEUI suite of map unit summary tools. Example output from the soilReports package: summary of select CA630 map units summary of select MLRA polygons Efficient summary of large raster data sources can be accomplished using: internally-compressed raster data sources, stored on a local disk, can be in any coordinate system polygons stored in an equal-area or UTM coordinate system, with CRS units of meters fixed-density sampling of polygons estimation of quantiles from collected raster samples Back to our example data. The first step is to check the MLRA polygons (mlra); how many features per MLRA symbol? Note that some MLRA have more than one polygon. table(mlra$MLRARSYM) Convert polygon area from square meters to acres and summarize. Note that this will only make sense when using a projected CRS with units of meters (equal area)! poly.area &lt;- terra::expanse(terra::vect(mlra)) / 4046.86 sf::sf_use_s2(TRUE) poly.area.s2 &lt;- units::set_units(x = sf::st_area(mlra), value = &quot;acre&quot;) sf::sf_use_s2(FALSE) poly.area.sf &lt;- units::set_units(x = sf::st_area(mlra), value = &quot;acre&quot;) summary(poly.area) sum(poly.area) sum(poly.area.s2) sum(poly.area.sf) Sample each polygon at a constant sampling density of 0.001 samples per acre (1 sample for every 1,000 ac.). At this sampling density we should expect approximately 16,700 samples–more than enough for our simple example. library(sharpshootR) # the next function requires a polygon ID: # each polygon gets a unique number 1--number of polygons mlra$pID &lt;- 1:nrow(mlra) cds &lt;- constantDensitySampling(mlra, n.pts.per.ac = 0.001) Extract MLRA symbol at sample points using the over() function. The result will be a data.frame object with all attributes from our MLRA polygons that intersect sampling points s. # spatial overlay: sampling points and MLRA polygons res &lt;- sf::st_intersection(sf::st_transform(sf::st_as_sf(cds), sf::st_crs(mlra)), mlra) # row / feature order is preserved, so we can directly copy cds$mlra &lt;- res$MLRARSYM # tabulate number of samples per MLRA table(cds$mlra) ## ## 15 18 ## 11620 5137 Extract values from the SpatVector of PRISM data as a data.frame. trs &lt;- terra::rast(rs) e &lt;- terra::extract(trs, terra::project(cds, terra::crs(trs))) # join columns from extracted values and sampling points s.df &lt;- cbind(as(cds, &#39;data.frame&#39;), e) # check results head(s.df) ## MLRARSYM MLRA_ID MLRA_NAME LRRSYM LRR_NAME ## 1 15 20 Central California Coast Range C California Subtropical Fruit, Truck, and Specialty Crop Region ## 2 15 20 Central California Coast Range C California Subtropical Fruit, Truck, and Specialty Crop Region ## 3 15 20 Central California Coast Range C California Subtropical Fruit, Truck, and Specialty Crop Region ## 4 15 20 Central California Coast Range C California Subtropical Fruit, Truck, and Specialty Crop Region ## 5 15 20 Central California Coast Range C California Subtropical Fruit, Truck, and Specialty Crop Region ## 6 15 20 Central California Coast Range C California Subtropical Fruit, Truck, and Specialty Crop Region ## pID mlra ID MAAT MAP FFD GDD rain_fraction effective_precipitation ## 1 1 15 1 15.19286 1149 306 2303 99 385.6023 ## 2 1 15 2 15.33926 1049 307 2369 99 252.4252 ## 3 1 15 3 15.42254 1041 313 2381 99 242.8284 ## 4 1 15 4 15.44636 1087 308 2382 99 283.1933 ## 5 1 15 5 15.39205 1116 316 2349 99 314.3419 ## 6 1 15 6 15.43280 1058 313 2387 99 258.3234 Summarizing multivariate data by group (MLRA) is usually much simpler after reshaping data from “wide” to “long” format. # reshape from wide to long format m &lt;- tidyr::pivot_longer(s.df, cols = c(MAAT, MAP, FFD, GDD, rain_fraction, effective_precipitation)) # check &quot;wide&quot; format head(m) ## # A tibble: 6 × 10 ## MLRARSYM MLRA_ID MLRA_NAME LRRSYM LRR_NAME pID mlra ID name value ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 15 20 Central California Coast Range C California Subtropical Fruit, Tr… 1 15 1 MAAT 15.2 ## 2 15 20 Central California Coast Range C California Subtropical Fruit, Tr… 1 15 1 MAP 1149 ## 3 15 20 Central California Coast Range C California Subtropical Fruit, Tr… 1 15 1 FFD 306 ## 4 15 20 Central California Coast Range C California Subtropical Fruit, Tr… 1 15 1 GDD 2303 ## 5 15 20 Central California Coast Range C California Subtropical Fruit, Tr… 1 15 1 rain… 99 ## 6 15 20 Central California Coast Range C California Subtropical Fruit, Tr… 1 15 1 effe… 386. A tabular summary of means by MLRA and PRISM variable using dplyr v.s. base tapply(). # tabular summary of mean values dplyr::group_by(m, mlra, name) %&gt;% dplyr::summarize(mean(value)) %&gt;% dplyr::arrange(name) ## # A tibble: 12 × 3 ## # Groups: mlra [2] ## mlra name `mean(value)` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 15 effective_precipitation -197. ## 2 18 effective_precipitation -193. ## 3 15 FFD 284. ## 4 18 FFD 273. ## 5 15 GDD 2387. ## 6 18 GDD 2496. ## 7 15 MAAT 15.2 ## 8 18 MAAT 15.7 ## 9 15 MAP 588. ## 10 18 MAP 631. ## 11 15 rain_fraction 98.6 ## 12 18 rain_fraction 97.2 # base R tapply(m$value, list(m$mlra, m$name), mean, na.rm = TRUE) ## effective_precipitation FFD GDD MAAT MAP rain_fraction ## 15 -196.8961 284.3748 2386.711 15.24977 587.8348 98.60990 ## 18 -192.9192 273.1376 2496.125 15.66251 631.3798 97.21803 4.8.3.4 Faster with exactextractr This example shows how to determine the distribution of Frost-Free Days across a soil series extent. The data are extracted from the raster data source very rapidly using the exactextractr package. library(sf) library(soilDB) library(terra) library(lattice) library(exactextractr) # 5-10 seconds to download Series Extent Explorer data series &lt;- c(&#39;holland&#39;, &#39;san joaquin&#39;) # make SpatialPolygonsDataFrame s &lt;- do.call(&#39;rbind&#39;, lapply(series, seriesExtent)) # load pointer to PRISM data r &lt;- rast(&#39;C:/workspace2/chapter-4/FFD.tif&#39;) # transform extent to CRS of raster with sf s &lt;- st_transform(st_as_sf(s), crs = st_crs(r)) # inspect s # use `st_union(s)` to create a MULTI- POINT/LINE/POLYGON from single # use `sf::st_cast(s, &#39;POLYGON&#39;)` to create other types system.time({ ex &lt;- exactextractr::exact_extract(r, s) }) # ex is a list(), with data.frame [value, coverage_fraction] # for each polygon in s (we have one MULTIPOLYGON per series) # combine all list elements `ex` into single data.frame `ex.all` # - use do.call(&#39;rbind&#39;, ...) to stack data.frames row-wise # - an anonymous function that iterates along length of `ex` # - adding the series name to as a new variable, calculated using `i` ex.all &lt;- do.call(&#39;rbind&#39;, lapply(seq_along(ex), function(i) { cbind(data.frame(group = series[i]), ex[[i]]) })) # simple summary densityplot(~ value | group, data = ex.all, plot.points = FALSE, bw = 2, lwd = 2, strip = strip.custom(bg = grey(0.85)), scales = list(alternating = 1), col = c(&#39;RoyalBlue&#39;), layout = c(1, 2), ylab = &#39;Density&#39;, from = 0, to = 400, xlab = &#39;Frost-Free Days (50% chance)\\n800m PRISM Data (1981-2010)&#39;, main = &#39;FFD Estimate for Extent of San Joaquin and Holland Series&#39; ) 4.8.3.5 Summarizing MLRA Raster Data with lattice graphics Lattice graphics are useful for summarizing grouped comparisons. The syntax is difficult to learn and remember, but there is a lot of documentation online. library(lattice) tps &lt;- list( box.rectangle = list(col = &#39;black&#39;), box.umbrella = list(col = &#39;black&#39;, lty = 1), box.dot = list(cex = 0.75), plot.symbol = list( col = rgb(0.1, 0.1, 0.1, alpha = 0.25, maxColorValue = 1), cex = 0.25 ) ) bwplot(mlra ~ value | name, data = m, # setup plot and data source as.table=TRUE, # start panels in top/left corner varwidth=TRUE, # scale width of box by number of obs scales=list(alternating=3, relation=&#39;free&#39;), # setup scales strip=strip.custom(bg=grey(0.9)), # styling for strips par.settings=tps, # apply box/line/point styling panel=function(...) { # within in panel, do the following panel.grid(-1, -1) # make grid lines at all tick marks panel.bwplot(...) # make box-whisker plot } ) 4.9 Additional Reading (Spatial) Ahmed, Zia. 2020. Geospatial Data Science with R. Gimond, M., 2019. Intro to GIS and Spatial Analysis https://mgimond.github.io/Spatial/ Hijmans, R.J. 2019. Spatial Data Science with R. https://rspatial.org/ Lovelace, R., J. Nowosad, and J. Muenchow, 2019. Geocomputation with R. CRC Press. https://bookdown.org/robinlovelace/geocompr/ Pebesma, E., and R.S. Bivand. 2005. Classes and methods for spatial data: The sp package. https://cran.r-project.org/web/packages/sp/vignettes/intro_sp.pdf. Pebesma, E. and R. Bivand, 2019. Spatial Data Science. https://keen-swartz-3146c4.netlify.com/ Applied Spatial Data Analysis with R "],["sampling.html", "Chapter 5 Sampling 5.1 Introduction 5.2 Sampling Strategies 5.3 Evaluating a Sampling Strategy 5.4 Additional Reading (Sampling)", " Chapter 5 Sampling 5.1 Introduction Sampling is a fundamental part of statistics. Samples are collected to achieve an understanding of a population because it is typically not feasible to observe all members of the population. The goal is to collect samples that provide an accurate representation of the population. Constraints on time and money dictate that the sampling effort must be efficient. More samples are needed to characterize the nature of highly variable populations than less variable populations. Define your purpose: What are you investigating? Examples include soil properties, soil classes, and plant productivity. How many samples are needed? The question of how many samples are required is complex. As per usual, the answer is ‘it depends’. For example, it depends on the type of analysis (e.g. non-spatial vs spatial) and properties of the data (e.g. continuous vs categorical). In all cases it requires an understanding/assumptions of the underlying population. If a scientist can estimate the several population variables, formulas exist to estimate the required number of samples. The following are a list of methods for estimating sample size: Standard error Confidence interval Power analysis (hypothesis testing) Distance/time Prediction error Rules of thumb NSSH 627.8 The first 3 methods use formulas for their corresponding statistics to solve for the sample size, each results in progressively higher sample size requirements. The PracTools R package (Valliant and Dever 2022) offers several functions for the 1st and 2nd methods. The pwr (Champely 2020) and WebPower (Zhang and Mai 2021) R packages offer several functions for the 3rd method, based on a variety of statistical models. Gelman, Hill, and Vehtari (2020) provides a nice graphical illustration of the variables involved with these methods. 5.1.1 DSM classes A variation of the 2nd method is described by Congalton and Green (2019) for use with categorical data, which is applicable to digital soil mapping. nMultinomial &lt;- function(k = NULL, p = 0.5, error = 0.1, alpha = 0.05) { # k = number of classes # p = proportion of the largest class # error = margin of error # alpha = confidence level probability ceiling(qchisq(1 - alpha / k, 1) * p * (1 - p) / error^2) } nMultinomial(k = c(10, 20, 30)) ## [1] 197 229 248 nMultinomial(k = 10, error = c(0.05, 0.1, 0.2)) ## [1] 788 197 50 nMultinomial(k = 10, alpha = c(0.05, 0.1, 0.2)) ## [1] 197 166 136 5.1.2 Rules of thumb Some rules of thumb for regression models are as follows: Use &gt; 10 observations (n) per predictor (m) (Kutner et al. (2005)). Use &gt; 20 n per m and n &gt; 104 + m to test regression coefficients (Rossiter 2017; Franklin and Miller 2009). Never use n &lt; 5*m (Rossiter 2017). 5.1.3 NSSH 627.8 Documentation requirement for the following data elements are specified in the section 627.8 of the National Soil Survey Handbook (NSSH). # NSSH 627.8 Documentation # soil series data.frame( level = &quot;soil series&quot;, n = c(5, 10), acres = c(2000, 20000) ) ## level n acres ## 1 soil series 5 2000 ## 2 soil series 10 20000 # components data.frame( level = &quot;components&quot;, n = 1, acres = 3000 ) ## level n acres ## 1 components 1 3000 # map units data.frame( level = &quot;map unit&quot;, n = cumsum(c(30, rep(10, 3))), acres = c(2000, seq(from = 4000, by = 4000, length.out = 3)) ) ## level n acres ## 1 map unit 30 2000 ## 2 map unit 40 4000 ## 3 map unit 50 8000 ## 4 map unit 60 12000 5.2 Sampling Strategies library(sf) library(ggplot2) # set the seed for the random number generator set.seed(4) # Create a sixteen square polygon bb &lt;- st_make_grid(st_bbox(c(xmin = 0, xmax = 4, ymin = 0, ymax = 4)), n = 4) grd &lt;- st_as_sf(bb) grd$ID &lt;- 1:length(bb) 5.2.1 Simple Random In simple random sampling, all samples within the region have an equal chance of being selected. A simple random selection of points can be made using either the st_sample() function within the sf R package or the Create Random Points tool in ArcGIS. Advantages Simple Unbiased (equal probability of inclusion) Requires little prior knowledge of the population Howell et al. (2004) - produced a “much more sensitive, more accurate, and greater range of estimated values” than the models from the subjective samples Disadvantages Inefficient (requires large numbers) Lower accuracy Higher cost Samples may not be representative of the feature attribute(s) Uneven spatial distribution (e.g. clustered) # Generate simple random sample test &lt;- st_sample(grd, size = 16, type = &quot;random&quot;) ggplot() + geom_sf(data = grd) + geom_sf(data = test) + ggtitle(&quot;Simple&quot;) 5.2.2 Systematic In systematic sampling, a sample is taken according to a regularized pattern. This approach ensures even spatial coverage. Patterns may be rectilinear, triangular, or hexagonal. This sampling strategy can be inaccurate if the variation in the population doesn’t coincide with the regular pattern (e.g., if the population exhibits periodicity). Advantages Simple Precise estimates Even spatial coverage Greater efficiency Disadvantages Biased estimates (particularly sampling variance) May miss individuals that don’t coincide with the sampling interval If so, the density needs to be increased Limited utility for areas larger than a single field Grid may not optimally fit irregular shapes # Generate systematic random sample test &lt;- st_sample(grd, size = 16, type = &quot;regular&quot;) ggplot() + geom_sf(data = grd) + geom_sf(data = test) + ggtitle(&quot;Systematic&quot;) 5.2.3 Spatial Coverage Sampling # spcosa ---- library(spcosa) grd2 &lt;- st_crs(grd, NA) grd_sp &lt;- as(grd, &quot;Spatial&quot;) strata &lt;- stratify(grd_sp, nStrata = 16) pts &lt;- spsample(strata) plot(strata, pts) + ggtitle(&quot;Spatial Coverage&quot;) 5.2.4 Stratified Random In stratified random sampling, the sampling region is spatially subset into different strata, and random sampling is applied to each strata. If prior information is available about the study area, it can be used to develop the strata. Strata may be sampled equally or in proportion to area; however, if the target of interest is rare in the population, it may be preferable to sample the strata equally Franklin and Miller (2009)]. Advantages More efficient than simple-random Higher accuracy Lower cost Sampling can be sized relative to proportion or variance Disadvantages Require pre-existing knowledge of the population (which may be flawed) If sampling is uneven, weights need to be known May need to construct the strata manually grd2 &lt;- st_cast(grd, &quot;MULTIPOLYGON&quot;) st_crs(grd2) &lt;- 5070 test &lt;- st_sample(grd2, size = 16, type = &quot;random&quot;, by_polygon = TRUE) ggplot() + geom_sf(data = grd) + geom_sf(data = test) + ggtitle(&quot;Stratified&quot;) Note that the spsample() function only stratifies the points spatially. Other more sophisticated designs can be implemented using the spsurvey, sharpshootR, or clhs packages. 5.2.5 Multistage Stratified Random In multistage random sampling, the region is separated into different subsets that are randomly selected (i.e., first stage), and then the selected subsets are randomly sampled (i.e., second stage). This is similar to stratified random sampling, except that with stratified random sampling each strata is sampled. Advantages Most efficient Lower cost Sampling can be sized relative to proportion or variance Disadvantages Less precise Uneven spatial distribution (e.g. clustered) Require pre-existing knowledge of the population (which may be flawed) If sampling is uneven, weights need to be known May need to construct the strata manually # Select 8 samples from each square idx &lt;- sample(1:nrow(grd), size = 2, replace = FALSE) grd_sub &lt;- grd[idx, ] test &lt;- sapply(1:2, function(i) { st_coordinates(st_sample(grd_sub[i, ], size = 8, type = &quot;random&quot;)) }) test &lt;- st_as_sf(as.data.frame(test), coords = 1:2) ggplot() + geom_sf(data = grd) + geom_sf(data = test) + ggtitle(&quot;Two-stage&quot;) 5.2.6 Conditioned Latin Hypercube (cLHS) Conditioned Latin hypercube sampling is a stratified random sampling technique to obtain representative samples from feature (attribute) space (Minasny and McBratney 2006). Advantages Maximumly stratifies the predictors Automated sample selection Incorporates cost constraints Can incorporate legacy points Easily scales to multiple GIS layers Disadvantages Not ideal for map validation (non – probability sample) Not ideal for developing map unit concepts (puts points in weird landscape positions) Difficult to find alternatives for non-accessible points Inefficient with large GIS layers For example, assume you have prior knowledge of a study area and have the time and resources to collect 120 points. You also know the following variables (strata), which are represented as coregistered raster datasets, to be of importance to the soil property or class being investigated: Normalized Difference Vegetation Index (NDVI), Topographic Wetness Index (a.k.a. Wetness Index, compound topographic index), Solar insolation (potential incoming solar radiation), and Relative elevation (a.k.a. relative position, normalized slope height). The cLHS procedure iteratively selects samples from the strata variables such that they replicate the range of values from each stratum. Without a technique such as cLHS, obtaining a sample that is representative of the feature space becomes increasingly difficult as the number of variables (strata) increases. To perform cLHS using R, you can use the clhs package (Roudier 2011). library(clhs) library(raster) # import volcano DEM, details at http://geomorphometry.org/content/volcano-maungawhau data(volcano) volcano_r &lt;- raster(as.matrix(volcano[87:1, 61:1]), crs = CRS(&quot;+init=epsg:27200&quot;), xmn = 2667405, xmx = 2667405 + 61 * 10, ymn = 6478705, ymx = 6478705 + 87 * 10) names(volcano_r) &lt;- &quot;elev&quot; # calculate slope from the DEM slope_r &lt;- terrain(volcano_r, opt = &quot;slope&quot;, unit = &quot;degrees&quot;) # Stack Elevation and Slope rs &lt;- stack(volcano_r, slope_r) # generate cLHS design cs &lt;- clhs(rs, size = 20, progress = FALSE, simple = FALSE) # Plot cLHS Samples par(mar=c(1,1,1,4)) plot(volcano_r, axes=FALSE) points(cs$sampled_data) # Summary of clhs object summary(cs$sampled_data)$data ## elev slope ## Min. : 96.0 Min. : 0.000 ## 1st Qu.:109.5 1st Qu.: 7.023 ## Median :126.0 Median :13.832 ## Mean :130.8 Mean :14.718 ## 3rd Qu.:146.8 3rd Qu.:21.348 ## Max. :181.0 Max. :37.351 ## NA&#39;s :1 # Summary of raster objects cbind(summary(volcano_r), summary(slope_r)) ## elev slope ## Min. 94 0.000000 ## 1st Qu. 108 7.054131 ## Median 124 14.103463 ## 3rd Qu. 150 21.665758 ## Max. 195 43.032469 ## NA&#39;s 0 292.000000 Although the above example works well on our small volcano dataset, the clhs package is inefficient if you are working with large raster datasets. To overcome this limitation, you can first take a large random sample and then subsample it using cLHS. sub_s &lt;- sampleRandom(volcano_r, size = 200, sp = TRUE) # random sample function from the raster package s &lt;- clhs(sub_s, size = 20, progress = FALSE, simple = FALSE) 5.3 Evaluating a Sampling Strategy To gauge the representativeness of a sampling strategy, you can compare the results it produces to the results for other variables you think might coincide with the soil properties or classes of interest (Hengl 2009). Examples include slope gradient, slope aspect, and vegetative cover. These other variables may be used to stratify the sampling design or to assess the representativeness of our existing samples (e.g., NASIS pedons). The simple example below demonstrates how to compare several sampling strategies by evaluating how well they replicate the distribution of elevation. # set seed set.seed(1234) # create a polygon from the spatial extent of the volcano dataset test &lt;- st_make_grid(extent(volcano_r), n = 1) # take a large random sample sr400 &lt;- st_sample(test, size = 400, type = &quot;random&quot;) # take a small random sample sr &lt;- st_sample(test, size = 20, type = &quot;random&quot;) sr2 &lt;- sampleRandom(volcano_r, size = 20) # take a small systematic random sample sys &lt;- st_sample(test, size = 20, type = &quot;regular&quot;) sys2 &lt;- sampleRegular(volcano_r, size = 20) # take a cLHS sample cs &lt;- clhs(rs, size = 20, progress = FALSE, simple = FALSE) # Combind and Extract Samples s &lt;- rbind(data.frame(method = &quot;Simple Random 400&quot;, extract(rs, st_coordinates(sr400)) ), data.frame(method = &quot;Simple Random&quot;, extract(rs, st_coordinates(sr)) ), data.frame(method = &quot;Systematic Random&quot;, extract(rs, st_coordinates(sys)) ), data.frame(method = &quot;cLHS&quot;, slot(cs$sampled_data, &#39;data&#39;) ) ) # Summarize the sample values aggregate(slope ~ method, data = s, function(x) round(summary(x))) ## method slope.Min. slope.1st Qu. slope.Median slope.Mean slope.3rd Qu. slope.Max. ## 1 cLHS 1 8 14 15 21 39 ## 2 Simple Random 0 7 17 15 23 28 ## 3 Simple Random 400 0 6 14 15 21 37 ## 4 Systematic Random 0 4 12 13 18 30 # Plot overlapping density plots to compare the distributions between the large and small samples ggplot(s, aes(x = slope, col = method)) + geom_density(cex = 2) volcano_r2 &lt;- as(volcano_r, &quot;SpatialPixelsDataFrame&quot;) # plot the spatial locations par(mfrow = c(1, 3), mar=c(1,2,4,5)) plot(volcano_r, main = &quot;Simple random&quot;, cex.main = 2, axes=FALSE) points(st_coordinates(sr), pch = 3, cex = 1.2) plot(volcano_r, main = &quot;Systematic random&quot;, cex.main = 2, axes=FALSE) points(st_coordinates(sys), pch = 3, cex = 1.2) plot(volcano_r, main = &quot;cLHS&quot;, cex.main = 2, axes=FALSE) points(cs$sampled_data, pch = 3, cex = 1.2) The overlapping density plots above illustrate the differences between large and small sets of samples using several sampling designs. The cLHS approach best duplicates the distribution of elevation (because elevation is explicitly used in the stratification process). The contrast is less severe in the summary metrics, but again cLHS more closely resembles the larger sample. Other comparisons are possible using the approaches in the following chapters. 5.3.1 Exercise: Design a Sampling Strategy Load the “tahoe_lidar_highesthit.tif” dataset in the gdalUtilities package (i.e. tahoe &lt;- raster::raster(system.file(\"extdata/tahoe.tif\", package = \"gdalUtilities\"))) or use your own data set. Compare two or more sampling approaches and evaluate how representative they are. Show your work and submit the results to your mentor. 5.4 Additional Reading (Sampling) Brungard, C., &amp; Johanson, J. (2015). The gate’s locked! I can’t get to the exact sampling spot … can I sample nearby? Pedometron(37), 8-9. http://www.pedometrics.org/Pedometron/Pedometron37.pdf Brus, D.J. and J.J. de Gruijter. “Random sampling or geostatistical modelling? Choosing between design-based and model-based sampling strategies for soil (with discussion).” Geoderma. Vol. 80. 1. Elsevier, 1997. 1-44. https://www.sciencedirect.com/science/article/pii/S0016706197000724 Brus, Dick J. Spatial Sampling with R. First edition, CRC Press, 2022, https://github.com/DickBrus/SpatialSamplingwithR. de Gruijter, J., D.J. Brus, M.F.P. Bierkens, and M. Knotters. 2006. Sampling for natural resource monitoring: Springer. http://www.springer.com/us/book/9783540224860. Viscarra Rossel, R.A., et al. “Baseline estimates of soil organic carbon by proximal sensing: Comparing design-based, model-assisted and model-based inference.” Geoderma 265 (2016): 152-163. https://www.sciencedirect.com/science/article/pii/S0016706115301312 References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
