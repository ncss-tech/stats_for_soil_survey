---
title: "Linear Regression"
author: "John R. Hammerly"
date: "`r format(Sys.Date(), format='%B %d %Y')`"
output:
  ioslides_presentation:
    widescreen: yes
nosite: |
  @bishop2001comparison, @faraway2002practical, @holland2011data, @seybold2009estimating, @whittingham2006we, @wills2013quantifying, @matthews2000storks
bibliography: bibliography.bib
---

## Objectives

- Understand linear regression and describe a case study
- Compute and interpret coefficients in a linear regression analysis in R.
- Interpolate regression model in R to produce a raster layer.

<div class="notes">
The general format for this topic of linear regression in the statistics for soil survey course will aim to complete the following 3 objectives.  This presentation will introduce the concepts of linear regression and provide a case study as an example.  Next, R will be used for generating the coefficients in a linear regression analysis.  Finally, a regression model in R will be used to produce a raster layer.
</div>

## Science

What are we doing?

> - Explain
> - Predict

<div class="notes">
Let's stop for a moment before we begin and discuss science.  Are there a few scientists out there?  Will we ever explain and predict everything about soil? Or plants? Or animals? No. I had a science teacher Mr. Wilkening in high school who always asked if anyone had any questions from our assignments due that day. One time, he simply asked "Any questions?" As a joke someone asked, "What is the meaning of life?" He calmly replied "42". Then he asked "Any other questions?" Unfortunately, no one in the class understood his clever reference to the popular book "A Hitchhiker's Guide to the Galaxy" at the time, including me. Has anyone read this book? I still haven't.  We will never know everything there is to know about soil (or the meaning of life). But we can however, explain and predict many interesting things about soil, and today we will discuss one way of doing these 2 things with a statistical analysis called linear regression.
</div>

## Connections

Why is it windy in Iowa?

> - Missouri sucks and Minnesota blows
> - No connection

Where do babies come from?

> - Storks deliver babies
> - No connection

Why are basements in Iowa full of cracks?

> - Soils in Iowa contain high amounts of shrink-swell clays
> - Connection

<div class="notes">
If we want to predict or explain something, like why it's windy in Iowa, we need to look for connections.  If this question reminds you of the punchline to a joke, you'd of course know the answer.  It's because Missouri sucks and Minnesota blows. This might be a funny joke (if you're from Iowa). Unfortunately though, there isn't a connection between how lousy a person thinks a couple of surrounding states are, and the average wind speed of their state.  This example demonstrates the importance of having a connection between a dependent variable and an independent variable.  The dependent variable is the one we are trying to predict. The independent variable is the one we are using to make the prediction. Another famous example demonstrating the importance of a connection is an attempt to explain where babies come from. If you plot the human birth rate and number of storks across several European countries, as Robert Matthews, a professor at Aston University has already done, you might come to the incorrect conclusion that storks deliver babies. If I ask why are basements in Iowa full of cracks, you might predict it has something to do with the soil surrounding the basements.  This is an example of a dependent and independent variable which have a connection.
</div>

## Predictions and Predicting with a Function

Predictions

> - None of us will report to work on Sunday
> - The average price of a gallon of gas in the US will be $100.00 on January 1

Predicting with a function

> - $$y= \beta_0 + \beta_1x + \epsilon$$

<div class="notes">
So let's make some predictions! Making predictions is easy.  I predict none of us will report to work on Sunday. This is probably a good prediction, since Sunday is not part of the typical work week.  I predict the average price of a gallon of gas in the US will be $100.00 on January 1. This is far less likely to be a good prediction. We don't want to just make predictions, we want to make good predictions. How? We can use a function.  This is a linear prediction function which includes a random error term.  Y is the dependent variable, what we want to predict. Beta zero is a constant term, the intercept.  It represents the predicted value of y when x=0.  What is x? X is the independent variable, what you are using to make the prediction. Going back now to beta one, this term is the slope of the line. The predicted change in y for each one unit change in x. Finally at the end of the function, epsilon. This is the random error term I mentioned before. Without the random error term, as a function, y can only represent an exact linear relationship between x. Including the random error term allows us to take into account any deviation of the actual y values which exist in our dataset from our predictions. To put it another way, the random error term accounts for other unpredictable factors. Why are we using a linear function? Our topic is linear regression right? It turns out lines can do a pretty good job predicting dependent variables as long as the slope of the line doesn't change as x changes. If this happened, it wouldn't really be a line.
</div>

## Assumptions

> - Linear
> - Independent
> - Homoscedastic
> - Normal

<div class="notes">
For linear regression to work well, there are 4 assumptions we must understand. As I just mentioned, the relation between what you are using to predict and what's being predicted needs to be linear. Suppose I wanted to predict percent clay in soils with argillic horizons using soil depth.  The typical "clay bulge" occurring with depth doesn't fit a linear relationship so linear regression won't work in this case. The second assumption is the independence of errors.  This means the errors are not related to each other.  Errors may not be independent if they tend to be the same within specific conditions.  The third assumption requires the variables are homoscedastic.  This means the variables have a constant variation.  If the variation is not constant, (heteroscedastic) this would indicate the function, for example is accurate predicting low values, but not when predicting high values.  Finally, the last assumption made in linear regression is that the errors are normally distributed.  We will revisit these assumptions a bit later.
</div>

## Interpreting the Linear Regression Model

```{r, warning = FALSE, echo = FALSE, message = FALSE}
library(ggplot2)
library(rms)

d<-trees
dd <- datadist(d)
options(datadist = "dd")
fit<- ols(Height ~ Volume, d)

ggplot(trees, aes(y = Height, x = Volume)) +
  geom_point() + stat_smooth(method = "lm", col = "black", se = F) +
  labs(title = "Black Cherry Trees") +
  expand_limits(x = 0, y = 0) +
  expand_limits(y = 80)  +
  annotate("segment", x = 0, xend = 0, y = 70, yend = 0, color = "red", size = 1.2) +
  annotate("text", x=5, y = 35, label = "beta[0]", parse = TRUE, size = 5) +
  annotate("segment", x = 60, xend = 75, y = 83, yend = 83, color = "red", size = 1.2) +
  annotate("text", x = 68, y = 78, label = "Delta~italic('x')", parse = TRUE) +
  annotate("segment", x = 75, y = 83, xend = 75, yend = 86.5, color = "red", size = 1.2) +
  annotate("text", x = 78, y = 84.5, label = "Delta~y", parse = TRUE) +
  annotate("segment", x = 27.4, y = 86, xend = 27.4, yend = 75.4, color = "red", size = 1.2) +
  annotate("text", x= 28.5, y = 81, label = "epsilon", parse = T, size = 6) +
  annotate("segment", x = 25.7, y = 71, xend = 25.7, yend = 75, color = "red", size = 1.2) +
  annotate("text", x = 27, y = 73.5, label = "epsilon", parse = T, size = 6) +
  annotate("text", x = 45, y =30, label = "y==beta[0]~+~beta[1]~italic('x')~+~epsilon", parse = TRUE, size = 6) +
  annotate("text", x = 45, y =20, label = paste0("Height==", round(fit$coefficients[1],0), "+", round(fit$coefficients[2],1), "~Volume~+~epsilon"), parse = TRUE, size = 6) +
  annotate("text", 65, y = 60, label = "beta[1]==frac(Delta~y, Delta~x)", parse = TRUE, size = 6)

```

<div class="notes">
Let's go back to the function again with some visuals.  I generated this plot from a sample dataset available in R called trees.  This dataset contains 31 observations of black cherry trees height, girth and volume.  This plot shows height on the y axis and volume on the x axis.  You can see the equation here again.  I've added annotations to the plot to help explain each term in the equation, and an example of what the terms would be in this particular case.  First, the y term, which is our y axis - Height.  Then beta zero, which is the value of y where the line crosses or "intercepts" the y axis.  This also corresponds to the value of y when x equals zero.  The next term, beta one is the calculated slope of the line.  We can calculate this by looking at the change in y, shown as delta y on the plot divided by the change in x. The x term is the x axis, Volume, and finally, epsilon, the error term is the difference between what an actual observation is and the line.
</div>

## Building the Linear Regression Model

> - Ordinary Least Squares
> - $$\beta_1= \frac{\sum(x_i - \bar x) (y_i - \bar y)} {\sum(x_i - \bar x)^2}$$
> - $$\beta_0= \bar y - \beta_1 \times \bar x$$

<div class="notes">
Where does this line come from? How do we know if the line is in the right spot?  The position of the line is calculated using ordinary least squares.  This method results in the least error over all the data points. The formula for beta one takes the sum of the product of the differences of each individual x and y observation and their means, over the sum of the squared differences of each individual x and the mean.  Beta zero is the mean y value minus the product of beta one and the mean x. If all these equations are getting confusing, don't worry, these can all be calculated quickly and easily in R, which we'll cover later.
</div>

## Examples

Wills et al., 2013

Carbon equivalent correction regression factor:
$$OC_{dc}= 0.25 + 0.86(OC_{wc})$$
where

$OC_{dc}=$
organic carbon by dry combustion (%)

$OC_{wc}=$
organic carbon by wet combustion (%)


<div class="notes">
Let's take a look at a few real world examples in soil science.  This equation is from some work Syke Wills and others did back in 2013.  The idea was to develop a way to predict soil organic carbon when different methods were used.  What value in the equation represents beta zero, or the "y intercept"? (0.25).  What value represents beta one, or the "slope" of the line? (0.86).
</div>

## Examples

<img src="ss.png" width = "75%" height = "75%">

<div class="notes">
Here is another example from Cathy Seybold and others. Each group has a different equation for predicting extractable acidity.
</div>

## Examples

<img src="nasis.png" width = "90%" height = "90%">

<div class="notes">
These are the same equations used in NASIS for the Extractable Acidity calculation.  Those of you using these calculations were using the predictive power of linear regression, and maybe you weren't even aware of it! Also note some equations have more than one independent variable.
</div>

## Simple vs. Multiple Linear Regression

>- Simple linear regression (SLR):  
Y is predicted from **one** independent variable ($x$)
>- Multiple linear regression (MLR):
Y is predicted from **two or more** independent variables ($x_1,x_2,x_3...$)

<div class="notes">
The organic carbon equation is example of simple linear regression, and the extractable acidity equations are either simple or multiple linear regression, depending on the category.
</div>

## Assumptions

> - Linear
> - Independent
> - Homoscedastic
> - Normal

<div class="notes">
Going back to our assumptions - what were they again?  Right, OK, how do we check if the assumptions are good and true assumptions?
</div>

## Testing Model Assumptions

- Normality
    + Histograms
    + QQ plots
    + Residual plots

- Outliers  
    + QQ plots
    + Box plots

- Multicollinearity  
    + Correlation $\geq0.7$ or $\leq-0.7$ indicates highly correlated
    + Variance inflation factors

- Homoscedasticity
    + Residual plots

<div class="notes">
We can check if many of these assumptions hold true using several things.  To check normality we can use histograms, QQ plots and residual plots.  For finding outliers we can also use QQ plots, or box plots.  We can check for multicollinearity by looking at the correlation values and the variance of the inflation factors.  And finally we can use residual plots to check for homoscedasticity.  Let's look at these tests in more detail.
</div>

```{r, echo = FALSE, results='hide', include=FALSE}
c1 <- curve((x^2)+10, from = 0, to = 10)
c1 <- data.frame(c1)
c2 <- curve((x^2)+10, from = 0, to = -10)
c2 <- data.frame(c2)
c3 <- curve((x^2)-40, from = 0, to = -10)
c3 <- data.frame(c3)
c4 <- curve((x^2)-40, from = 0, to = 10)
c4 <- data.frame(c4)
c5 <- curve((-x^2)-10, from = 0, to = -10)
c5 <- data.frame(c5)
c6 <- curve((-x^2)-10, from = 0, to = 10)
c6 <- data.frame(c6)
```

## Heteroscedasticity

<div class="notes">
These 4 plots give a general view of data which is heteroscedastic.  In the upper left plot, the data is bunched together in a line around the y axis.  The upper right plot shows the data forming a "U" shape.  The bottom left shows data tightly grouped on the left and spreading out moving to the right.  Finally, the bottom right plot shows a spread of data at the ends of the plot, and tight grouping in the middle.  These trends in data indicate the data is not homoscedastic, rather it is heteroscedastic.
</div>

```{r, echo = FALSE, fig.show='hold', out.width = '40%'}
ggplot() +
  geom_hline(yintercept = 0) +
  geom_rect(aes(xmin = 0, ymin = -3, xmax = 10, ymax = 3), alpha = 0.5) +
  expand_limits(y = 10) +
  expand_limits(y = -10) +
  labs(x = "Predicted y", y = "Standardized Residuals") +
  theme(axis.text.x=element_blank()) +
  theme(axis.text.y = element_blank()) +
  theme(axis.ticks.x=element_blank()) +
  theme(axis.ticks.y=element_blank()) +
  theme(axis.title=element_text(size=20))

ggplot() +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(x = c1$x, ymax = c1$y, ymin = 0), alpha = 0.5) +
  geom_ribbon(aes(x = c2$x, ymax = c2$y, ymin = 0), alpha = 0.5) +
  geom_ribbon(aes(x = c3$x, ymax = c3$y, ymin = 0), alpha = 0.5) +
  geom_ribbon(aes(x = c4$x, ymax = c4$y, ymin = 0), alpha = 0.5) +
  expand_limits(y = 10) +
  expand_limits(y = -10) +
  labs(x = "Predicted y", y = "Standardized Residuals") +
  theme(axis.text.x=element_blank()) +
  theme(axis.text.y = element_blank()) +
  theme(axis.ticks.x=element_blank()) +
  theme(axis.ticks.y=element_blank()) +
  coord_cartesian(xlim = c(-4, 4), ylim = c(50, -50)) +
  theme(axis.title=element_text(size=20))  

ggplot() +
  geom_hline(yintercept = 0) +
  geom_polygon(aes(x = c(0,10,10,0), y = c(2,7,-7,-2)), alpha = 0.5) +
  expand_limits(y = 10) +
  expand_limits(y = -10) +
  labs(x = "Predicted y", y = "Standardized Residuals") +
  theme(axis.text.x=element_blank()) +
  theme(axis.text.y = element_blank()) +
  theme(axis.ticks.x=element_blank()) +
  theme(axis.ticks.y=element_blank()) +
  theme(axis.title=element_text(size=20))

ggplot() +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(x = c1$x, ymax = c1$y, ymin = 0), alpha = 0.5) +
  geom_ribbon(aes(x = c2$x, ymax = c2$y, ymin = 0), alpha = 0.5) +
  geom_ribbon(aes(x = c5$x, ymax = c5$y, ymin = 0), alpha = 0.5) +
  geom_ribbon(aes(x = c6$x, ymax = c6$y, ymin = 0), alpha = 0.5) +
  expand_limits(y = 10) +
  expand_limits(y = -10) +
  labs(x = "Predicted y", y = "Standardized Residuals") +
  theme(axis.text.x=element_blank()) +
  theme(axis.text.y = element_blank()) +
  theme(axis.ticks.x=element_blank()) +
  theme(axis.ticks.y=element_blank()) +
  coord_cartesian(xlim = c(-4, 4), ylim = c(50, -50)) +
  theme(axis.title=element_text(size=20))
```

## Analysis of Residuals

- Heteroscedasticity
    + causes estimates of regression coefficients to be less precise
- Non-normality
    + compromises interpretability of significance tests of the regression coefficients
- Multicollinearity
    + over-estimates the variances of the regression coefficients
- Spatial Autocorrelation
    + results in an underestimation of the standard error of the estimates of the regression coefficients and a bias towards rejecting the $H_0$ that the value of the coefficient is zero
    
<div class="notes">
As we look at residuals we may find issues such as these.
</div>
    
## Interpreting Model Results

```{r, echo = FALSE, warning = FALSE}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

```{r, output.lines=10}
fit
```

<div class="notes">
The first part of the summary output shows the model formula, followed by the quartiles, or 5 number summary of the residuals.
</div>

## Interpreting Model Results

```{r, output.lines=10:20}
fit
```

<div class="notes">
The next part of the summary lists the estimate of the y-intercept and the slope of each independent variable.  It also provides the standard error.

T values test the hypothesis that the coefficient is different from 0. You can get the t-values by dividing the coefficient by its standard error. The t-values also show the importance of a variable in the model.

The next column to the right is the two-tail p-values which test the hypothesis that each coefficient is different from 0.

The last part is the Residual standard error which is the square root of the residual sum of squares over the degrees of freedom.  The degrees of freedom calculated by taking the number of observations and subtracting 2 (n-2).

$R^2$ shows the amount of variance of Y explained by X. Adjusted $R^2$ shows the same as $R^2$ but adjusted by the # of cases and # of variables. When the # of variables are small and the # of cases is very large then Adj $R^2$ is closer to $R^2$. This provides a more honest association between X and Y.

F-statistic = tests the null hypothesis that all the model coefficients are 0

The p-value of the model tests whether $R^2$ is different from 0. 
</div>

## Diagnostic Plots

- Residuals vs Fitted
- Quantile - Quantile (QQ)
- Spread-Location
- Leverage Plot

<div class="notes">
A series of diagnostic plots can easily be generated after the linear model is created in R by using plot(fit) where fit is the object created from the lm() function. You can also create the plots yourself in ggplot2, which I used in these examples.
</div>

## Residuals vs Fitted

```{r, out.width = '50%'}
ggplot() +
  geom_point(aes(x = fit$fitted.values, y = fit$residuals)) +
  geom_smooth(aes(x = fit$fitted.values, y = fit$residuals),
              method = "loess", formula = "y ~ x", se = F) +
  geom_abline(intercept = 0, slope = 0, linetype = "dashed") +
  labs(title = "Residuals vs Fitted", x = "Fitted Values", y = "Residuals")

```

<div class="notes">
This first plot is a residuals vs fitted plot which can be used to detect non-linearity, unequal error variances, and outliers. In a perfect world, the observations would be equally spread around a horizontal line without any distinct patterns.

Questions:

Is there a linear trend?
What about their variances?
Are there any outliers?
</div>

## QQ Plot

```{r, out.width = '50%'}
ggplot() +
  geom_qq(aes(sample = fit$residuals)) +
  geom_qq_line(aes(sample = fit$residuals), linetype = "dashed") +
  labs(title = "Normal Q-Q", x = "Theoretical Quantiles \n ols(Height ~ Volume)",
       y = "Standardized Residuals")
```

<div class="notes">
Quantile plot is a probability plot that can be used to compare the shapes of the data vs theoretical distribution, i.e. location, scale, and skewness. If the quantiles of the theoretical and data distributions agree, observations will plot on the line shown in the figure above. What do you notice about the tails of this distribution? The tails are positively skewed and there are a few outliers denoted by the observations labels--same outliers in this plot as the last. 
</div>

## Spread-Location

```{r, out.width = '40%'}
ggplot() +
  geom_point(aes(x = fit$fitted.values, y = sqrt(abs(fit$residuals)))) +
  geom_smooth(aes(x = fit$fitted.values, y = sqrt(abs(fit$residuals))),
              method = "loess", formula = "y ~ x", se = F) +
  geom_abline(intercept = 0, slope = 0, linetype = "dashed") +
labs(title = "Scale-Location", x = "Fitted Values \n ols(Height ~ Volume)",
     y = "Square Root of Residuals")
```

<div class="notes">
This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It's good if you see a horizontal line with equally (randomly) spread points. What does this plot tell you?
</div>

## Leverage Plot

```{r, out.width = '40%'}
class(fit) <- "lm"
ggplot() +
  geom_point(aes(x = hat(fit$fitted.values), y = fit$residuals, size = cooks.distance(fit))) +
  geom_smooth(aes(x = hat(fit$fitted.values), y = fit$residuals),
              method = "loess", formula = "y ~ x", se = F) +
  geom_abline(intercept = 0, slope = 0, linetype = "dashed") +
labs(title = "Residuals vs Leverage", x = "Leverage \n ols(Height ~ Volume)",
     y = "Standardized Residuals")
```

<div class="notes">
Unlike the other plots, this time patterns are irrelevant. Leverage is a measure of how much each data point influences the regression. Because the regression must pass through the centroid, points that lie far from the centroid have greater leverage, and their leverage increases if there are fewer points nearby. As a result, leverage reflects both the distance from the centroid and the isolation of a point. The plot also shows values of Cook's distance, which measures how much the regression would change if a point was deleted. Cook's distance is increased by leverage and by large residuals: a point far from the centroid with a large residual can severely alter the regression. On this plot, you want to see that the blue smoothed line stays close to the horizontal dashed line and that no points have a large Cook's distance (i.e. >0.5). What do you notice about this plot?
</div>

## Exercise: Linear Regression

<http://ncss-tech.github.io/stats_for_soil_survey/book2/linear-regression.html>

<div class="notes">
Are there any questions so far?  Ok, let's continue with an exercise you can find this on the chapters page on GitHub.
</div>

## Summary

- Linear regression models are intuitive, quick to execute, and easy to interpret, making them useful for NASIS calculations and pedotransfer functions. 
- Due to the non-linear nature of environmental data, data transformations or deletions are often needed to meet model assumptions. 
- Tacit knowledge is needed throughout model development.

<div class="notes">
Let's do some review on Linear Regression.
</div>

## Linear Regression in R

$$ols(formula, data)$$

**formula** $response \sim predictor_1+predictor_2+predictor_x$  
**data** specifies the dataset
**fit**    detailed statistical summary  

```{r, eval = FALSE}
library(rms)
d <- trees
dd <- datadist(d)
options(datadist = "dd")
fit <- ols(Height ~ Volume, d)
fit
```

<div class="notes">
These are the commands you can use in R to create and interpret a linear regression model.
</div>

## Linear Regression in R - Diagnostics Tests

**cor()** - correlation matrix  
  
**hist()** - histogram  
  
**vif()** - variance-inflation and generalized variance inflation factors for linear and generalized linear models (in car package)  

```{r, eval = FALSE}
cor(d)
hist(d$Height)
library(car)
vif(fit)
```

<div class="notes">
Additional commands you can use in R to create and interpret a linear regression model.
</div>

## Other Types of Regression

**Step-wise regression** - typically used for exploratory data analysis or datasets with large sets of predictors; not recommended for modeling due to its unreliability  
  
**Weighted least squares regression** - potentially useful when the homoscedasticity assumption is violated in OLS; gives more weight to observations with small error variance. WLS is not recommended unless the variance structure is known.  
  
**Logistic regression** - useful when predicting a binary outcome from a set of continuous predictor variables.

<div class="notes">
Linear regression is not the only type of regression.  Stephen will cover logistic regression in more detail next.
</div>

## Review Questions

- Kahoot (<http://kahoot.it>)

<div class="notes">
It's time for some review questions.  Use the link provided to access the Kahoot Quiz.
</div>

## Additional Resources

Sum of Squares Regression: $SSR = \sum(\hat{y}-\bar{y})$  
  
Sum of Squares Error $SSE = \sum(y-\hat{y})$  
  
Total Sum of Squares $SST = \sum(y - \hat{y})^2 = SSR + SSE$  
  
  
where  
  
$y=$ observed  
  
$\hat{y}=$ predicted  
  
$\bar{y}=$ population mean

<div class="notes">
Here are some additional resources, and equations you may find helpful after this course concludes.
</div>

## Additional Resources

Root Mean Square Error $=\sqrt{MSE}$  
  
$R^2=\frac{SSR}{SST}$  
  
$R^2(adj)=1 - \big( \frac{N - 1}{N - K - 1}\big) \frac{SSE}{SST}$  
  
Residual Standard Error $=\sqrt{\frac{SSE}{N - 2}}$  
  
$t=\frac{\beta}{SE_\beta}$  
  
N = number of observations  
K = number of variables  
$\beta$ = coefficient

## Additional Resources - ANOVA

|Source|SS|DF|MS|F|
|------|--|--|--|-|
|Regression (or explained)|$SSR$|$K$|$MSR = \frac{SSR}{K}$|$F=\frac{MSR}{MSE}$|
|Error (or residual)|$SSE$|$N-K-1$|$MSE = \frac{SSE}{(N-K-1)}$||
|Total|$SST$|$N-1$|$MST=\frac{SST}{(N-1)}$||

An alternative formula for F, which is sometimes useful when the original data are not available (e.g. when reading someone else's article) is  
$F=\frac{R^2\times(N-K-1)}{(1-R^2)\times K}$
  
where N = number of observations  and K = number of variables

<div class="notes">
And some resources on ANOVA as well.
</div>

## References

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):before {
  background: none;
}
</style>