---
title: Tabular Data in R
author: Jay Skovlin, Dylan Beaudette, Stephen Roecker
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: no
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: false
editor_options: 
  chunk_output_type: console
---

![](../1_introduction/figure/logo.jpg)  

```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(knitr, quietly=TRUE)

opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.align='center', fig.retina=2, dev='png', tidy=FALSE, verbose=FALSE, antialias='cleartype', cache=FALSE)

# options for R functions
options(width=100, stringsAsFactors=FALSE)
```

# Objectives
- Learn more about R and how to inspect objects and data types
- Use the soilDB package to load NASIS pedon data into R
- Learn about the checks run on data loaded by the fetchNASIS() function
- Understand the structure of data stored in a Soil Profile Collection (SPC)
- Learn ways to filter and subset SPC data in R
- Learn how functions can be used to bundle operations
- Review additional data that is accessible via extended data functions


# The soilDB Package

What if you could extract, organize, and visualize data from NASIS and many other commonly used soil database sources with a couple of lines of code? 

![](figure/soilDB_figure.png)


## soilDB Functions

- **fetchNASIS()**
    - Gets and re-packages data from a local NASIS database.
        + [NASIS pedon/horizon data](http://ncss-tech.github.io/AQP/soilDB/fetchNASIS-mini-tutorial.html)
        + [NASIS DMU/MU/component data](http://ncss-tech.github.io/AQP/soilDB/NASIS-component-data.html)
        
- **fetchNASISLabData()**
    - Gets KSSL laboratory pedon/horizon layer data from a local NASIS database.
    
- **fetchKSSL()**
    - Gets KSSL data from the SoilWeb system via BBOX, MLRA, or series name query.
        + [KSSL Data Demo](http://ncss-tech.github.io/AQP/soilDB/KSSL-demo.html)
        + [Water Retention Curve Development from KSSL Data](http://ncss-tech.github.io/AQP/soilDB/fetchKSSL-VG-demo.html)
- **fetchOSD()**
    - Fetches a limited subset of horizon- and site-level attributes for named soil series from the SoilWeb system.
- **fetchRaCA()**
    - Gets Rapid Carbon Assessment (RaCA) data by State, geographic bounding-box, RaCA site ID, or series query from the SoilWeb system.
        + [RaCA Data Demo](http://ncss-tech.github.io/AQP/soilDB/RaCA-demo.html)
- **fetchSCAN()**
    - Queries soil and climate data from USDA-NRCS SCAN Stations.
      + [A Unified Interface to SCAN/SNOTEL Data](http://ncss-tech.github.io/AQP/soilDB/fetchSCAN-demo.html)
- **fetchHenry()**
    - Downloads data from the Henry Mount Soil Climate Database.
        + [Henry Mount Soil Climate Database Tutorial](http://ncss-tech.github.io/AQP/soilDB/Henry-demo.html)
- **fetchPedonPC()**
    - Fetches commonly used site and horizon data from a PedonPC v.5 database.
- **fetchSDA()**
    - Fetches component data from Soil Data Access.
- **SDA_query()**
    - Submits queries to the Soil Data Access system.
        + [Soil Data Access Tutorial](http://ncss-tech.github.io/AQP/soilDB/SDA-tutorial.html)
        + [SDA and Spatial Data](http://ncss-tech.github.io/AQP/soilDB/SDA-tutorial-2.html)
        + [SDA and Interpretations](http://ncss-tech.github.io/AQP/soilDB/SDA-cointerp-tutorial.html)


### Package References

 * [**Tutorials on the AQP website**](http://ncss-tech.github.io/AQP/)
 * [**Package 'aqp' manual**](http://ncss-tech.github.io/aqp/docs/)
 * [**Package 'soilDB' manual**](http://ncss-tech.github.io/soilDB/docs/)
 * [**Package 'sharpshootR' manual**](http://ncss-tech.github.io/sharpshootR/docs/)



## Importance of Pedon Data

The importance of pedon data for present and future work cannot be overstated.  These data represent decades of on-the-ground observations of the soil resource for a given area.  As difficult as it may be to take the time to enter legacy pedon data, it is vitally important that we capture this resource and get these data into NASIS as an archive of point observations.  

```{r pedons_a, echo=FALSE, results='hide', warning=FALSE}
library(ggplot2)
library(dplyr)

data("us_ss_timeline", package = "soilDB")

test <- as.data.frame(table(us_ss_timeline$year), stringsAsFactors = FALSE)

names(test)[names(test) %in% c("Var1", "Freq")] <- c("year", "Count")
test <- mutate(test, 
               year = as.numeric(year)
               )



g1 <- ggplot(test, aes(x = year, y = Count)) +
  geom_area(alpha = 0.7) + 
  ylim(0, max(test$Count, na.rm = TRUE) * 1.5) +
  scale_x_continuous(breaks = seq(1880, 2030, 8)) +
  # theme(aspect.ratio = 1) + 
  xlab("Year") +
  ggtitle("Number of Published US Soil Survey Manuscripts by Year")

g2  <- ggplot(test, aes(x = year, y = cumsum(Count))) +
  geom_area(alpha = 0.7) + 
  ylim(0, max(cumsum(test$Count), na.rm = TRUE) * 1.5) +
  scale_x_continuous(breaks = seq(1880, 2030, 8)) +
  # theme(aspect.ratio = 1) +
  xlab("Year") + ylab("Count") +
  ggtitle("Cumulative Number of Published US Soil Survey Manuscripts by Year")

# gridExtra::grid.arrange(g1, g2, ncol = 1)


pedons <- read.csv("https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/pedons.csv", stringsAsFactors = FALSE)
pedons <- filter(pedons, obs_year %in% 1950:2018) %>%
  mutate(year = obs_year,
         lab = FALSE,
         Count = n_peiid
         )

labpedons <- read.csv("https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/labpedons.csv", stringsAsFactors = FALSE)
labpedons <- filter(labpedons, obs_year %in% 1950:2018) %>%
  mutate(year = obs_year,
         lab = TRUE,
         Count = n_peiid
         )


g3 <- ggplot(pedons, aes(x = year, y = Count)) + 
  geom_area(aes(fill = lab), stat = "identity") + 
  geom_area(data = labpedons, aes(x = year, y = Count, fill = lab), stat = "identity") +
  ylim(0, max(pedons$Count, na.rm = TRUE) * 1.2) +
  ylab("Count") + xlab("Observation Year") +
  scale_x_continuous(breaks = seq(1880, 2030, 8)) +
  ggtitle("Number of Pedons per Year") 

g4 <- ggplot(pedons, aes(x = year, y = cumsum(Count))) + 
  geom_area(aes(fill = lab), stat = "identity") +
  ylim(0, max(cumsum(pedons$Count), na.rm = TRUE) * 1.2) +
  ylab("Count") + xlab("Observation Year") +
  scale_x_continuous(breaks = seq(1880, 2030, 8)) +
  ggtitle("Cumulative Number of Pedons per Year")

gridExtra::grid.arrange(g1, g3, ncol = 1)

``` 

![](figure/sites_nat.png)

## Some Issues With Pedon Data

- Making and documenting observations of soil requires hard work. Digging is difficult, and writing soil descriptions is time consuming!
- Our confidence in observations commonly weakens with the depth of the material described.
    
  - If we acknowledge this, which we must, then how do we deal with it in pedon data?
      - Use a cutoff depth, for example 100 cm, can be used to truncate observations to a zone of greater confidence.
      - Show the relative confidence of the data with depth.
    

<!-- insert section labels for references via URL -->
<a id="data-types-and-classes"></a>

# R Basics - Data Types and Classes

The following examples are meant to be copied from this document and pasted into the **R** console, where they can be run interactively. Comments are specified with a '#' prefix and briefly describe the purpose of any following code. Further documentation on objects and functions discussed in this chapter can be accessed by typing `help(soilDB)` or `help(aqp)` at the **R** console. The general form for a help request is `?function_name`. 



## Learning the R Language

This chapter relies heavily on a basic understanding of the **R** language and syntax. There are many excellent manuals, tutorials, and example-based learning tools available. We recommend some of the following.

 * [**R Spatial, Chapter 1: Introduction**](http://www.rspatial.org/intr/index.html)
 * [**Quick R website**](http://www.statmethods.net/)
 * [**Advanced R**](http://adv-r.had.co.nz/)


## Classes of Objects Used in R
 
One of the most versatile things about R is that it can manipulate and work with data in many ways.  Below are examples of ways to create and reference information in a common data type called a dataframe. Within the R session, *objects* contain information that is loaded from files, extracted from NASIS, created on the fly, or calculated by some function. If none of the base classes are sufficient for a task, it is possible to define custom classes. The `SoilProfileCollection` is an example of a custom class which is tailored to the structure and organization of soil pedon and horizon data.

For a more in-depth review of data types in R, please review the [**Chapter 2a Appendix - Data Types**](http://ncss-tech.github.io/stats_for_soil_survey/chapters/2_data/2a_appendix_data_types.html), or one of the websites suggested above.


### Dataframes

Dataframes are central to most work in R. They describe rectangular data, which can be thought of as a spreadsheet with rows and columns. Each column in the dataframe is constrained to a single data type: numeric, date-time, character, Boolean, and so on. Note that each column of a dataframe is a vector, and all column vectors must be the same length (hence the adjective "rectangular").

```{r datatype2, eval=TRUE}
  # Using the concatenate function we can create the following character and logical vectors
  # character vector: taxonomic subgroup
  subgroup <- c("typic haplocryepts","andic haplocryepts","typic dystrocryepts")  
  subgroup
  # logical vector: boolean field for presence or absence of andic soil properties diagnostic feature
  andic <- c(FALSE,TRUE,FALSE) 
  andic
  
  # Take our two character and logical vectors and convert them into a more useful dataframe.
  # we'll use the data.frame() function to glue these two vectors together into object 'd'
  d <- data.frame(subgroup, andic)
  d
``` 
You can see that the dataframe was created and it worked, but the vector names are not very informative. A couple of useful functions for working with column names are `names()`, which renames columns in a dataframe and `colnames()`, which creates a vector of column names.

```{r datatype2.1, eval=TRUE}  
  # get the column names of a dataframe
  names(d)
  # we can use 'names()' and 'c()' to rename the columns in a dataframe
  names(d) <- c('tax_subgroup', 'andic.soil.properties')
  d
```

#### Referencing within dataframes
Note in dataframe `d` that each row has an index number in front of it.  Using the square brackets notation, you can reference any part of a dataframe: rows or columns or specific row- and column-selections.  Here are some examples:
  
```{r datatype2a, eval=TRUE}
  # format: dataframe_name[rows, columns]
  d[1, ] # first row of dataframe
  d[, 1] # first column of dataframe
  d[2, 2] # second row, second column
  
  # In dataframes we can also use the '$' symbol to reference vector columns within a specific dataframe object
  d$tax_subgroup
  
  # Other useful functions for checking objects and working with dataframes
  # the structure 'str()' function will show you the structure of an object and the data types of the vectors within it
  str(d)
  # 'class()' will tell you the object type or data type
  class(d)
  # use 'colnames()' to get a vector of column names from a dataframe
  colnames(d)
  # ncol and nrow give dimensions
  ncol(d)
  nrow(d)
  
  # building on what we've learned above, we can use the square bracket notation on a dataframe to re-order columns
  d <- d[ ,c('andic.soil.properties', 'tax_subgroup')]
  d
  # another way we could do this is to use the column indexes within the concatenate function
  d <- d[ , c(2,1)]
``` 

How would you remove a vector or column from a dataframe?

`d$tax_subgroup <- NULL` will remove this column from the dataframe.



# Using the soilDB Package

## How does it work and what does it do?

The soilDB package for R works with soil-resource-related data sources.  It has a series of convenience functions for accessing data in NASIS, KSSL, SDA, and other sources. The `fetchNASIS` convenience function extracts data from a NASIS selected set via Structured Query Language (SQL). Basic data checks are run within the fetch functions, then the data are assembled into a combined site-level and horizon-level data structure within a custom R object called a `Soil Profile Collection (SPC)`.  The [`SoilProfileCollection`](http://ncss-tech.github.io/AQP/aqp/aqp-intro.html) class simplifies the process of working with collections of data associated with soil profiles, e.g., site-level data, horizon-level data, spatial data, diagnostic horizon data, metadata, etc.. The SPC object inerits many of the familiar methods that operate on vectors and `data.frame` objects, such as `nrow()` (*"how many horizons"*) or `length()` (*"how many pedons"*).

Note that the import process in `fetchNASIS()` is not comprehensive. It does not pull all of the data for every table related to pedon data out of NASIS. Instead, it pulls much of the most commonly used pedon and horizon data.  In addition, much of the nested complexity of the NASIS data structure is simplified in the resulting SPC object.  Higher level functions like `fetchNASIS()` bundle a series of lower level functions that get specific parts of the data structure.

```{r structure_diagram_a, echo=FALSE, results='hide', warning=FALSE}
library(diagram, quietly=TRUE)
# reset figure margins
par(mar = c(1, 1, 1, 1))

# simple diagram of the pedon data structure
names <- c("Site", "Siteobs", "Pedon", "Horizon")
M <- matrix(nrow = 4, ncol = 4, byrow = TRUE, data = 0)
M[4, 3] <- M[3, 2] <- M[2, 1] <- ""
pos <- cbind (c(1, 1, 1, 1))
plotmat(M, pos = pos, name = names, lwd = 1, box.lwd = 2, cex.txt = 0.8, box.size = 0.1, box.type = "square", box.prop = 0.4, mx=-0.2)

# parallel simplified SPC structure
names <- c("Site-level", "Horizon-level")
M <- matrix(nrow = 2, ncol = 2, byrow = TRUE, data = 0)
 M[2, 1] <- ""
#pos <- cbind (c(2, 2))
plotmat(M, pos = c(1, 1), name = names, lwd = 1, box.lwd = 2, cex.txt = 0.8, box.size = 0.14, box.type = "square", box.prop = 0.75, mx=0.3, my=-0.1, add=TRUE)

# add arrows to the diagram
arrows(0.42, 0.1, x1=0.65, y1=0.1, length = 0.25, code=2, lwd=2, angle = 15)
arrows(0.42, 0.35, x1=0.65, y1=0.54, length = 0.25, code=2, lwd=2, angle = 15)
arrows(0.42, 0.61, x1=0.65, y1=0.61, length = 0.25, code=2, lwd=2, angle = 15)
arrows(0.42, 0.87, x1=0.65, y1=0.68, length = 0.25, code=2, lwd=2, angle = 15)
```

One-to-many relationships are flattened where possible by `fetchNASIS()`. This flattening aggregates the data into one site record with related horizon records.  Selected additional data elements that may have a one-to-many relationship to a site or pedon can be gathered from a NASIS selected set via the  `get_extended_data_from_NASIS_db()` function and then joined to the site-level data.  More on this process in later.....

In short, the soilDB package greatly simplifies the process of getting pedon data from NASIS into R for further analysis.

 
## Set Up an Open Database Connectivity (ODBC) Connection to NASIS

After setting up an ODBC connection, you can use R to access data from a selected set defined in your local NASIS database. See this job aid: [**How to Create an ODBC Connection and Setup SoilDB for Use with R**](http://ncss-tech.github.io/AQP/soilDB/setup_local_nasis.html).

Query and load some pedon data into your NASIS selected set.
 
Does NASIS need to be open and running to query data using soilDB?

No, `fetchNASIS()` works whether the NASIS application is running or not. You just need to make sure that the data you want has been loaded into your selected set.


## Prepare Example Data
Take a moment to open the NASIS client and create a selected set with some site/pedon objects that will be used in the following sections. Using a query that includes both site and pedon objects, download to your local database and selected set sites with *MT647* in the user site ID. Depending on the query, you will need to include wildcard characters like this: `%MT647%`.


<!-- insert section labels for references via URL -->
<a id="fetchNASIS-pedon-data-checks"></a>

### Data Checks Run by the `fetchNASIS()` Function

When you load pedons using the fetchNASIS() function, the following data checks are performed:

- **Presence of multiple map datums**. Results reported to the user and the data are not modified.

- **Inconsistent horizon boundaries**. Pedons with inconsistent horizon boundaries are not loaded.  In most cases, this occurs when the bottom depth of a horizon is not the same as the upper depth of the next lower horizon.

```{r example_a, echo=FALSE, results='show', warning=FALSE}
top <- c(0,38,56,121,135)
bot <- c(30,56,121,135,'')
hzname <- c('A', 'Bt1', 'Bt2', 'Bk', 'R')
d <- data.frame(hzname, top, bot)
d
```
Note the issue above. The bottom depth of the A horizon and the upper depth of the Bt1 horizon should be the same: either 30 or 38 cm. The correct depth needs to be determined.

- **Missing lower horizon depths.** Offending horizons are fixed by replacing the missing bottom depth with the top depth plus 2 cm. In the case of the profile shown above, a bottom depth of 137 cm would be inserted where the depth is missing.

- **Sites missing pedon records**. Data without corresponding horizons are not loaded.


#### How can you find the site ID's where these errors occur and fix them in NASIS?
If errors in the pedon data are detected when loading data using fetchNASIS(), the following "get" functions can trace them back to the corresponding records in NASIS:

- **get('sites.missing.pedons', envir=soilDB.env)**
    - Returns user site ID's for sites missing pedons.
  
- **get('dup.pedon.ids', envir=soilDB.env)**
    - Returns pedon ID's for sites with duplicate pedon ID's.
  
- **get('bad.pedon.ids', envir=soilDB.env)**
    - Returns user pedon ID's for pedons with inconsistent horizon depths.
    
- **get('bad.horizons', envir=soilDB.env)**
    - Returns a dataframe of horizon-level information for pedons with inconsistent horizon depths.

For more information on the design of soilDB functions, see the following documentation: [**Introduction to soilDB**](http://ncss-tech.github.io/AQP/soilDB/soilDB-Intro.html).


### fetchNASIS() arguments

fetchNASIS() is a versatile function have has many arguments (i.e. options) that can be chosen.

- **from = 'pedons' or 'components' or 'pedon_report'** 
    - This option allows you to select which data you want to load from NASIS. Choosing either 'pedons' or 'components' will load data from your local database. If 'pedon_report' is specified then it will load data from the text file generated by the NASIS report 'fetchNASIS', which is handy for loading more than 5000 pedons at one time, such as for an entire MLRA Soil Survey Office. 
    
- **url =** 'https://nasis.sc.egov.usda.gov/OfflineReports/fetchNASIS_04e6ec7d-fab5-4a90-bb88-9b9dc56dfdd8.txt'
    - If from = 'pedon_report' this option will load data from the URL that is generated when the NASIS report 'fetchNASIS' is run offline against the national database. This is particularly useful for loading more than 20,000 pedons at one time, such for an entire Soil Survey Region.
    
- **SS = TRUE/FALSE** 
    - The Selected Set (SS) option allows you to choose whether you want the data to load from your current selected set in NASIS or from the local database tables.  The default is set to **TRUE** so if unspecified `fetchNASIS()` will always load from the data in the selected set. 
    
- **stringAsFactors = TRUE/FALSE** 
    - This option allows you to select whether to convert strings into factors or not.  The default is set to **FALSE**, which will handle strings as character formats. Manually set this option to **TRUE** if you wish to handle character strings as factors.   

- **rmHzErrors = TRUE/FALSE** 
    - Setting this value to **TRUE** (the default) enables checks for horizon depth consistency. Consider setting this argument to **FALSE** if you aren't concerned about horizon-depth errors or if you know that your selected set contains many combination horizons (e.g., consisting of E/Bt horizons or similar two-part horizons described individually for the same depth range). Note that any pedons flagged as having horizon-depth errors (rmHzErrors = TRUE) are omitted from the data returned by `fetchNASIS()`.
    
- **nullFragsAreZero = TRUE/FALSE**
    - Setting this value to **TRUE** (the default) converts null entries for rock fragment volumes to 0. This is typically the right assumption because rock fragment data are typically populated only when observed. If you know that your data contain a combination of omitted information (e.g. no rock fragment volumes are populated) then consider setting this argument to **FALSE.**
    
- **soilColorState = 'moist' or 'dry'** 
    - This option allows you to select whether to treat strings as factors or not.  The default is set to **FALSE**, which will not treat strings as factors.
    
- **lab = TRUE/FALSE** 
    - This option allows for loading the data associated with horizons that may be in the phlabresults table.  The default is set to **FALSE**, which will not load records from the phlabresults table.
  
For more information on the data checks and adjusting the default options to `fetchNASIS()` function, see the following resource: [**Tips on Getting Data from NASIS into R**](http://ncss-tech.github.io/AQP/soilDB/fetchNASIS-mini-tutorial.html).

<!-- insert section labels for references via URL -->
<a id="spc-object-structure"></a>

## Internal Structure of an `SoilProfileCollection` (SPC) Object

### The `Gopheridge` Sample Dataset
The `gopheridge` sample dataset is very similar to the type of data returned from `fetchNASIS()`. The following demonstration shows the structure of the Soil Profile Collection (SPC) object that is returned by `fetchNASIS()`. 

Before proceeding, you may find it helpful to review the following: [**SoilProfileCollection Object Introduction**](http://ncss-tech.github.io/AQP/aqp/aqp-intro.html). This tutorial provides an excellent overview of how the SPC object is constructed. Also, the manual pages for `soilDB` and `aqp` are accessible (click **index** at the bottom of the **Help** tab in RStudio) by entering the following into the R console: 

```{r, eval=FALSE}
# not run
library(soilDB)
help(soilDB)

# for links to lots of great examples look here!
library(aqp)
help(aqp)
```


Open RStudio, and set up the environment by loading packages and the Gopheridge sample dataset.


```{r gopheridge_a}
options(width=95, stringsAsFactors=FALSE)
library(soilDB)
library(aqp)

# load example dataset
data(gopheridge)

# what kind of object is this?
class(gopheridge)

# what does the internal structure look like?
str(gopheridge, 2)

# let's take a look at the fields at the site and horizon levels within the SPC
siteNames(gopheridge)
horizonNames(gopheridge)
```

#### Quickly generate sketches from a `SoilProfileCollection` object
The `plot()` function applied to a `SoilProfileCollection` object generates sketches based on horizon depths, designations, and colors. The `fetchNASIS()` function automatically converts moist Munsell colors into R-style colors. Multiple colors per horizon are mixed. See `?plotSPC` for a detailed list of arguments and examples.

```{r, fig.width=10, fig.height=4}
par(mar=c(1,1,1,1))
# ommiting pedon IDs and horizon designations
plot(gopheridge, print.id=FALSE, name='')
title('Pedons from the `gopheridge` sample dataset', line=-0.5)
```

Additional documentation and examples can be found in:
   
   * [http://ncss-tech.github.io/AQP/aqp/aqp-intro.html](http://ncss-tech.github.io/AQP/aqp/aqp-intro.html)
   * [http://ncss-tech.github.io/AQP/aqp/merged-legend-plot.html](http://ncss-tech.github.io/AQP/aqp/merged-legend-plot.html)
   * [http://ncss-tech.github.io/AQP/aqp/profile-summary.html](http://ncss-tech.github.io/AQP/aqp/profile-summary.html)
   * [http://ncss-tech.github.io/AQP/aqp/SPC-plotting-ideas.html](http://ncss-tech.github.io/AQP/aqp/SPC-plotting-ideas.html)


#### Subset example of the data in the site-level portion of the SPC

```{r gopheridge_a1, eval=TRUE, echo=FALSE, results='show', warning=FALSE, collapse=TRUE}
s <- site(gopheridge)
# show table of site data
knitr::kable(s[1:2, 1:10])
knitr::kable(s[1:2, 11:20])
knitr::kable(s[1:2, 21:28])
knitr::kable(s[1:2, 28:36])

# use the following to show the data in the R console
#head(site(gopheridge), 2) # show the first 2 lines of the site data
```

#### Subset example of the data in the horizon-level portion of the SPC

```{r gopheridge_a2, eval=TRUE, echo=FALSE, results='show', warning=FALSE, collapse=TRUE}
h <- horizons(gopheridge)
# show table of site data
knitr::kable(h[1:8, 1:10])
knitr::kable(h[1:8, 11:19])
#knitr::kable(h[1:2, 21:28])
#knitr::kable(h[1:2, 28:36])

# use the following to show the data in the R console
#head(horizons(gopheridge), 5) # show the first 5 rows of the horizon data
```

<!-- insert section labels for references via URL -->
<a id="spc-object-your-turn"></a>

#### Follow along with your own data

Explore the site- and horizon-level data in your own SPC using the following code. Note: You must have pedons in your local NASIS selected set.

```{r owndata_a, results='hide'}
# load required libraries
library(soilDB)
library(aqp)

# load data from a NASIS selected set
f <- fetchNASIS(from = 'pedons')

# what kind of object is this?
class(f)

# how many pedons
length(f)

# let's take a look at the fields at the site and horizon levels within the SPC
siteNames(f)
horizonNames(f)

# look at the first 2 rows of site and horizon data
head(site(f), 2)
head(horizons(f), 2)
```

How can you find out how many site and horizon records are in the data you just loaded?

<!-- insert section labels for references via URL -->
<a id="pedon-locations"></a>

## Viewing Pedon Locations
### Plotting Geographic Data Directly in R

**Quick check:** Does the data plot roughly where you expect it?
 
Plotting the data directly as an R graphic can give you some idea of how the data look spatially and whether their distribution approximates what you expect.  Typos are relatively common when coordinates are manually entered. Viewing the data spatially is a quick way to see if any points plot far outside of the geographic area of interest and therefore clearly have an error.


```{r gopheridge_b}
# plot the locations of the gopheridge pedons within R
# Steps:
# 1) subset to a new data frame
# 2) create a spatial points data frame (SPDF)
# 3) plot the data

# load libraries
library(sp)
library(mapview)

# subset standard WGS84 decimal degree coordinates from the gopheridge SPC by specifying column names
gopher.locations <- site(gopheridge)[, c('site_id', 'x_std', 'y_std')]

# initialize coordinates in an SPDF
coordinates(gopher.locations) <- ~ x_std + y_std
# define coordinate system
proj4string(gopher.locations) <- '+proj=longlat +datum=WGS84'

# creat interactive map
mapview(gopher.locations)
```

### Displaying Pedon Data in Google Earth
Google Earth is a powerful viewer for point data.  Geographic data is displayed in Google Earth using the Keyhole Markup Language (KML) format.  Using the [plotKML](https://cran.r-project.org/web/packages/plotKML/plotKML.pdf) package, you can easily create a KML file to inspect and view in Google Earth.  See the related material in this tutorial: [**Export Pedons to Google Earth**](http://ncss-tech.github.io/AQP/soilDB/export-points-from-NASIS-to-Google-Earth.html). 

### Exporting Pedon Data to an ESRI Shapefile
Another way you can view the data is to export a shapefile from R.  For further information, see this tutorial: [**Export Pedons to Shapefile**](http://ncss-tech.github.io/AQP/soilDB/export-points-from-NASIS.html).

#### Follow along with your own data

Use the script below to make an R plot of pedon data loaded from your NASIS selected set.

The following script plots the standard lat/long fields from NASIS. In some cases, these fields might be incomplete due to insufficient data or to not having been calculated from UTM coordinates in NASIS. In these cases, you can omit sites with "NA" values in the coordinates a couple of ways. The `na.omit()` or `complete.cases()` functions remove any rows in a dataframe that have "NA" values.

Run the following script on the data loaded from your local NASIS selected set:
```{r r_plot_pedons, eval=FALSE, echo=TRUE, results='show', warning=FALSE}
# load libraries
library(soilDB)
library(sp)
library(mapvew)

# get pedons from the selected set
f <- fetchNASIS(from = 'pedons')

# subset standard WGS84 decimal degree coordinates from the gopheridge SPC by specifying column names
f.locations <- site(f)[, c('site_id', 'x_std', 'y_std')]
nrow(f.locations)

# remove any sites lacking standard lat/long coordinates
# notice that there may now be fewer rows of data
f.locations <- na.omit(f.locations)
nrow(f.locations)

# initialize coordinates in an SPDF
coordinates(f.locations) <- ~ x_std + y_std
# define coordinate system
proj4string(f.locations) <- '+proj=longlat +datum=WGS84'

# plot
mapview(f.locations)
```


<!-- insert section labels for references via URL -->
<a id="NASIS-spc-object-examples"></a>

# Working with SPC Data in R

## Summarizing Data

Now that you've loaded some data, you can look at additional ways to summarize data elements and filter the SPC to specific sites of interest. The `table()` function is very useful for quick summary operations.  This function can be combined with other functions, such as `sort()` and `is.na()` or `!is.na()` (is not NA). Follow along with your own data.


```{r owndata_b, results='hide'}
# summarize which soil taxa we have loaded
table(f$taxonname)
# sort results in descending order
sort(table(f$taxonname), decreasing=TRUE)

# could do the same thing for taxonomic subgroups or any column of the SPC at the site or horizon levels
table(f$taxsubgrp)
sort(table(f$taxsubgrp), decreasing=TRUE)

# table() is also useful when testing for null data using IS NA, is.na() or IS NOT NA, !is.na()
table(is.na(f$taxsubgrp))
table(!is.na(f$taxsubgrp))

# it can also be applied to horizon level columns in the SPC
sort(table(f$texture), decreasing=TRUE)
```


## Filtering

A variety of methods are available to subset a collection of soil profiles or an SPC. The results can then be placed into another SPC. This capacity can be useful for generating subset SPC objects from the original dataset.

### Logical Operators

  - `%in%` Equivalent to `IN ()` in SQL. Can use `c()` to concatenate lists of vectors.
    - Example: `f$taxpartsize %in% c('loamy-skeletal', 'sandy-skeletal')`
  - `!=` Not-equal-to character "string."
  - `==` Note in the example above that R uses a double equal sign as "equal to."
  - `<, >, <=, >=` Less than, greater than, less than or equal to, and greater than or equal to.


### Pattern Matching

The following examples use the `grep()` function to pattern match within the data, create an index of the SPC for records that match the specified pattern within that column, and then use that index to filter to specific sites and their corresponding profiles. Patterns are specified in [regular expression](http://regexr.com/) (REGEX) syntax.

This process can be applied to many different columns in the SPC based on how you need to filter the data.  This example pattern matches on the `tax_subgroup` column, but another useful application might be to pattern match on geomorphology or parent material.

Note that the `grep()` below also has an ***invert option***, which is specified as either true or false (the default is false).  When set to true, this option is very useful for excluding the results of the pattern matching process by inverting the selection.

```{r owndata_d, results='hide'}
# say we wanted to look at what the variation of particle size classes are within a specific subgroup?
# use of grep() to filter and create an index, then apply that index to the SPC 
# and create a new SPC called 'f1' using the square bracket notation
idx <- grep('lithic', f$taxsubgrp, invert=FALSE)
# save this subset of 'lithic' soils for later use  
f1 <- f[idx, ]
# or use the index directly to summarize a field
sort(table(f$taxpartsize[idx]), decreasing=TRUE)
```

Do a quick graphical check to ensure that the "lithic" profiles are selected. Plot them in R using the `SoilProfileCollection` "plot" method (e.g., specialized version of the generic `plot()` function).

```{r owndata_e, results='show', fig.width=8, fig.height=4}
# adjust margins
par(mar=c(1,0,0,1))
# plot the first 10 profiles of the 'f1' subset
# limit plotting to a depth of about 60cm
plot(f1[1:10, ], label='site_id', max.depth=60)
title('Pedons with the word "lithic" at subgroup-level of Soil Taxonomy', line=-2)
```

For more information on using regular expressions in `grep()` for pattern matching operations, see: [Regular-expression-syntax](https://www.gnu.org/software/findutils/manual/html_node/find_html/grep-regular-expression-syntax.html).

#### Additional syntax options for REGEX pattern matching

  - `|` Equivalent to "or" in SQL.
    - Example:  `grep('loamy | sandy', f$taxpartsize)`
  - `^` Anchors to the left side of the string.
    - Example:  `grep('^sandy', f$taxpartsize)`.
  - `$` Anchors to the right side of the string.
    - Example:  `grep('skeletal$', f$taxpartsize)`.


### Filtering Data by Specifying a Criteria Using the `which()` Function

Another method of subsetting a collection of soil profiles is to specify a criteria using the `which()` function. The following examples use the `which()` and `grep()` functions to reference the indexing of the SPC to create subsets and to filter for specific sites or their corresponding profiles.

```{r owndata_f, results='show', fig.width=8, fig.height=4}
# say we wanted to look at what the variation of particle size classes are within a specific subgroup?
# first: use grep to pattern match the tax_subgroup field for the string 'aqu'
idx <- grep('lithic', f$taxsubgrp)
# save this subset
f1 <- f[idx, ]
# check taxonomic range of particle size classes in the data
sort(table(f1$taxsubgrp), decreasing=TRUE)
sort(table(f1$taxpartsize), decreasing=TRUE)

# then further query the subset for only those profiles with particle size class of 'sandy-skeletal'
# notice: a double equal sign '==' is used for exact character or numeric criteria
idx <- which(f1$taxpartsize == 'sandy-skeletal')
# save this subset
f2 <- f1[idx, ]
table(f2$taxpartsize)
# plot  profiles 1 thru 10
par(mar=c(0,0,2,1))
plot(f2[1:10, ], label='site_id')
title('Sandy-skeletal particle size control section class')
```



### Extracting Site and Horizon Data

Soil Profile Collections are designed to be dismantled so they can work more easily with either site or horizon data. The SPC has a slot for site-level data and a slot for horizon-level data. You can reference these slots using the `site()` and `horizons()` functions within the AQP package. These "get" functions extract all the site or horizon variables as a dataframe for further use.  

```{r owndata_g, results='show'}
# extract site data from SPC into dataframe 's'
s <- site(f)
names(s)
# extract horizon data from SPC into dataframe 'h'
h <- horizons(f)
names(h)
```
You can also use these functions when referencing the data within an SPC to specify that you want to look specifically in the site or horizon data. 


<!-- insert section labels for references via URL -->
<a id="fetchNASIS-data-checks-your-turn"></a>

### Review of Data Checks Run by `fetchNASIS()`

Now that you've loaded some data and learned a little about how to filter data in the SPC, you can quickly review some of the `get()` functions used to track data issues detected in the process of loading data back to the NASIS records in your selected set.

```{r owndata_a1, eval=FALSE, results='hide'}
# use each one of these to return a vector of the pedons where errors were detected
#get('sites.missing.pedons', envir=soilDB.env)
#get('dup.pedon.ids', envir=soilDB.env)
#get('bad.pedon.ids', envir=soilDB.env)
# example of pedon_id's returned
#[1] "2011MT0810001" "2011MT0810009" "2011MT0810015" "2011MT0810027" "2011MT0810034"

#get('bad.horizons', envir=soilDB.env)

# How could you then remove these from your SPC?
# since the get() returns the string of bad pedon id's we can use a which() to query any pedon id's that don't match the bad id's
idx <- which(horizons(f)$pedon_id != get('bad.pedon.ids', envir=soilDB.env))
f <- f[idx, ]
```
Another useful function is `dput()`, which concatenates a variable. It converts something like this:

"2011MT0810001" "2011MT0810009" "2011MT0810015" "2011MT0810027" "2011MT0810034"

into a comma delimited string like this:

c("2011MT0810001", "2011MT0810009", "2011MT0810015", "2011MT0810027", "2011MT0810034").

Such a string can be copied and then pasted back as a concatenated string or could even be used as string for NASIS list queries. The `dput()` function is also helpful when sending questions or examples to colleagues via email.


<!-- insert section labels for references via URL -->
<a id="writing-functions"></a>

## Functions
If objects are analogous to nouns in a spoken language, then functions are analogous to verbs. A function defines an action that (typically) results in the creation of an object. An object that a function "acts on" is referred to as an "argument" of that function. Follow this terminology with a very basic example:

```{r }
# make a new object with a sequence of values from 1 to 10
a <- seq(from=1, to=10, by=1)
# result
#[1]  1  2  3  4  5  6  7  8  9 10

# define a function that performs a simple action on a vector of numbers
# "i" is a temporary object created inside of the context of this function based on the argument supplied
aFunction <- function(i) {
  # do something
  res <- i * 10
  # send the results back to the calling context
  return(res)
}

# apply our new function to object "a"
aFunction(a)
```


### Defining a Function

Functions bundle operations and can come in the form of small helper functions or involve larger multi-step processes that can be re-used.  This section focuses on functions that work with the horizon data in the SPC.  The general workflow typically goes like this:

- The need for a function is realized,

- steps for a function are defined,

- the function is applied iteratively to each soil profile,

- results are returned, and

- results may, in many cases, need to be further summarized before joining back to the original SPC.


### Function Examples

Functions can bundle a series of operations and then be applied to each profile within a collection (SPC) using `profileApply()`.  Consider, for example, if you wanted to use some pedon data to model  depth to the top of an argillic horizon.  Ideally, this task would be performed using data from the pedon diagnostic features table. However, these records are not always consistently populated. So, how else might you accomplish this task?  One way to try would be using horizon designations to derive a depth to the "t" suffix horizon designation.

What steps would return an upper depth to an argillic horizon for each site?

 - *Extract* the horizon data for each profile,
 - *iterate* through the horizon designations (hzname) and search for the pattern "t",
 - *apply* the function to each profile via `profileApply()`,
 - *summarize* the data returned by the function to one value per profile, and
 - *join* the summarized depth value back to the site data.
 
 
#### Finding the top of the first "Bt" horizon

```{r owndata_i_gopher, echo=TRUE, results='show', warning=FALSE}
# load required libraries
library(aqp)
library(soilDB)
library(plyr)

# load example dataset
data(gopheridge)

# rename gopheridge as SPC object 'f'
f <- gopheridge

# the argument 'i' is a single soil profile
findBtHorizons <- function(i) {
  # extract horizons for current profile
  h <- horizons(i) 
  # search for pattern 't' in horizon designations
  idx <- grep('t', h$hzname)
  # subset these horizons
  h2 <- h[idx, ] 
  # subset columns in resulting dataframe
  res <- h2[, c('peiid', 'phiid', 'hzname', 'hzdept', 'hzdepb', 'clay', 'phfield')]
  # return data
  return(res)
}

# apply function to a single profile as a demonstration
findBtHorizons(f[1, ])
```

You still need to summarize to get the upper depth from multiple "Bt" horizons per profile. Also, note that additional variables were returned by the function.

Next, apply the function to all profiles in your collection. The results are a list of dataframes.
```{r}
l <- profileApply(f, FUN=findBtHorizons, simplify=FALSE)

# convert list into a dataframe, dropping all pedons with no 't' horizons 
Bt.horizons <- ldply(l)
```


You still need to reduce this to one depth value per profile. The `ddply()` function conveniently iterates over groups of rows in a dataframe and computes summaries (similar to `GROUP BY` in SQL). The group-wise summaries are re-combined into a new dataframe along with ID's for each group.
```{r fig.width=4.5, fig.height=3.5}
# standard ddply syntax is as follows (type '?ddply' into the R console):
# ddply(.data, .variables, .fun)
Bt.horizons.top <- ddply(Bt.horizons, 'peiid', summarise, depth_to_Bt_cm=min(hzdept))

# since we have peiid in the 'Bt.horizons.top' dataframe we can easy join it back to site data in the SPC
# NOTE: when used in conjunction with site(), the assignment operator will perform a left-join
site(f) <- Bt.horizons.top

# summary of depth to argillic in the data using a histogram
# reset figure margins
par(mar=c(4.5,4.5,1,1))
hist(f$depth_to_Bt_cm, xlab='Depth to Bt Horizon (cm)', main='')
```


Generate sketches for the first 15 profiles and annotate with the results of our analysis.
```{r fig.width=10, fig.height=5}
# index to the first 15 profiles
idx <- 1:15

# reset figure margins
par(mar=c(0,0.5,3,1))

# plot indexed profiles, omitting IDs
plot(f[idx, ], print.id=FALSE)

# add the top depth of the first "Bt" horizon
points(x=1:15, y=f$depth_to_Bt_cm[idx], pch=21, bg='black', col='white')

# title / subtitle
title(main = "Select pedons from the 'gopheridge' sample dataset", line=-0.5)
title(sub= "Depth to first 'Bt' horizon identified", line=-2)
```
 

*What is a potential problem with this operation, and what was not accounted for?*


#### Thickness of organic horizons 

```{r owndata_i1_gopher, eval=FALSE, echo=TRUE, results='show', warning=FALSE}
# This time we'll go after the thickness of the organic horizons where present.

# load library
library(plyr)

f.organic <- function(i) {
  # extract horizons
  h <- horizons(i)
  # pattern match for 'O' horizon designations in horizon data
  idx <- grep('O', h$hzname)
  h2 <- h[idx, ] 
  # subset results
  res <- h2[, c('peiid', 'phiid', 'hzname', 'hzdept', 'hzdepb')]
  # return data
  return(res)
}

# apply function to each profile, results are a list of data.frames
l <- profileApply(f, FUN=f.organic, simplify=FALSE)

# convert list into a dataframe
organic <- ldply(l)

# show contents of the 'organic' dataframe
head(organic)

# summarize this dataframe down to one max bottom depth value for each profile
organic1 <- ddply(organic, 'peiid', summarise, organic_thickness_cm=max(hzdepb))

# since we have peiid in the 'organic1' dataframe we can join back to site data in the SPC
site(f) <- organic1

# summary of organic thickness in the data
# reset figure margins
par(mar=c(4.5,4.5,1,1))
hist(f$organic_thickness_cm, xlab='Thickness of Organic horizons (cm)', main='')
```

How would you subtract the `organic_thickness_cm values` from `depth_to_Bt_cm` values to get a more realistic upper depth to Bt (e.g. possibly argillic) horizons?


#### Follow along with your own data

The following example uses pedon data to extract the depth to calcium carbonate.  

What steps would be needed to return an upper depth to carbonates for each site?

 - *Extract* the horizon data for each profile,
 - *iterate* through the horizon designations (hzname) pattern matching for "k",
 - *apply* the function to each profile via `profileApply()`,
 - *summarize* the data returned by the function to one value per profile, and
 - *join* the summarized depth value back to the site data.

Note: This example may need modification to evaluate different horizon designations than the ones used here as these may not be present in your currently loaded data.
 
```{r owndata_i, eval=FALSE, echo=TRUE, results='show', warning=FALSE}
# load required libraries
library(plyr)

# the argument 'i' is a single soil profile
f.limy <- function(i) {
  # extract horizons for current profile
  h <- horizons(i) 
  # search for pattern 'k' in horizon designations
  idx <- grep('k', h$hzname)
  # subset these horizons
  h2 <- h[idx, ] 
  # subset columns in resulting dataframe
  res <- h2[, c('peiid', 'phiid', 'hzname', 'hzdept', 'hzdepb', 'phfield', 'effclass')]
  # return data
  return(res)
}

# apply function to each profile, results are a list of dataframes
l <- profileApply(f, FUN=f.limy, simplify=FALSE)

# convert list into a dataframe, dropping all pedons with no 'k' horizons 
limy <- ldply(l)

# view the top 6 rows
head(limy)

# still need to reduce this down to one depth value for each profile
## ddply() will apply a function (summarise the min(hzdept)) then combine the results into a data frame.
## standard ddply syntax is as follows (type '??ddply' into the R console):
## ddply(.data, .variables, .fun = NULL....)

limy1 <- ddply(limy, 'peiid', summarise, depth_to_carbonates_cm=min(hzdept))

# since we have peiid in the 'limy1' dataframe we can easy join it back to site data in the SPC
# this won't work if there were no horizons with 'k' suffice
site(f) <- limy1

# summary of depth to carbonates in the data using a histogram
# reset figure margins
par(mar=c(4.5,4.5,1,1))
hist(f$depth_to_carbonates_cm, xlab='Depth to Calcium Carbonates (cm)', main='')
```

If your data contains organic horizons, use the `f.organic` function outlined above to derive values for `organic_thickness_cm`.

You can then subtract the `organic_thickness_cm` values from `depth_to_carbonates_cm` values to get a more realistic upper depth to calcium carbonates.

Consider examples where you truncate the thickness or are interested in summarizing the data for a depth zone, for example, 25 to 100 cm.  What is the weighted average of clay for the 25 to 100 cm thickness?

Simple example: `slab(f, fm= peiid ~ clay, slab.structure=c(25,100), slab.fun=mean, na.rm=TRUE)`.

Excellent examples using the `slice()` and `slab()` functions in the AQP package can be seen at:
[**Introduction to SoilProfileCollection**](http://ncss-tech.github.io/AQP/aqp/aqp-intro.html).


<!-- insert section labels for references via URL -->
<a id="nasis-pedon-extended-data"></a>

## Getting Additional Data: Extended Data Functions

Additional data related to both site and horizon information can be fetched using the `get_extended_data_from_NASIS()` function. This data is not automatically brought into R because these data elements are typically related to the site or horizon data in one-to-many relationships. For example, multiple diagnostic features could exist within one pedon. Below is a summary of additional information that can be readily brought into R from your NASIS selected set via the `get_extended_data_from_NASIS()` function.

```{r owndata_j1, eval=TRUE, echo=TRUE, results='show', warning=FALSE, collapse=TRUE}
# fetch extended site and horizon data
e <- get_extended_data_from_NASIS_db()

### site and pedon related extended data
# vegetation data summary
colnames(e$ecositehistory) 

# diagnostic features
colnames(e$diagnostic) 

# surface rock fragments
colnames(e$surf_frag_summary)

# geomorphic description
colnames(e$geomorph)

# taxonomic history data
colnames(e$taxhistory)

# linked photo stored in site textnotes
colnames(e$photo) 

# site parent materials
colnames(e$pm)

### horizon related extended data
# rock fragments 
colnames(e$frag_summary) 

# soil texture modifers
colnames(e$texmodifier)

# soil structure data
colnames(e$struct) 
```

The "Geomorphic description" and "parent materials" attributes are important soil data. They can be useful handles for exploring other data. The soilDB package flattens the nested table structure of parent material and geomorphic description within NASIS into single strings for each site-level record. The pattern matching concepts demonstrated above select profiles based on parts of these strings. The following code generates a simple graphical summary of the 10 most commonly occurring landforms in `fetchNASIS()` data so you can see their frequency of occurrence. 

```{r owndata_c, results='show'} 
# graphically tabulate the occurrence of landforms
# load required libraries
library(soilDB)
# required for dotchart2()
library(Hmisc)

# load data from a NASIS selected set
f <- fetchNASIS(from = 'pedons')

# create 'lf' object of landform factors sorted in descending order
lf <- sort(table(f$landform_string), decreasing = TRUE)

# plot top 10
dotchart2(lf[1:10], col='black', xlim = c(0, max(lf)), cex.labels = 0.75)
```

### Deriving Thicknesses of Diagnostic Features

#### Boolean diagnostic feature columns in the site data

If diagnostic features are populated in the pedon diagnostic features table in NASIS, then Boolean (`TRUE` or `FALSE`) fields are created for each diagnostic feature in the data brought into R by soilDB. These fields can be readily used to model the presence or absence of a diagnostic soil feature by extracting the site data.

You could use the following code to pull the upper depth to calcium carbonates using the "calcic horizon" and/or the "secondary carbonates" diagnostic features. However, data consistency is critical. You can only use those fields if they are consistently populated for all pedons that you are working with in your selected set. As you start working with larger pedon data sets, you will quickly find that there can be great inconsistencies in the way the data were populated by different people in different offices on different surveys over different time frames.

The following is an example of how you could use the diagnostic features (if populated!) from the extended data to determine the thickness of a diagnostic feature of interest.

```{r owndata_j3, results='show', warning=FALSE, fig.width=6, fig.height=4}
# rename gopheridge data
f <- gopheridge

# get diagnostic features associated with pedons loaded from selected set
d <- diagnostic_hz(f)

# summary of the diagnostic features in your data!
unique(d$featkind)
sort(table(droplevels(factor(d$featkind))), decreasing = TRUE)

# subset argillic horizons
d <- d[d$featkind == 'argillic horizon', ]

# create a new column and subtract the upper from the lower depth
d$argillic_thickness_cm <- d$featdepb - d$featdept

# create another new column with the upper depth to the diagnostic feature
d$depth_to_argillic_cm <- d$featdept

# omit NA values
d <- na.omit(d)

# subset to pedon records IDs and calculated thickness
d <- d[, c('peiid', 'argillic_thickness_cm', 'depth_to_argillic_cm')]
head(d)

# join these data with existing site data
site(f) <- d

# plot as histogram
# reset figure margins
par(mar = c(4.5,4.5,1,1))
hist(f$argillic_thickness_cm, xlab = 'Thickness of argillic diagnostic (cm)', main='')
hist(f$depth_to_argillic_cm, xlab = 'Depth to argillic diagnostic (cm)', main = '')
```



#### Follow along with your own data

```{r owndata_j4, eval=TRUE, echo=TRUE, results='show', warning=FALSE}
# start fresh with your own data
f <- fetchNASIS(from = 'pedons')
# get diagnostic features associated with pedons loaded from selected set
d <- diagnostic_hz(f)
# summary of the diagnostic features in your data!
unique(d$featkind)
# top 5 most frequent
sort(table(d$featkind), decreasing = TRUE)[1:5]

# subset argillic horizons - or choose your own diagnostic feature and modify this script!
#idx <- which(d$diag_kind == 'your_diagnostic')
#d <- d[idx, ]

# how would you do the rest.....see if you can work it out!

```

What can you do with the Boolean diagnostic feature data?

### Diagnostic Feature Diagrams

```{r owndata_k, eval=TRUE, echo=TRUE, results='show', warning=FALSE, fig.height=9, fig.width=7}
## work up diagnostic plot based on gopheridge dataset
library(aqp)
library(soilDB)
library(sharpshootR)

# load data
data(gopheridge)

# can limit which diagnostic features to show by setting 'v' manually
v <- c('ochric.epipedon', 'cambic.horizon', 'argillic.horizon', 'paralithic.contact', 'lithic.contact')

# generate diagnostic property diagram
diagnosticPropertyPlot(gopheridge, v, k=5, grid.label='site_id', dend.label = 'taxonname', sort.vars = FALSE)

# plot again, this time with diagnostic features ordered according to co-occurrence
diagnosticPropertyPlot(gopheridge, v, k=5, grid.label='site_id', dend.label = 'taxonname', sort.vars = TRUE)
```

#### Follow along with your own data

Use the following script to generate a diagnostic-feature diagram for the pedon data you've loaded from your NASIS selected set.
Note: If the data includes more than about 20 pedons, the script might generate figures that are very hard to read. You also need to be certain that pedon diagnostic feature were populated in your data.

```{r owndata_l, eval=FALSE, echo=TRUE, results='hide', warning=FALSE, fig.height=9, fig.width=7}
library(soilDB)
library(sharpshootR)

# load data
f <- fetchNASIS(from = 'pedons')

# may need to subset to a particular series or taxa here....to reduce the number of pedons!

# select a series of diagnostic properties or automatically pull diagnostic feature columns
# get all diagnostic feature columns from site data by pattern matching on '[.]' in the colnames
idx <- grep('[.]', colnames(site(f)))
v <- colnames(site(f))[idx]
v

# or insert diagnostics of interest found in your data here from the list of possible diagnostics in 'v'
v <- c('ochric.epipedon', 'cambic.horizon', 'argillic.horizon', 'paralithic.contact', 'lithic.contact')

# generate diagnostic property diagram
diagnosticPropertyPlot(f, v, k=5, grid.label='site_id', dend.label = 'taxonname')
```

For more information on generating diagnostic feature diagrams, see the following tutorial:
[**Diagnostic Feature Property Plots**](http://ncss-tech.github.io/AQP/sharpshootR/diagnostic-property-plot.html).


## Customized Queries to Local NASIS Database
There are times when it is necessary to send customized queries to the local NASIS database. Queries are written in [T-SQL](https://en.wikipedia.org/wiki/Transact-SQL) which is the dialect of [SQL](https://en.wikipedia.org/wiki/SQL) used to communicate with Microsoft SQL Servers (e.g. the local NASIS database). The `fetchNASIS` and related convenience functions are essentially wrappers around commonly used chunks of SQL.

The following example will return all records from the `sitesoiltemp` table, along with a couple of fields from the `site`, `siteobs`, and `pedon` tables. This is a convenient way to collect all of the field-based soil temperature data associated with the pedons in your selected set for further analysis.

```{r, fig.width=7, fig.height=4}
library(RODBC)

# write query as a long text object
q <- "
-- columns to return
SELECT siteiid as siteiid, peiid, usiteid as site_id, upedonid as pedon_id, obsdate as obs_date,
soitemp, soitempdep

FROM
-- tables that are queried and join conditions
site_View_1 
INNER JOIN siteobs_View_1 ON site_View_1.siteiid = siteobs_View_1.siteiidref
LEFT OUTER JOIN sitesoiltemp_View_1 ON siteobs_View_1.siteobsiid = sitesoiltemp_View_1.siteobsiidref
LEFT OUTER JOIN pedon_View_1 ON siteobs_View_1.siteobsiid = pedon_View_1.siteobsiidref
-- ordering of rows
ORDER BY obs_date, siteiid;"

# setup connection local NASIS
channel <- odbcDriverConnect(connection = getOption("soilDB.NASIS.credentials"))

# exec query
d <- sqlQuery(channel, q, stringsAsFactors=FALSE)

# close connection
odbcClose(channel)

# check results
str(d)

# remove records missing values
d <- na.omit(d)

# tabulate unique soil depths
table(d$soitempdep)

# extract doy of year
d$doy <- as.integer(format(d$obs_date, "%j"))

# when where measurements collected?
hist(d$doy, xlim=c(1,366), breaks=30, las=1, main='Soil Temperature Measurements', xlab='Day of Year')

# soil temperature by day of year
plot(soitemp ~ doy, data=d, type='p', xlim=c(1, 366), ylim=c(-1, 25), xlab='Day of Year', ylab='Soil Temperature at 50cm (deg C)', las=1)
```



## Common Challenges in Working with Pedon Data

 - Consistency
    - Missing data
 - Confidence in the observations
    - Uncertainty with depth
 - Description style differences
    - Depth described, horizonation usage styles
 - Legacy data vintage
    - Decadal span of data
    - Taxonomy updates, horizon nomenclature
 - Location confidence
    - Origin of the location information
    - Datum used for data collection
    - Accuracy for GPS values at the time of data collection
    
### Meeting the Challenges
  
  - Graphical display of the data and summary outputs ([**slice-wise aggregation**](http://ncss-tech.github.io/AQP/aqp/profile-summary.html))
  - Generalized Horizon Labels (GHL). Derive an aggregate soil profile and summarize soil properties for groups of similar soils. 
      - More on that process can be seen in the following tutorial:
      [**GHL Aggregation Presentation**](http://ncss-tech.github.io/AQP/presentations/ghl-aggregation.html) and [**GHL Aggregation Tutorial**](http://ncss-tech.github.io/AQP/aqp/gen-hz-application.html).

For more information regarding difficult pedon data, see the following tutorial in the "aqp" package:  
[**Dealing with Troublesome data**](http://ncss-tech.github.io/AQP/aqp/dealing-with-bad-data.html).



----------------------------
This document is based on `aqp` version `r utils::packageDescription("aqp", field="Version")`, `soilDB` version `r utils::packageDescription("soilDB", field="Version")`, and `sharpshootR` version `r utils::packageDescription("sharpshootR", field="Version")`.

This document can be regenerated by loading sites and pedon from the *MT647* soil survey area.

