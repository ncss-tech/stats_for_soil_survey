---
title: "Spatial Data"
author: "Dylan Beaudette, Stephen Roecker, Jay Skovlin, Skye Wills, Tom D'Avello"
date: "`r Sys.Date()`"
output:
  html_document:
    mathjax: null
    jquery: null
    smart: no
    keep_md: no
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: false
editor_options: 
  chunk_output_type: console
---

```{r setup, purl=FALSE, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# setup
library(knitr, quietly=TRUE)
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.align='center', fig.retina=2, dev='png', tidy=FALSE, verbose=FALSE, antialias='cleartype', cache=FALSE)

# captions added to figures
knit_hooks$set(htmlcap = function(before, options, envir) {
  if(!before) {
    paste('<p class="caption" style="font-size:85%; font-style: italic; font-weight: bold;">',options$htmlcap,"</p><hr>",sep="")
    }
    })
```


<hr>

![](figure/sampling-example.png)

# Introduction

Most of us are familiar with spatial data types, sources, and the jargon used to describe interaction with these data. GIS software provides a convenient framework for most of the spatial analysis that we do, however, the combination of statistical routines, advanced graphics, and data access functionality make *R* an ideal environment for soil science. For example, with a couple of lines of *R* code, it is possible to quickly integrate soil morphology (NASIS), lab data (KSSL), map unit polygons (SSURGO), and climate data (PRISM raster files). 

This chapter is a very brief demonstration of several possible ways to process spatial data in R. A much more thorough description can be found in [*Applied Spatial Data Analysis with R*](http://www.asdar-book.org/) or [*Spatial Data Analysis and Modeling with R*](http://rspatial.org/spatial/). The [*Spatial Cheatsheet*](http://www.maths.lancs.ac.uk/~rowlings/Teaching/UseR2012/cheatsheet.html) is a convenient summary of the most commonly used functions. The [*Drawing beautiful maps programatically*](https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html) tutorial is an excellent primer on making maps in R.


## Objectives
 * Gain experience with creating, editing, and exporting spatial data objects in R.
 * Learn the basics of the `sp` classes and functions.
 * Learn the basics of the `raster` classes and functions.
 * Learn about some interfaces to NCSS spatial data sources.
 * Develop a strategy for navigating the many possible spatial data processing methods.
 * Learn how to integrate multiple data sources to create something new.


# Spatial Data in R

There are several foundational libraries that extend the basic functionality of the R environment to accommodate the special requirements of spatial data:

 * `sp`: vector / raster storage and processing (memory-bound operations)
 * `raster`: raster access and efficient processing of large files (not memory-bound)
 * `rgdal`: interface to GDAL library for data import/export and coordinate system transformations
 
There are [many, many more packages](https://cran.r-project.org/web/views/Spatial.html) available for working with spatial data, however we only have time to cover the above libraries. 
 

The next couple of sections will require loading these libraries into the R session.
```{r}
library(aqp)
library(sp)
library(raster)
library(rgdal)
library(soilDB)
```


## Spatial Data Sources

Conventional spatial data sources:

  * raster data sources (elevation, PRISM, etc.): GeoTiff, ERDAS, BIL, ASCII grid, WMS, ...
  * vector data sources (points/lines/polygons): Shape File, "file" geodatabase, KML, GeoJSON, GML, WFS, ...

Conventional data sources that can be *upgraded* to spatial data:

  * NASIS/LIMS reports: typically site coordinates
  * web pages: [GeoJSON](http://geojson.org/), [WKT](https://en.wikipedia.org/wiki/Well-known_text), or point coordinates
  * Excel file: typically point coordinates
  * CSV files: typically point coordinates

R-based interfaces to NCSS data sources via [`soilDB`](https://cran.r-project.org/web/packages/soilDB/index.html) package:

  * functions that return tabular data which can be *upgraded* to spatial data:
    + [`fetchKSSL()`](http://ncss-tech.github.io/AQP/soilDB/KSSL-demo.html): KSSL "site" data contain x,y coordinates
    + [`fetchNASIS()`](http://ncss-tech.github.io/AQP/soilDB/fetchNASIS-mini-tutorial.html): NASIS "site" data contain x,y, coordinates
    + [`fetchRaCA()`](http://ncss-tech.github.io/AQP/soilDB/RaCA-demo.html): RaCA central pedon x,y coordinates
    
  * functions that return spatial data:
    + [`seriesExtent()`](http://ncss-tech.github.io/AQP/soilDB/series-extent.html): simplified series extent as polygons
    + [`fetchHenry()`](http://ncss-tech.github.io/AQP/soilDB/Henry-demo.html): sensor / weather station locations as points
    + [`SDA_query()`](http://ncss-tech.github.io/AQP/soilDB/SDA-tutorial-2.html): SSURGO data as points, lines, polygons (via SDA)
    + `mapunit_geom_by_ll_bbox()`: SSURGO data as polygons (via WFS)


**Note:** currently, there is no way to read raster data from ESRI "file" geodatabases. Vector data (and associated attribute tables) can be accessed.



## Recent Additions to the R Spatial Data Suite 

We won't be talking much about these packages, but they are worth considering when you start your project.

### A new foundation: `sf`

The [sf package](https://r-spatial.github.io/sf/) represents the latest and greatest in spatial data processing within the comfort of an R session. `sf` combines the functionality of many packages that we will be discussing in this presentation, but relies on more efficient algorithms and a simpler representation of spatial data. 


### Interactive mapping: `mapview` and `leaflet`

These packages make it possible to display interactive maps of spatial objects in RStudio, or within an HTML document generated via R Markdown (e.g. this document).

```{r}
# load required packages
library(mapview)
library(leafem)

# get series extents from SoilWeb
pentz <- seriesExtent('pentz')
redding <- seriesExtent('redding')

# make a simple map
m <- mapView(pentz)
# add more data to the map and display
addFeatures(m, redding, color='black', fillColor='red', weight=1)
```


### FAST raster sampling: `velox`

The `velox` package provides an interesting approach to sampling raster values that intersect with vector features, *as long as the raster data fit into available memory*. For example, sampling all of the raster values that overlap with the estimated extent of the [San Joaquin](https://casoilresource.lawr.ucdavis.edu/see/#san joaquin) soil series (about ~ 458,000 acres) takes about 0.8 seconds.

Note that if you would like to try this example, please download the chapter 2 sample data outlined in Section 4.
```{r fig.width=6, fig.height=5, results='hide'}
library(soilDB)
library(raster)
library(lattice)

# you may have to install this package
library(velox)

# 5-10 seconds for download of SEE data, 
s <- seriesExtent('san joaquin')

# load pointer to PRISM data, note that this file is stored on my local machine:
r <- raster('C:/workspace/chapter-2b/FFD.tif')

# 0.8 seconds for sampling
vx <- velox(r)
system.time(e <- vx$extract(s))

# simple summary
densityplot(e$`SAN JOAQUIN`, plot.points=FALSE, bw=2, lwd=2, col='RoyalBlue', xlab='Frost-Free Days (50% chance)\n800m PRISM Data (1981-2010)', ylab='Density', main='FFD Estimate for Extent of San Joaquin Series')
```


### Related Tutorials and Documentation

   * [`sf` package website](https://r-spatial.github.io/sf/)
   * [`mapview` package website](https://github.com/r-spatial/mapview)
   * [`leaflet` package website](https://rstudio.github.io/leaflet/)
   * [`velox` package website](https://hunzikp.github.io/velox/)
   * [R Advanced Spatial Lessons by Ben Best](http://benbestphd.com/R-adv-spatial-lessons/)
   * [Spatial Data in R by Pierre Roudier](http://pierreroudier.github.io/teaching/20171014-DSM-Masterclass-Hamilton/spatial-data-in-R.html)




## The `sp` Package

The data structures ("classes") and functions provided by the [`sp`](https://cran.r-project.org/web/packages/sp/index.html) package have served a foundational role in the handling of spatial data in R. Many of the following examples will reference names such as `SpatialPoints`, `SpatialPointsDataFrame`, and `SpatialPolygonsDataFrame`. These are specialized classes, implemented by the `sp` package, that maintain linkages between all of the components of spatial data. For example, a point, line, or polygon feature will typically be associated with:
 
 * coordinate geometry
 * bounding box
 * coordinate reference system
 * attribute table

While it is possible to hand-make `sp` class objects, it is usually faster to import existing raster or vector files from disk. More on this later. For the sake of demonstration we will make a `SpatialPointsDataFrame` object. This is analogous to the point type feature class or shapefile.

```{r}
# create point geometry from coordinates
p <- SpatialPoints(coords = cbind(-97.721210, 40.446068))
# attach attribute table
p <- SpatialPointsDataFrame(p, data=data.frame(id=1, taxonname='alpha'))
# check internal structure
str(p)
```

This is an "S4" object with "slots" (e.g. `@data`) that are used to store various components. Most all S4 objects have specialized functions for getting and setting the contents of the slots. For example, the `bbox()` function is used to get or set the contents of the `@bbox` slot.


There is a convenient shortcut for "upgrading" point data that is stored in a `data.frame`. This is especially useful for preparing NASIS/KSSL data for spatial operations.
```{r}
# make some fake data
d <- data.frame(x=runif(10), y=runif(10), id=1:10, code=letters[1:10])
# upgrade to SpatialPointsDataFrame
coordinates(d) <- ~ x + y
# check
class(d)
```

See the [sp gallery](https://edzer.github.io/sp/) and [this vignette](https://cran.r-project.org/web/packages/sp/vignettes/intro_sp.pdf) for a much more detailed description of the `sp` classes.



### Interacting with `sp` Objects

`sp` objects are modeled after `data.frame` objects; rows (features) and columns (attributes) are accessed using `[]` or `$` notation.

Extract the first 5 features (and all associated attributes) from a `SpatialPointsDataFrame` object:
```{r }
# [rows, columns]
# [features, attributes]
d[1:5, ]
```

Extract the attributed called `id`:
```{r}
d$id
```

Create some random numbers and save to a new attribute called `rand`:
```{r}
# new data must be of the same length as number of features, otherwise recycling will occur
d$rand <- rnorm(n=nrow(d))
```

Filter features based on attributes:
```{r}
idx <- which(d$rand > 0)
d[idx, ]
```

Convert back to a `data.frame` object:
```{r}
as(d, 'data.frame')
```




### Coordinate Reference Systems and `proj4` Syntax

Spatial data aren't all that useful without an accurate description of the coordinate reference system (CRS). This type of information is typically stored within the ".prj" component of a shapefile, or in the header of a GeoTiff. Without a CRS it is not possible to perform coordinate transformations (e.g. conversion of geographic coordinates to projected coordinates), spatial overlay (e.g. intersection), or geometric calculations (e.g. distance or area).

In R, CRS information is encoded using ["proj4" notation](http://proj4.org/), which looks something like this: `'+proj=longlat +datum=WGS84'` (geographic coordinates referenced to the WGS84 datum). You can look-up CRS definitions in many different formats on the [spatialreference.org](http://spatialreference.org/) website. Some examples:

 * `+proj=longlat +datum=NAD83`: geographic coordinate system, NAD83 datum
 * `+proj=longlat +datum=NAD27`: geographic coordinate system, NAD27 datum
 * `+proj=utm +zone=10 +datum=NAD83`: projected coordinate system (UTM zone 10), NAD83 datum
 * `+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23.0 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs`: AEA CONUS / gSSURGO


Returning to the example from above, lets **assign** a CRS to this point using the `proj4string()` function. Note that this process doesn't alter the geometry of the object.
```{r }
proj4string(p) <- '+proj=longlat +datum=WGS84' 
str(p)
```

Transformation of points, lines, and polygons is a simple matter--as long the source and destination CRS data are available.
```{r}
# transform to UTM zone 14
p.utm <- spTransform(p, CRS('+proj=utm +zone=14 +datum=NAD83'))
cbind(gcs=coordinates(p), utm=coordinates(p.utm))

# transform to GCS NAD27
p.nad27 <- spTransform(p, CRS('+proj=longlat +datum=NAD27'))
cbind(coordinates(p), coordinates(p.nad27))
```

CRS definitions can also be generated from [EPSG](http://epsg.io/) codes. For example, a geographic coordinate system referenced to the WGS84 datum can be accessed using EPSG code 4326:
```{r}
CRS('+init=epsg:4326')
```


## Accessing Spatial Data Sources via GDAL/OGR
Many of the vector data sources we use can be directly translated to `sp` class objects via `rgdal` package. The `rgdal` package provides linkages to the powerful [GDAL/OGR](http://www.gdal.org/) library, the details of which are well beyond the scope of this training. Suffice to say, `rgdal` makes it possible to import just about any vector or raster data using two basic functions:

 * `readOGR()`: vector data sources such as shape files, KML, etc.
 * `readGDAL()`: raster data sources such as GeoTiff, IMG, ArcGRID, ASCII grids, etc.
 
A full listing of available drivers can be accessed with `ogrDrivers()` and `gdalDrivers()`. Note that it is not yet possible to load raster data from an ESRI File Geodatabase.

This module will not cover importing or exporting raster data via `rgdal` as the `raster` package provides a much more convenient interface to the GDAL library.


### Importing / Exporting Vector Data
In order to accommodate a wide range of possible formats, the `readOGR()` and `writeOGR()` functions expect a specialized description of the data source. 

Import the `ca630_a` feature class from a ESRI File Geodatabase:
```{r eval=FALSE}
x <- readOGR(dsn='E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb', layer='ca630_a')
```

Import data from shapefile stored at `E:/gis_data/ca630/pedon_locations.shp`. Note that the trailing "/" is omitted from the `dsn` (data source name) and the ".shp" suffix is omitted from the `layer`.
```{r eval=FALSE}
x <- readOGR(dsn='E:/gis_data/ca630', layer='pedon_locations')
```

Export object `x` to shapefile. Note that the trailing "/" is omitted from the `dsn` (data source name) and the ".shp" suffix is omitted from the `layer`.
```{r eval=FALSE}
writeOGR(x, dsn='E:/gis_data/ca630', layer='pedon_locations', driver = 'ESRI Shapefile')
```

The `readOGR()` and `writeOGR()` functions have many arguments and format-specific details. It is worth spending some time with the associated manual pages.


### Things to Keep in Mind

 * `sp` class objects are memory-bound, you won't likely be able to load the full gSSURGO grid any time soon
 * operations on `sp` class objects (e.g. CRS transformations) are relatively slow
    + this isn't a problem unless you are working with 100k+ features
    + much of the functionality in the `sp` package is being re-imagined as part of the `sf` package, it is likely much more efficient
 * it is not possible to efficiently "warp" raster data stored in `sp` class objects
    + usually much faster to warp raster data via GDAL command line tools (`gdalUtils` package) or GIS



## The `raster` Package
The release of the [`raster` package](https://cran.r-project.org/web/packages/raster/index.html) was a transformative event for scientists working with spatial data in R because of the unique way in which `RasterLayer` objects can be accessed *on-disk*. Previously, most of the available data structures (e.g. `SpatialGridDataFrame`, `SpatialPixelsDataFrame`, `matrix`, etc.) for storing raster data were memory-bound and not efficient for very large (> 10,000 x 10,000 cells) grids. That said, these data structures are still very useful for collections of raster data that fit into available memory. Many packages that perform advanced spatial analysis (e.g. `gstat` and `spatstat` packages) rely on the older data structures.

The `raster` package provides most of the commonly used grid processing functionality that one might find in a conventional GIS:
  
  * re-sampling / interpolation
  * warping (coordinate system transformations of gridded data)
  * cropping, mosaicing, masking
  * local and focal functions
  * raster algebra
  * contouring
  * raster/vector conversions
  * terrain analysis
  * model-based prediction (more on this in later chapters)

A more complete background on the capabilities of the `raster` package are described in the [*Introduction to the raster package*](https://cran.r-project.org/web/packages/raster/vignettes/Raster.pdf) vignette and recently released [*Spatial Data Analysis and Modeling with R*](http://rspatial.org/) online book.

### Importing / Exporting Data

Importing data is a simple matter. Note that the object `r` is a reference to the file.
```{r fig.width=6, fig.height=6}
# use an example from the raster package
f <- system.file("external/test.grd", package="raster")
# create a reference to this raster
r <- raster(f)
# print the details
print(r)
# default plot method
plot(r)
```

The disk-based reference can be converted to an in-memory `RasterLayer` with the `readAll()` function. Processing of raster data in memory is always faster than processing on disk, as long as there is sufficient memory.
```{r}
# check: file is on disk
inMemory(r)
# load into memory, if possible
r <- readAll(r)
# check: file is in memory
inMemory(r)
```


Exporting data requires consideration of the output format, datatype, encoding of NODATA, and other options such as compression. See the manual pages for `writeRaster()`, `writeFormats()`, and `dataType()` for details. For example, suppose you had a `RasterLayer` object that you wanted to save to disk as an internally-compressed GeoTiff:
```{r eval=FALSE}
# using previous example data set
writeRaster(r, filename='r.tif', options=c("COMPRESS=LZW"))
```

The `writeRaster()` function interprets the given (and missing) arguments as:

  * '.tif' suffix interpreted as `format=GTiff`
  * creation options of "LZW compression" passed to GeoTiff driver
  * default `datatype`
  * default `NAflag`

#### Data Types
It is worth spending a couple minutes going over some the commonly used `datatypes`; "unsigned integer", " signed integer", and "floating point" of variable precision.

 * `INT1U`: integers from 0 to 255
 * `INT2U`: integers from 0 to 65,534
 * `INT2S`: integers from -32,767 to 32,767
 * `INT4S`: integers from -2,147,483,647 to 2,147,483,647
 * `FLT4S`: floating point from -3.4e+38 to 3.4e+38
 * `FLT8S`: floating point from -1.7e+308 to 1.7e+308

It is wise to manually specify an output `datatype` that will "just fit" the required precision. For example, if you have generated a `RasterLayer` that warrants integer precision and ranges from 0 to 100, then the `INT1U` data type would provide enough precision to store all possible values *and* the NODATA value. Raster data stored as integers will always be smaller (sometimes 10-100x) than floating point, especially when internal compression is enabled.

#### Notes on Compression
In general, it is always a good idea to create internally-compressed raster data. The [GeoTiff format](https://gdal.org/drivers/raster/gtiff.html) can accomodate many different compression algorithms, including lossy (JPEG) compression. See [this article](https://kokoalberti.com/articles/geotiff-compression-optimization-guide/) for some ideas on optimization of file read/write times and associated compressed file sizes. Usually, the default "LZW" or "DEFLATE" compression will result in significant savings, especially for data encoded as integers.
```{r eval=FALSE}
# reasonable compression
writeRaster(r, filename='r.tif', options=c("COMPRESS=LZW"))

# takes longer to write the file, but better compression
writeRaster(r, filename='r.tif', options=c("COMPRESS=DEFLATE", "PREDICTOR=2", "ZLEVEL=9"))
```



### Object Properties
`RasterLayer` objects similar to `sp` objects in that they keep track of the linkages between data, coordinate reference system, and optional attribute tables. Getting and setting the contents of `RasterLayer` objects should be performed using functions such as:
  
  * `NAvalue(r)`: get / set the NODATA value
  * `crs(r)` or `proj4string(r)`: get / set the coordinate reference system
  * `res(r)`: get / set the resolution
  * `extent(r)`: get / set the extent
  * `dataType(r)`: get / set the data type
  * ... many more, see the `raster` package manual


### Things to Keep in Mind
  * the `raster` package provides data structures for working with grids either in memory or on disk
    + this enables relatively fast processing of small grids and reasonable processing of large grids
  * the `raster` package provides functions for performing most types of grid-based analysis
    + this is very convenient when working with small grids (no need to switch to GIS)
    + raster operations on large grids will always be faster with a dedicated GIS or GDAL tools
  * CRS transformations on vector data are always faster than raster data
  * time spent with the `raster` [documentation](http://rspatial.org/) is well invested


# Conclusions

R is a convenient environment for integrating spatial and tabular data sources for the purposes of soil correlation, estimation of RICs, model development, or any other task that requires quantitative analysis.

Pre-processing large raster and vector data sources in a conventional GIS is usually more efficient than doing this work in R. However, you can use R to direct high performance GIS tools such as [GDAL](https://cran.r-project.org/web/packages/gdalUtils/index.html), [SAGA](https://cran.r-project.org/web/packages/RSAGA/index.html), [GRASS GIS](https://cran.r-project.org/web/packages/rgrass7/), and [QGIS](https://github.com/jannes-m/RQGIS) in order to fully coordinate data pre-processing and analysis within a single script. [Chapter 4](http://ncss-tech.github.io/stats_for_soil_survey/chapters/4_exploratory_analysis/4_exploratory_analysis.html) contains several examples of linking R to SAGA GIS. 

Ten minutes spent with documentation or a tutorial will save you an hour of frustration. Don't hesitate to search for examples or similar work from which new tools can be adapted.
  


# Applied Examples / Exercises
Here is your chance to learn by doing. These examples may seem simplistic and task-specific, however, they lay the ground work for the more advanced analysis of spatial data in the next couple of chapters. Mastery of these basic skills well help you develop an intuition for the special requirements of spatial data.

## Download Example Data

If you haven't yet done so, please setup the sample data sets for this module.
```{r, eval=FALSE}
# store path as a variable, in case you want to keep it somewhere else
ch2b.data.path <- 'C:/workspace/chapter-2b'

# make a place to store chapter 2b example data
dir.create(ch2b.data.path, recursive = TRUE)

# download example data from github
# polygons
download.file('https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_2b-spatial-data/chapter-2b-mu-polygons.zip', paste0(ch2b.data.path, '/chapter-2b-mu-polygons.zip'))

# raster data
download.file('https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_2b-spatial-data/chapter-2b-PRISM.zip', paste0(ch2b.data.path, '/chapter-2b-PRISM.zip'))

# unzip
unzip(paste0(ch2b.data.path, '/chapter-2b-mu-polygons.zip'), exdir = ch2b.data.path, overwrite = TRUE)
unzip(paste0(ch2b.data.path, '/chapter-2b-PRISM.zip'), exdir = ch2b.data.path, overwrite = TRUE)
```



## Example Data

Just in case, load required packages into the current R Session.
```{r results="hide"}
library(aqp)
library(sp)
library(raster)
library(rgdal)
library(soilDB)
```


We will be using polygons associated with MLRAs 15 and 18 as part of this demonstration. Import these data now with `readOGR()`; recall the somewhat strange syntax.
```{r results='hide'}
# establish path to example data
ch2b.data.path <- 'C:/workspace/chapter-2b'

# load MLRA polygons
mlra <- readOGR(dsn=ch2b.data.path, layer='mlra-18-15-AEA')
```

Next, load in the example raster data with the `raster()` function. These are 800m PRISM data that have been cropped to the extent of the MLRA 15 and 18 polygons.
```{r}
# mean annual air temperature, Deg C
maat <- raster(paste0(ch2b.data.path, '/MAAT.tif'))
# mean annual precipitation, mm
map <- raster(paste0(ch2b.data.path, '/MAP.tif'))
# frost-free days
ffd <- raster(paste0(ch2b.data.path, '/FFD.tif'))
# growing degree days
gdd <- raster(paste0(ch2b.data.path, '/GDD.tif'))
# percent of annual PPT as rain
rain_fraction <- raster(paste0(ch2b.data.path, '/rain_fraction.tif'))
# annual sum of monthly PPT - ET_p
ppt_eff <- raster(paste0(ch2b.data.path, '/effective_preciptitation.tif'))
```

Sometimes it is convenient to "stack" raster data that share a common grid size, extent, and coordinate reference system into a single `RasterStack` object.
```{r}
rs <- stack(maat, map, ffd, gdd, rain_fraction, ppt_eff)
# reset layer names
names(rs) <- c('MAAT', 'MAP', 'FFD', 'GDD', 'rain.fraction', 'eff.PPT')
```

Quick inspection of the data.
```{r}
# object class
class(mlra)
class(maat)
class(rs)

# the raster package provides a nice "print" method for raster and sp classes
print(maat)

# coordinate reference systems: note that they are not all the same
proj4string(mlra)
proj4string(maat)
proj4string(rs)
```


Basic plot methods (class-specific functions) for the data. Note that this approach requires that all layers in the "map" are in the same coordinate refrence system (CRS).
```{r, fig.width=5, fig.height=6.75}
# MLRA polygons in native coordinate system
# recall that mlra is a SpatialPolygonsDataFrame
plot(mlra, main='MLRA 15 and 18')
box()

# MAAT raster
# recall that maat is a raster object
plot(maat, main='PRISM Mean Annual Air Temperature (deg C)')


# plot MAAT raster with MLRA polygons on top
# this requires transforming to CRS of MAAT
mlra.gcs <- spTransform(mlra, CRS(proj4string(maat)))
plot(maat, main='PRISM Mean Annual Air Temperature (deg C)')
plot(mlra.gcs, main='MLRA 15 and 18', add=TRUE)
```


## Spatial Overlay Operations

Spatial data are lot more useful when "combined" (overlay, intersect, spatial query, etc.) to generate something new. For simplicity, we will refer to this kind of operation as an "extraction".

The attribute data in one `sp` object can be extracted from another using the `over()` function, as long as the CRS are the same. Access the associated vignette by pasting `vignette("over")` in the console.
```{r}
# hand make a SpatialPoints object
# note that this is GCS
p <- SpatialPoints(coords = cbind(-120, 37.5), proj4string = CRS('+proj=longlat +datum=WGS84'))

# spatial extraction of MLRA data requires a CRS transformation
p.aea <- spTransform(p, proj4string(mlra))
over(p.aea, mlra)
```

The values stored in a `RasterLayer` or `RasterStack` object can be extracted using the `extract()` function. As long a the "query" feature has a valid CRS defined, the `extract()` function will automatically perform any required CRS transformation.
```{r}
# extract from a single RasterLayer
extract(maat, p)

# extract from a RasterStack
extract(rs, p)
```


The `extract()` function can perform several operations in one pass, such as buffering (in projected units) then extracting. See the manual page for an extensive listing of optional arguments and what they do.
```{r}
# extract using a buffer with radius specified in meters (1000m)
extract(rs, p, buffer=1000)
```



## Raster Data Sampling

Typically, spatial queries of raster data by polygon features are performed in two ways:
  
  * for each polygon, collect all pixels that overlap
  * for each polygon, collect a subset of pixels defined by [sampling points](http://ncss-tech.github.io/AQP/sharpshootR/sample-vs-population.html)
  
The first method ensures that all data are included in the analysis, however, processing is very slow and the results may not fit into memory. The second method can be far more computationally efficient (10-100x faster), require less memory, and remain statistically sound--as long as a reasonable sampling strategy is applied. A detailed description of sampling strategies in [chapter 3](http://ncss-tech.github.io/stats_for_soil_survey/chapters/3_sampling/3_sampling.html).



### Examples

Sampling and extraction, result is a `matrix` object.
```{r, eval=FALSE}
# sampling single RasterLayer
sampleRegular(maat, size=10)

# sampling RasterStack
sampleRegular(rs, size=10)
```

Sampling and extract, result is a `SpatialPointsDataFrame` object.
```{r fig.width=8, fig.height=5}
par(mfcol=c(1,2), mar=c(1,1,3,1))

# regular sampling + extraction of raster values
x.regular <- sampleRegular(maat, size = 100, sp=TRUE)
plot(maat, axes=FALSE, legend=FALSE, main='Regular Sampling')
points(x.regular)

# random sample + extraction of raster values
# note that NULL values are removed
x.random <- sampleRandom(maat, size = 100, sp=TRUE, na.rm=TRUE)
plot(maat, axes=FALSE, legend=FALSE, main='Random Sampling with NA Removal')
points(x.random)
```

Note that the mean can be efficiently estimated, even with a relatively small number of samples.
```{r}
# this will be very slow for large grids
mean(values(maat), na.rm=TRUE)

# generally much faster and just as good, 
# given reasonable sampling strategy and sample size
mean(x.regular$MAAT, na.rm=TRUE)

# this value will be different than what you get
# it is after all, random sampling
mean(x.random$MAAT, na.rm=TRUE)
```

Just how much variation can we expect when collecting 100, randomly-located samples over such a large area? This is better covered in chapter 4 (Sampling), but a quick experiment might be fun. Do this 100 times: compute the mean MAAT from 100 randomly-located samples.
```{r}
# takes a couple of seconds
z <- replicate(100, mean(sampleRandom(maat, size = 100, na.rm=TRUE), na.rm = TRUE))

# 90% of the time the mean MAAT values were within:
quantile(z, probs=c(0.05, 0.95))
```


# Extracting Raster Data at KSSL Pedon Locations
Extract PRISM data at the coordinates associated with KSSL pedons that have been correlated to the [AUBURN](https://casoilresource.lawr.ucdavis.edu/sde/?series=auburn) series. We will use the [`fetchKSSL()`](http://ncss-tech.github.io/AQP/soilDB/KSSL-demo.html) function from the `soilDB` package to get KSSL data from the most recent snapshot. This example can be easily adapted to pedon data extracted from NASIS using [`fetchNASIS()`](http://ncss-tech.github.io/AQP/soilDB/fetchNASIS-mini-tutorial.html).

Get some KSSL data and upgrade the "site" data to a `SpatialPointsDataFrame`.
```{r}
# result is a SoilProfileCollection object
auburn <- fetchKSSL(series = 'auburn')

# extract site data
s <- site(auburn)

# these are GCS WGS84 coordinates from NASIS
coordinates(s) <- ~ x + y
proj4string(s) <- '+proj=longlat +datum=WGS84'
```

Extract PRISM data (the `RasterStack` object we made earlier) at the Auburn KSSL locations and summarize.
```{r}
# return the result as a data.frame object
e <- extract(rs, s, df=TRUE)

# summarize out the extracted data
# note that we are "leaving out" the first column which contains an ID
summary(e[, -1])
```

Join the extracted PRISM data with the original `SoilProfileCollection` object. More information on [`SoilProfileCollection objects here`](http://ncss-tech.github.io/AQP/aqp/aqp-intro.html).
```{r}
# don't convert character data into factors
options(stringsAsFactors = FALSE)

# combine site data with extracted raster values, row-order is identical
res <- cbind(as(s, 'data.frame'), e)

# extract unique IDs and PRISM data
res <- res[, c('pedon_key', 'MAAT', 'MAP', 'FFD', 'GDD', 'rain.fraction', 'eff.PPT')]

# join with original SoilProfileCollection object via pedon_key
site(auburn) <- res
```

The extracted values are now part of the "auburn" `SoilProfileCollection` object. Does there appear to be a relationship between soil morphology and "effective precipitation"? Not really.
```{r fig.width=12, fig.height=6.5}
# create an ordering of pedons based on the extracted effective PPT
new.order <- order(auburn$eff.PPT)

# setup figure margins
# set to single figure output, just in case you are working from the top of the examples
par(mar=c(5,2,4,2), mfcol=c(1,1))
# plot profile sketches
plot(auburn, name='hzn_desgn', print.id=FALSE, color='clay', plot.order=new.order, cex.names=0.85)

# add an axis with extracted raster values
axis(side=1, at = 1:length(auburn), labels = round(auburn$eff.PPT[new.order]), cex.axis=0.75)
mtext('Annual Sum of Monthly (PPT - ET_p) (mm)', side=1, line=2.5)
```

Note that negative values are associated with a net deficit in monthly precipitation vs. estimated ET.






# Additional Reading
   
   * Ahmed, Zia. 2020. [Geospatial Data Science with R](https://zia207.github.io/geospatial-r-github.io/index.html).
   
   * Gimond, M., 2019. Intro to GIS and Spatial Analysis [https://mgimond.github.io/Spatial/](https://mgimond.github.io/Spatial/)

   * Hijmans, R.J. 2019. Spatial Data Science with R. [https://rspatial.org/](https://rspatial.org/)
   
   * Lovelace, R., J. Nowosad, and J. Muenchow, 2019. Geocomputation with R. CRC Press. [https://bookdown.org/robinlovelace/geocompr/](https://bookdown.org/robinlovelace/geocompr/)

   * Pebesma, E., and R.S. Bivand. 2005. Classes and methods for spatial data: The sp package. [https://cran.r-project.org/web/packages/sp/vignettes/intro_sp.pdf](https://cran.r-project.org/web/packages/sp/vignettes/intro_sp.pdf).
   
   * Pebesma, E. and R. Bivand, 2019. Spatial Data Science. [https://keen-swartz-3146c4.netlify.com/](https://keen-swartz-3146c4.netlify.com/)

