[
["index.html", "Statistics for Soil Survey - Part 1 Chapter 1 Prerequisites", " Statistics for Soil Survey - Part 1 Soil Survey Staff 2021-01-15 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. "],
["pre-course-assignment.html", "Chapter 2 Pre-course Assignment 2.1 Create Workspace 2.2 Configure RStudio 2.3 Essentials 2.4 Personalization 2.5 Install .RProfile 2.6 Install Required Packages 2.7 Common Errors 2.8 Packages not on CRAN 2.9 Connect Local NASIS 2.10 Proof", " Chapter 2 Pre-course Assignment 2.1 Create Workspace Make a local folder C:\\workspace2 to use as a working directory for this course. Use all lower case letters please. 2.2 Configure RStudio Open RStudio, and edit the “Global Options” (Main menu: Tools → Global Options). 2.3 Essentials These options are important for pleasant, reproducible and efficient use of the RStudio environment: Change the default working directory to C:\\workspace2 (R General Tab) Uncheck “Restore .Rdata into workspace at startup” (R General Tab) VERY IMPORTANT Figure 1: Example of RStudio General settings. RStudio detects the available R installations on your computer. Individual versions are certified for the Software Center as they become available, and sometimes there is a more recent version available for download. It is worth taking the time before installing packages to get the latest version of R available to you. This is to minimize compatibility issues which arise over time. 2.4 Personalization Figure 2: Example of RStudio Code/Editing settings. Optional: Check “Soft-wrap R source files” (Code/Editing Tab) Optional: Show help tooltips, control auto-completion and diagnostics (Code/Completion and Diagnostics Tabs) Optional: Update code font size, colors and theme (Appearance) Optional: Use RStudio Projects (top-right corner) to manage working directories 2.5 Install .RProfile The code you run next will establish a safe location for your R package library. Your package library should ideally be on a local disk with about 1 - 2 GB of free space. We want to prevent installs to ~ (your $HOME directory) which is typically on a network share (such as H:/), not a local disk. Copy the following code in the box below and paste into the R console panel after the command prompt (&gt;) and press enter. Hint: the R console is the lower left or left window in RStudio with a tab labeled “Console”. source(&#39;https://raw.githubusercontent.com/ncss-tech/soilReports/master/R/installRprofile.R&#39;) installRprofile(overwrite=TRUE) An updated set of library paths will be printed. Close and re-open RStudio, or Restart R (Main menu: Session → Restart R; or Ctrl+Shift+F10), before continuing to the next steps. Figure 3: Example of RStudio Console - the R library paths are on a local drive “C:/” When your .Rprofile is set up correctly you will see output in a new R console/session confirming your library paths are: on a local drive (such as C:/) specific to the version number of R installed (such as 4.0) 2.6 Install Required Packages Packages can be installed by name from the Comprehensive R Archive Network (CRAN) using the base R function install.packages There are a lot of packages out there – many more than you will download here, and many of which are useful for Soil Survey work. The first time you install packages, R will ask you if you want to create a local repository in your User Documents folder. Click Yes. For example, to download and install the remotes package from CRAN: install.packages(&quot;remotes&quot;) To install the R packages used in this class copy all of the code from the box below and paste into the R console window. Paste after the command prompt (&gt;) and press enter. Downloading and configuring the packages will take a while if you are installing or upgrading all of the packages in the list below. # ipkCRAN: a helper fuction for installing required packages from CRAN # - p: vector of package names # - up: logical - upgrade installed packages? Default: TRUE ipkCRAN &lt;- function(p, up = TRUE){ message(&#39;installing packages from CRAN...&#39;) if (up) { # default is to re-install everything install.packages(p, dependencies = TRUE) } else { # but if up != TRUE install just what is needed new.pkg &lt;- p[! (p %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg) &gt; 0) { install.packages(new.pkg, dependencies = TRUE) } } # finally, check and see if any failed missing.pkg &lt;- p[! (p %in% installed.packages()[, &quot;Package&quot;])] if (length(missing.pkg) &gt; 0) { warning(sprintf(&#39;\\033[31mOne or more packages failed to install!\\033[39m\\n%s&#39;, sprintf(&quot;Restart R then try `\\033[35minstall.packages(c(%s))\\033[39m`&quot;, paste0(sprintf(&#39;&quot;%s&quot;&#39;, missing.pkg), collapse = &quot;,&quot;))), call. = FALSE) } } ## character vector of packages packages &lt;- c( # soil &quot;aqp&quot;, &quot;soilDB&quot;, &quot;sharpshootR&quot;, &quot;soiltexture&quot;, # gis &quot;rgdal&quot;, &quot;raster&quot;, &quot;sp&quot;, &quot;sf&quot;, &quot;terra&quot;, &quot;gdalUtils&quot;, &quot;rgrass7&quot;, &quot;RSAGA&quot;, &quot;exactextractr&quot;, &quot;fasterize&quot;, # data management &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;devtools&quot;, &quot;roxygen2&quot;, &quot;Hmisc&quot;, &quot;RODBC&quot;, &quot;circular&quot;, &quot;DT&quot;, &quot;remotes&quot;, # graphics &quot;ggplot2&quot;, &quot;latticeExtra&quot;, &quot;maps&quot;, &quot;spData&quot;, &quot;tmap&quot;, &quot;mapview&quot;, &quot;plotrix&quot;, &quot;rpart.plot&quot;, &quot;visreg&quot;, # modeling &quot;car&quot;, &quot;rms&quot;, &quot;randomForest&quot;, &quot;ranger&quot;, &quot;party&quot;, &quot;caret&quot;, &quot;vegan&quot;, &quot;ape&quot;, &quot;shape&quot;, # sampling &quot;clhs&quot; ) ## install vector of CRAN packages &quot;safely&quot; ## up = TRUE to download all packages ## up = FALSE to download only packages you don&#39;t already have installed ipkCRAN(packages, up = TRUE) The ipkCRAN function will let you know if any of the above packages fail to install. Any time you run code, always check the console output for warnings and errors before continuing. If a lot of output is produced by a command you should scroll up and sift through. It may be best early on to send commands individually to learn about and inspect their output. 2.7 Common Errors 2.7.1 No output is produced after pasting into console If you do not have a new command prompt (&gt;) and a blinking cursor on the left hand side of your console, but instead see a + after you run a command, R may think you are still in the middle of submitting input to the “read-eval-print-loop” (REPL). If this is not expected you are possibly missing closing quotes, braces, brackets or parentheses. R needs to know you were done with your expression, so you may need to supply some input to get the command to be complete. You can use the shortcut Ctrl+C to get out of a partially-complete command. Pasting code line-by-line is useful but prone to input errors with multi-line expressions. Alternately, you can run commands or an entire file using the GUI or keyboard shortcuts such as Ctrl+Enter. You have a chance to try this in the example at the end. 2.7.2 ‘SOMEPACKAGE’ is not available (for R version X.Y.Z) This means either: A package named ‘SOMEPACKAGE’ exists but it is not available for your version of R CRAN does not have a package with that name You can try again, but first check for spelling and case-sensitivity. When in doubt search the package name on Google or CRAN to make sure you have it right. Note that not all R packages are available on CRAN: there are many other ways that you can deliver packages (including GitHub described below). 2.8 Packages not on CRAN To install the latest version of packages from the Algorithms for Quantitative Pedology (AQP) suite off GitHub we use the remotes package. The AQP packages are updated much more frequently on GitHub than they are on CRAN. Generally, the CRAN versions (installed above) are the “stable” releases whereas the GitHub repositories have new features and bug fixes. remotes::install_github(&quot;ncss-tech/aqp&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/soilDB&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/soilReports&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/sharpshootR&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) 2.9 Connect Local NASIS Establish an ODBC connection to NASIS by following the directions at the following hyperlink (ODBC Connection to NASIS). Once you’ve successfully established a ODBC connection, prove it by loading your NASIS selected set with the site and pedon tables for any pedons from your local area. You only need a few pedons at a minimum for this demo – too many (say, &gt;20) will make the example profile plot cluttered. Paste the below code at the command prompt (&gt;) and press enter, as you did above. Or create a new R script (Main menu: File → New File → R Script) and paste code into the “Source” pane (script editor window). Then, click the Run button in the top-right corner of the Script Editor or use Ctrl+Enter to run code at the cursor location / any selected code. This will execute the code in the Console. Submit the resulting plot to your mentor (from “Plot” pane (bottom-right): Export → Save as PDF…) # load packages into the current session library(aqp) # provides &quot;SoilProfileCollection&quot; object &amp; more library(soilDB) # provides database access methods # get pedons from NASIS selected set test &lt;- fetchNASIS(from = &#39;pedons&#39;) # inspect the result str(test, max.level = 2) # make a profile plot # set margins smaller than default par(mar=c(1,1,1,1)) # make profile plot of selected set, with userpedonid as label plot(test, label=&#39;pedon_id&#39;) 2.10 Proof Follow the one line example below, copy the output, and submit the results to your mentor. This will help us to verify that all of the required packages have been installed. # dump list of packages that are loaded into the current session sessionInfo() Additional Support/Optional Readings Paul Finnel’s webinar Spatial Data Analysis and Modeling with R (highly recommended) R-Intro R for Beginners The R Inferno AQP Website and Tutorials Stats for Soil Survey Webinar Soil Data Aggregation using R Webinar "],
["intro.html", "Chapter 3 Introduction to R and RStudio 3.1 Outline 3.2 Course Overview 3.3 What is R? 3.4 RStudio: An Integrated Development Environment (IDE) for R 3.5 Review 3.6 Additional Readings 3.7 References", " Chapter 3 Introduction to R and RStudio 3.1 Outline Course Overview Review Course Objectives Why is this training needed? Why is course organized this way? What is R? Why should I use R? What can R do? How do I get started? RStudio interface How to import, export, and view files How to navigate the Help tab What are packages? How to save files 3.2 Course Overview 3.2.1 Course Objectives Develop solutions to investigate soil survey correlation problems and update activities. Evaluate investigations for interpretive results and determine how to proceed. Create a continuous surface from point data. Summarize data for populations in NASIS. Help to pursue the question “why” 3.2.2 Why is this training needed? Long standing goal of the Soil Science Division to have a course in statistics (Mausbach, 2003) Opportunities to learn these techniques are limited, especially at the undergraduate level (Hennemann and Rossiter, 2004) Consistent methodology (data analysis, data population, sampling design, etc.) There is continually a greater need to use these techniques: Mapping of lands at high production rates (MacMillan et al., 2007; Kempen et al., 2012; Brevik et al., 2016) Ecological Sites (Maynard et al., 2019) Soil survey refinement (disaggregation) (Chaney et al., 2016; Ramcharan et al., 2017) 3.2.3 Why is course organized this way? Our best judgment for assembling into 24 hours what could be 6 University level courses Mixture of slides and script enabled web pages is new for NRCS The web content is a long-term investment and should serve as a permanent reference Feel free to provide guidance for improving the class for future offerings 3.3 What is R? R is a free, open-source software and programming language developed in 1995 at the University of Auckland as an environment for statistical computing and graphics (Ikaha and Gentleman, 1996). Since then R has become one of the dominant software environments for data analysis and is used by a variety of scientific disiplines, including soil science, ecology, and geoinformatics (Envirometrics CRAN Task View; Spatial CRAN Task View). R is particularly popular for its graphical capabilities, but it is also prized for it’s GIS capabilities which make it relatively easy to generate raster-based models. More recently, R has also gained several packages which are designed specifically for analyzing soil data. a software environment: statistics graphics programming calculator GIS etc… a language to explore, summarize, and model data functions = verbs objects = nouns 3.3.1 Why Should I Learn R? While the vast majority of people use Microsoft Excel for data analysis, R offers numerous advantages, such as: Cost. R is free! (“Free as in free speech, not free beer.”) Reproducible Research (self-documenting, repeatable) repeatable: code + output in a single document (‘I want the right answer, not a quick answer’ - Paul Finnell) easier the next time (humorous example) numerous Excel horror stories of scientific studies gone wrong exist (TED Talk) scalable: applicable to small or large problems R in a Community Numerous Discipline Specific R Groups Numerous Local R User Groups (including R-Ladies Groups) Stack Overflow Learning Resources (quantity and quality) R books (Free Online) R Books R is ‘becoming’ the new norm (paradigm shift?) “If we don’t accept these challenges, other who are less qualified will; and soil scientists will be displaced by apathy.” (Arnold and Wilding, 1992) While some people find the use of a commandline environment daunting, it is becoming a necessary skill for scientists as the volume and variety of data has grown. Thus scripting or programming has become a third language for many scientists, in addition to their native language and disipline specific terminology. Other popular programming languages include: SQL (i.e. NASIS), Python (i.e. ArcGIS), and JavaScript. ODBC and GDAL link R to nearly all possible formats/interfaces 3.3.2 What can R do? 3.3.3 Packages Base R (functionality is extended through packages) basic summaries of quantitative or qualitative data data exploration via graphics GIS data processing and analysis Soil Science R Packages aqp - visualization, aggregation, classification soilDB - access to commonly used soil databases soilReports - handful of report templates soiltexture - textural triangles Ecology R packages vegan - ordination, diversity analysis, etc. dismo - species distribution modeling 3.3.3.1 Soil Science Applications 3.3.3.1.1 Create Maps 3.3.3.1.2 Draw Soil Profiles 3.3.3.1.3 Draw Depth Plots 3.3.3.1.4 Estimate the Range in Characteristics (RIC) variable genhz pct10 median pct90 clay A 12 16 22 clay BAt 16 20 25 clay Bt1 18 24 32 clay Bt2 23 30 44 clay Cr 15 15 15 phfield A 6 6 7 phfield BAt 5 6 6 phfield Bt1 5 6 7 3.4 RStudio: An Integrated Development Environment (IDE) for R RStudio is an integrated development environment (IDE) that allows you to interact with R more readily. RStudio is similar to the standard RGui, but is considerably more user friendly. It has more drop-down menus, windows with multiple tabs, and many customization options. The first time you open RStudio, you will see three windows. A forth window is hidden by default, but can be opened by clicking the File drop-down menu, then New File, and then R Script.. Detailed information on using RStudio can be found at at RStudio’s Website. RStudio Windows / Tabs Location Description Console Window lower-left location were commands are entered and the output is printed Source Tabs upper-left built-in text editor Environment Tab upper-right interactive list of loaded R objects History Tab upper-right list of key strokes entered into the Console Files Tab lower-right file explorer to navigate C drive folders Plots Tab lower-right output location for plots Packages Tab lower-right list of installed packages Help Tab lower-right output location for help commands and help search window Viewer Tab lower-right advanced tab for local web content 3.4.1 USDA Computer Setup R and RStudio have been installed on all USDA computers that have NASIS installed. R and RStudio are typically updated and CCE-approved once a year. The versions on USDA machines may be one to three releases behind the latest version available for public download. Having an outdated version of R rarely creates a problem, although warnings will appear. It is generally best to update to the latest version that is generally available in the Software Center. 3.4.2 Basic Tips for using R R is command-line driven. It requires you to type or copy-and-paste commands after a command prompt (&gt;) that appears when you open R. This is called the “Read-Eval-Print-Loop” or REPL. After typing a command in the R console and pressing Enter on your keyboard, the command will run. If your command is not complete, R issues a continuation prompt (signified by a plus sign: +). R is case sensitive. Make sure your spelling and capitalization are correct. Commands in R are also called functions. The basic format of a function in R is: object &lt;- function.name(argument_1 = data, argument_2 = TRUE). The up arrow (^) on your keyboard can be used to bring up previous commands that you’ve typed in the R console. The $ symbol is used to select a particular column within the table (e.g., table$column). Any text that you do not want R to act on (such as comments, notes, or instructions) needs to be preceded by the # symbol (a.k.a. hash-tag, comment, pound, or number symbol). R ignores the remainder of the script line following #. For example: # make some data mydata &lt;- data.frame(x = 1:10) mydata$y &lt;- mydata$x ^ 2 # make a plot plot(mydata$x, mydata$y) # This is a comment 3.4.2.1 Brief Example # Addition 1 + 1 ## [1] 2 # Multiplication 10 * 10 ## [1] 100 # Compute Logarithm log10(100) ## [1] 2 # Print Text &quot;Hello World&quot; ## [1] &quot;Hello World&quot; # Plot Histogram hist(npk$yield) # Assignment test &lt;- 1 # or test = 1 3.4.3 Working Directories 3.4.3.1 Setting the Working Directory Before you begin working in R, you should set your working directory (a folder to hold all of your project files); for example, “C:\\workspace2\\…”. To change the working directory in RStudio, select main menu Session &gt;&gt; Set Working Directory &gt;&gt; …. Or, from the “Files” tab click More &gt;&gt; Set As Working Directory to use the current location of the “Files” tab as your working directory. Setting the working directory via the menus is the same as doing it in the Console with the setwd() command: setwd(&quot;C:/workspace2&quot;) This directory is where all your input data should be stored and also is the default location for plot files and other output. Essentially, you want to have the inputs for your code to be found in the working directory so that you can refer to them using relative file paths. Relative file paths make it easier if you move the folder containing your script(s) around. Or, if you share it with someone else, they will have little issue getting your code to work on their own file system. NOTE: Beware when specifying any file paths that R uses forward slashes / instead of back slashes \\. Back slashes are reserved for use as an escape character. To check the file path of the current working directory (which should now be “C:\\workspace2”), type: getwd() 3.4.3.2 RStudio Projects (.Rproj files) You can also manage your working directory using RStudio Projects. An RStudio Project file (.Rproj) is analogous to, for example, a .mxd file for ArcMap. It contains information about the specific settings you may have set for a “project”. You open or create projects using the drop down menu in the top right-hand corner of the RStudio window (shown below) RStudio Project Menu Here is what a typical Project drop-down menu looks like: RStudio Project Menu (expanded) You can create new projects from existing or new directories with “New Project…”. When you click “Open Project…”, your working directory is automatically set to the .Rproj file’s location – this is extremely handy Any projects you have created/used recently will show up in the “Project List” Keeping working directories simple and specific to a single “project” is a good practice that helps keeps your code and input data organized, and helps you come back to a project after some time away from it. 3.4.4 Data Management in RStudio 3.4.4.1 Importing Data After your working directory is set, you can import data from .csv, .txt, etc. One basic command for importing data into R is read.csv(). The command is followed by the file name and then some optional instructions for how to read the file. First, create an example file by copying the contents below. Paste the content into Notepad and save the file as sand_example.csv in your C:\\workspace2 folder. location,landuse,horizon,depth,sand city,crop,A,14,19 city,crop,B,25,21 city,pasture,A,10,23 city,pasture,B,27,34 city,range,A,15,22 city,range,B,23,23 farm,crop,A,12,31 farm,crop,B,31,35 farm,pasture,A,17,30 farm,pasture,B,26,36 farm,range,A,15,25 farm,range,B,24,29 west,crop,A,13,27 west,crop,B,29,25 west,pasture,A,11,21 west,pasture,B,31,26 west,range,A,14,23 west,range,B,24,24 This dataset can either be imported into R using the Import Dataset button from the Environment tab, or by typing the following command into the R console: sand &lt;- read.csv(&quot;C:/workspace2/sand_example.csv&quot;) # if your workspace was already set you could simply use the filename, like so sand &lt;- read.csv(&quot;sand_example.csv&quot;) 3.4.4.1.1 Note on Microsoft Excel files R can import Excel files, but generally speaking it is a bad idea to use Excel. Excel has a dangerous default which automatically converts data with common notations to their standard format without warning or notice. For example, the character “11-JUN” entered into a cell automatically becomes the date 6/11/2017, even though the data is still displayed as 11-JUN. The only way to avoid this default behavior is to manually import your data into Excel via the Data Tab&gt;Get External Data Ribbon, and manually set the data type of all your columns to text. Failure to do so has resulted in numerous retracted research articles (Washington Post Article). 3.4.4.2 Exporting Data To export data from R, use the command write.csv() function. Since we have already set our working directory, R automatically saves our file into the working directory. write.csv(sand, file = &quot;sand_example2.csv&quot;) # or use the write.table() function to export other text file types 3.4.4.3 Viewing and Removing Data Once the file is imported, it is imperative that you check to ensure that R correctly imported your data. Make sure numerical data are correctly imported as numerical, that your column headings are preserved, etc. To view data simply click on the sand dataset listed in the Environment tab. This will open up a separate window that displays a spreadsheet like view. Additionally you can use the following functions to view your data in R. Function Description print() prints the entire object (avoid with large tables) head() prints the first 6 lines of your data str() shows the data structure of an R object names() lists the column names (i.e., headers) of your data ls() lists all the R objects in your workspace directory Try entering the following commands to view the sand dataset in R: str(sand) names(sand) head(sand) ls() A data object is anything you’ve created or imported and assigned a name to in R. The Environment tab allows you to see what data objects are in your R session and expand their structure. Right now sand should be the only data object listed. If you wanted to delete all data objects from your R session, you could click the broom icon from the Environments tab. Otherwise you could type: # Remove all R objects rm(list = ls(all = TRUE)) # Remove individal objects rm(sand) 3.4.5 Getting Help R has extensive documentation, numerous mailing lists, and countless books (many of which are free and listed at end of each chapter for this course). To learn more about the function you are using and the options and arguments available, learn to help yourself by taking advantage of some of the following help functions in RStudio: Use the Help tab in the lower-right Window to search commands (such as hist) or topics (such as histogram). Type help(read.csv) or ?read.csv in the Console window to bring up a help page. Results will appear in the Help tab in the lower right-hand window. Certain functions may require quotations, such as help(\"+\"). # Help file for a function help(read.csv) # or ?read.csv # Help files for a package help(package = &quot;soiltexture&quot;) 3.4.5.1 Exercise: Examine the read.csv() function Read the help file for the read.csv() function. How would how you would disable the first row in the sand dataset from being intrepreted as a header? Report your answer to your mentor. 3.4.6 Packages Packages are collections of additional functions that can be loaded on demand. They commonly include example data that can be used to demonstrate those functions. Although R comes with many common statistical functions and models, most of our work requires additional packages. 3.4.6.1 Installing Packages To use a package, you must first install it and then load it. These steps can be done at the command line or using the Packages Tab. Examples of both approaches are provided below. R packages only need to be installed once (until R is upgraded or re-installed). Every time you start a new R session, however, you need to load every package that you intend to use in that session. Within the Packages tab you will see a list of all the packages currently installed on your computer, and 2 buttons labeled either “Install” or “Update”. To install a new package simply select the Install button. You can enter install one or more than one packages at a time by simply separating them with a comma. To find out what packages are installed on your computer, use the following commands: library() # or installed.packages() One useful package for soil scientists is the soiltexture package. It allows you to plot soil textural triangles. The following command shows how to install this package if you do not have currently have it downloaded: # CRAN (static version) installed.packages(c(&quot;aqp&quot;, &quot;soilDB&quot;, &quot;soilReports&quot;, &quot;soiltexture&quot;)) # GitHub (development version) devtools::install_github(&quot;ncss-tech/soilDB&quot;, dependencies = FALSE, upgrade_dependencies = FALSE, build = FALSE) 3.4.6.2 Loading Packages Once a package is installed, it must be loaded into the R session to be used. library(soiltexture) You can also load packages using the Packages Tab, by checking the box next to the package name. Documentation about the soiltexture package is available from the help functions in R. help(package = &quot;soiltexture&quot;) This help command sends you to a webpage. Scroll down and select the link “TT.plot”. This link brings up a webpage that has instructions on how to use the TT.plot() function in R. The basic usage of the TT.plot() function is: TT.plot(class.sys, tri.data). The “class.sym” argument specifies a character string naming the textural classificaiton system. # Copied from soiltexture vignette # Create a dummy data frame of soil textures: example &lt;- data.frame( CLAY = c(05, 60, 15, 05, 25, 05, 25, 45, 65, 75, 13, 47), SILT = c(05, 08, 15, 25, 55, 85, 65, 45, 15, 15, 17, 43), SAND = c(90, 32, 70, 70, 20, 10, 10, 10, 20, 10, 70, 10), OC = c(20, 14, 15, 05, 12, 15, 07, 21, 25, 30, 05, 28) ) TT.plot(class.sys = &quot;USDA-NCSS.TT&quot;, tri.data = example) For more examples see the the soiltexture vignette. Vignettes are a short tutorials that provide detailed examples and descriptions of packages. Unfortunately not all packages come with a vignette. 3.4.7 Writing Scripts RStudio’s Source Tabs serve as a built-in text editor. Prior to excuting R functions at the Console, commands are typically written down (or scripted). Scripting is essentially showing your work. The sequence of functions necessary to complete a task are scripted in order to document or automate a task. While scripting may seems cumbersome at first, it ultimately saves time in the long run, particularly for repetitive tasks (humorous YouTube Video on Scripting). Benefits include: allows others to reproduce your work, which is the foundation of science serves as instruction/reminder on how to perform a task allows rapid iteration, which saves time and allows the evaluation of incremental changes reduces the chance of human error 3.4.7.1 Basic Tips for Scripting To write a script, simply open a new R script file by clicking File&gt;New File&gt;R Script. Within the text editor type out a sequence of functions. Place each function (e.g. read.csv()) on a separate line. If a function has a long list of arguments, place each argument on a separate line. A command can be excuted from the text editor by placing the cursor on a line and typing Crtl + Enter, or by clicking the Run button. An entire R script file can be excuted by clicking the Source button. 3.4.8 Saving R Files In R, you can save several types of files to keep track of the work you do. The file types include: workspace, script, history, and graphics. It is important to save often because R, like any other software, may crash periodically. Such problems are especially likely when working with large files. You can save your workspace in R via the command line or the File menu. 3.4.8.1 R script (.R) An R script is simply a text file of R commands that you’ve typed. You may want to save your scripts (whether they were written in R Editor or another program such as Notepad) so that you can reference them in the future, edit them as needed, and keep track of what you’ve done. To save R scripts in RStudio, simply click the save button from your R script tab. Save scripts with the .R extension. R assumes that script files are saved with only that extension. If you are using another text editor, you won’t need to worry about saving your scripts in R. You can open text files in the RStudio text editor, but beware copying and pasting from Word files as discussed below. To open an R script, click the file icon. 3.4.8.2 Microsoft Word Files Using Microsoft Word to write or save R scripts is generally a bad idea. Certain keyboard characters, such as quotations \"“, are not stored the same in Word (e.g. they are”left\" and “right” handed). The difference is hard to distinguish, but will not run in R. Also, pasting your R code or output into Wword documents manually is not reproducible, so while it may work in a pinch, it ultimately costs you time. You can use the word_document Rmarkdown template to automatically “Knit” .docx files from R code using a template, which is very handy for quickly getting a nice looking document! 3.4.8.3 R Markdown (.Rmd) Stub about the basic benefits of Rmarkdown for reproducibility, interactive reports like Shiny, but also “Notebooks” in RStudio. This document is made in bookdown! You can make websites with blogdown, etc. You can knit visually appealing and high-quality documents into rich HTML, PDF or Word documents. These are all based off of the powerful pandoc engine and the tools in the Rmarkdown ecosystem. 3.4.8.4 R history (.Rhistory) An R history file is a copy of all your key strokes. You can think of it as brute force way of saving your work. It can be useful if you didn’t document all your steps in an R script file. Like an R file, an Rhistory file is simply a text file that lists all of the commands that you’ve executed. It does not keep a record of the results. To load or save your R history from the History Tab click the Open File or Save button. If you load an Rhistory file, your previous commands will again become available with the up-arrow and down-arrow keys. You can also use the command line to load or save your history. savehistory(file = &quot;sand.Rhistory&quot;) loadhistory(file = &quot;sand.Rhistory&quot;) history(max.show=Inf) #displays all previous commands 3.4.8.5 R Graphics Graphic outputs can be saved in various formats. Format Function pdf pdf(“graphic.pdf”) window metafile win.metafile(“graphic.wmf”) png png(“graph.png”) jpeg jpeg(“graph.jpg”) bmp bmp(“graph.bmp”) postscript postscript(“graph.ps”) To save a graphic: (1) Click the Plots Tab window, (2) click the Export button, (3) Choose your desired format, (3) Modify the export settings as you desire, and (4) click Save. The R command for saving a graphic is: png(file = &quot;npk_yield.png&quot;) plot(npk$yield) dev.off() The first line of this command creates a blank file named sand with a JPEG extension. The second line plots the data object that you want to create a graphic of (here it is conveniently the same name as the JPEG file we are creating). The third line closes the graphics device. 3.5 Review Given what you now know about R, try to answer the following questions: Can you think of a situation where an existing hypothesis or convientional wisdom was not repeatable? What are packages? What is GitHub? Where can you get help? What does the file .Rprofile do? What do you hope to get out of this class? 3.6 Additional Readings Introductory R Books Quick-R R Cookbook Reproducible Research with R &amp; RStudio (not free) Advanced DSM R Books Predictive Soil Mapping with R Using R for Digital Soil Mapping (not free) Soil Science R Applications aqp and soilDB tutorials ISRIC WOrld Soil Information Example Training Courses ISRIC World Soil Information YouTube Channel OpenGeoHub Courses OpenGeoHub YouTube Channel David Rossiter’s Cornell Homepage Pierre Roudier Soil Sciences and Statistics Review Articles Arkely, R., 1976. Statistical Methods in Soil Classification Research. Advances in Agronomy 28:37-70. https://www.sciencedirect.com/science/article/pii/S0065211308605520 Mausbach, M., and L. Wilding, 1991. Spatial Variability of Soils and Landforms. Soil Science Society of America, Madison. https://dl.sciencesocieties.org/publications/books/tocs/sssaspecialpubl/spatialvariabil Wilding, L., Smeck, N., and G. Hall, 1983. Spatial Variability and Pedology. In : L. Widling, N. Smeck, and G. Hall (Eds). Pedogenesis and Soil Taxonomy I. Conceps and Interactions. Elseiver, Amsterdam, pp. 83-116. https://www.sciencedirect.com/science/article/pii/S0166248108705993 This document is based on aqp version 1.26, soilDB version 2.5.9, and sharpshootR version 1.6.6. 3.7 References Brevik, E.C., J.A. Homburg, B.A. Miller, T.E. Fenton, J.A. Doolittle, and S.J. Indorante, 2016. Selected highlights in American soil science history from the 1980s to the mid-2010s. Catena 146:128-146. Chaney, N., E. Wood, A.B. McBratney, J.W. Hempel, T.W. Nauman, C.W. Brungard, and N.P. Odgers, 2016. POLARIS: A 30-meter probabilistic soil series maps of the contiguous United States. Geoderma 274(15)54-67. https://www.sciencedirect.com/science/article/pii/S0016706116301434 Hennemann, G.R., and Rossiter, DG., 2004. Training needs for the next generation of soil surveyors. International Conference on Innovative Techniques in Soil Survey; 22-26 March 2004, Cha-Am, Thailand. http://www.css.cornell.edu/faculty/dgr2/Docs/ChaAm/ChaAmKeynoteHennemann.pdf Kempen, B., D. Brus, J. Stoorvogel, G. Heuvelink, F. de Vries, 2012. Efficiency Comparison of Conventional and Digital Soil Mapping for Updating Soil Maps. Geoderma 76(6)2095-2115. https://acsess.onlinelibrary.wiley.com/doi/10.2136/sssaj2011.0424 Ihaka, R., and R. Gentleman. 1996. R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics 5(3):399–314. https://www.stat.auckland.ac.nz/~ihaka/downloads/R-paper.pdf MacMillian, R., D. Moon, and R. Coupe, 2007. Automated predictive ecological mapping in a Forest Region in B.C., Canada, 2001-2005. Geoderma 140(4)353-373. www.sciencedirect.com/science/article/pii/S0016706107001152 Mausbach, 2003. The Importance of Statistical Documentation - Keeping Soil Survey Information Relevant in the 21st Century. 2003 National Cooperative Soil Survey Conference, Plymouth, MA. https://www.nrcs.usda.gov/Internet/FSE_DOCUMENTS/nrcs142p2_051833.pdf Ramcharan, A., T. Hengl, T. Nauman, C. Brungard, S. Waltman, S. Wills, and J. Thompson, 2017. Soil Property and Class Mas of the Conterminous United States at 100-Meter Spatial Resolution. Soil Science Society of America Journal, 82(1)186-201. https://acsess.onlinelibrary.wiley.com/doi/10.2136/sssaj2017.04.0122 "],
["the-data-we-use.html", "Chapter 4 The Data We Use 4.1 Objectives 4.2 The Structure of Our Data 4.3 Challenges with Pedon Data 4.4 The aqp SoilProfileCollection 4.5 Using the soilDB Package 4.6 Viewing Pedon Locations 4.7 Working with Data in R 4.8 Extended Data Functions 4.9 Soil Reports 4.10 Customized Queries to Local NASIS Database", " Chapter 4 The Data We Use 4.1 Objectives Learn more about R and how to inspect objects and data types Use the soilDB package to load NASIS pedon data into R Learn about the checks run on data loaded by the fetchNASIS() function Understand the structure of data stored in a SoilProfileCollection (SPC) Learn ways to logically filter and subset data in R Learn how functions can be used to bundle operations Review additional data that is accessible via extended data functions Introduce soilReports R package 4.2 The Structure of Our Data What if you could extract, organize, and visualize data from NASIS and many other commonly used soil database sources with a couple of lines of code? We have created the soilDB package just for this! soilDB returns tabular data, spatial data objects and SoilProfileCollection objects! SoilProfileCollection objects are defined in the aqp (Algorithms for Quantitative Pedology) package as a handy abstraction for commonly used data and “bookkeeping” of said data. 4.2.1 Package References SoilProfileCollection Object Introduction Tutorials on the AQP website Package ‘aqp’ manual Package ‘soilDB’ manual Package ‘sharpshootR’ manual The manual pages for soilDB and aqp are accessible (click index at the bottom of the Help tab in RStudio) by entering the following into the R console: # not run library(soilDB) help(soilDB) # for links to lots of great examples look here! library(aqp) help(aqp) 4.2.2 soilDB Functions soilDB functions are the quickest way to get up and running: fetchNASIS() Gets and re-packages data from a local NASIS database. NASIS pedon/horizon data NASIS DMU/MU/component data fetchNASISLabData() Gets KSSL laboratory pedon/horizon layer data from a local NASIS database. fetchKSSL() Gets KSSL data from the SoilWeb system via BBOX, MLRA, or series name query. KSSL Data Demo Water Retention Curve Development from KSSL Data fetchOSD() Fetches a limited subset of horizon- and site-level attributes for named soil series from the SoilWeb system. OSDquery() Full-text searching of OSD sections. fetchRaCA() Gets Rapid Carbon Assessment (RaCA) data by State, geographic bounding-box, RaCA site ID, or series query from the SoilWeb system. RaCA Data Demo fetchSCAN() Queries soil and climate data from USDA-NRCS SCAN Stations. A Unified Interface to SCAN/SNOTEL Data fetchHenry() Downloads data from the Henry Mount Soil Climate Database. Henry Mount Soil Climate Database Tutorial fetchPedonPC() Fetches commonly used site and horizon data from a PedonPC v.5 database. fetchSDA() Fetches component data from Soil Data Access. fetchSDA_spatial() Fetches polygon, bounding box and centroid data from SSURGO, STATSGO and the sapolygon (Soil Survey Area Polygon) from Soil Data Access SDA_query() Submits queries to the Soil Data Access system. Soil Data Access Tutorial SDA and Spatial Data SDA and Interpretations 4.2.3 Importance of Pedon Data The importance of pedon data for present and future work cannot be overstated. These data represent decades of on-the-ground observations of the soil resource for a given area. As difficult as it may be to take the time to enter legacy pedon data, it is vitally important that we capture this resource and get these data into NASIS as an archive of point observations. 4.2.4 Some Issues With Pedon Data Making and documenting observations of soil requires hard work. Digging is difficult, and writing soil descriptions is time consuming! Our confidence in observations commonly weakens with the depth of the material described. If we acknowledge this, which we must, then how do we deal with it in pedon data? Use a cutoff depth, for example 100 cm, can be used to truncate observations to a zone of greater confidence. Show the relative confidence of the data with depth. 4.3 Challenges with Pedon Data Consistency Missing data Confidence in the observations Uncertainty with depth Description style differences Depth described, horizonation usage styles Legacy data vintage Decadal span of data Taxonomy updates, horizon nomenclature Location confidence Origin of the location information Datum used for data collection Accuracy for GPS values at the time of data collection 4.3.1 Meeting the Challenges Graphical display of the data and summary outputs (slice-wise aggregation) Generalized Horizon Labels (GHL). Derive an aggregate soil profile and summarize soil properties for groups of similar soils. More on that process can be seen in the following tutorial: GHL Aggregation Presentation and GHL Aggregation Tutorial. For more information regarding difficult pedon data, see the following tutorial in the “aqp” package: Dealing with Troublesome data. 4.4 The aqp SoilProfileCollection The SoilProfileCollection class (SPC) provided by the aqp package is a soil-specific tool. It is a type of object that simplifies the process of working with collections of data associated with soil profiles, e.g., site-level data, horizon-level data, spatial data, diagnostic horizon data, metadata, etc. A SoilProfileCollection is like the NASIS Pedon “object” in that it provides generalizations, specific routines and rules about the fundamental tables and their relationships that are relevant to soil observations. In many ways the SPC is more adaptable than the NASIS Pedon concept, strictly speaking. It can “contain” aggregations of any relevant parts of essentially any soil data schemas (table and column structure). Through this mechanism, it can be a mediator between formats and algorithms. 4.4.1 SoilProfileCollection methods Many “familiar” methods are defined for the SoilProfileCollection object. Some are unique, and others operate like more common functions of vector and data.frame objects, such as nrow() (“how many horizons?”) or length() (“how many sites/pedons?”). Perhaps most importantly, when you access the site() data or the horizons() data of a SoilProfileCollection, you get a data.frame object that you can use like any other you might use or make in R. 4.4.1.1 Promoting data.frame to SoilProfileCollection The SoilProfileCollection object is a collection of 1-D profile descriptions, of the type conventionally described on a Form 232, or on tabular data returned from laboratory. The object is “horizon data forward” in that you start with that, and can create site-level attributes by normalization, joins, calculations and more. Most of the time if you are using your NASIS data, or an official database, there are defined ways of getting the data out. In the pre-course, we had you set up a process so you could connect to your local NASIS instance to “fetch” data and have methods like fetchNASIS put things together for you. This input to make a SoilProfileCollection can be represented most simply as a data.frame with unique site or profile ID and depth combinations for each horizon or layer. Essentially just a portion of the phorizon or chorizon table in NASIS. A simple example of this type oftabular horizon data is the sp4 data set: some serpentine soil profiles stored in a data.frame in the aqp package (after McGahan et al., 2009). library(aqp) # Load sample serpentine soil data (McGahan et al., 2009) data(sp4, package = &quot;aqp&quot;) head(sp4) ## id name top bottom K Mg Ca CEC_7 ex_Ca_to_Mg sand silt clay CF ## 1 colusa A 0 3 0.3 25.7 9.0 23.0 0.35 46 33 21 0.12 ## 2 colusa ABt 3 8 0.2 23.7 5.6 21.4 0.23 42 31 27 0.27 ## 3 colusa Bt1 8 30 0.1 23.2 1.9 23.7 0.08 40 28 32 0.27 ## 4 colusa Bt2 30 42 0.1 44.3 0.3 43.0 0.01 27 18 55 0.16 ## 5 glenn A 0 9 0.2 21.9 4.4 18.8 0.20 54 20 25 0.55 ## 6 glenn Bt 9 34 0.3 18.9 4.5 27.5 0.20 49 18 34 0.84 class(sp4) ## [1] &quot;data.frame&quot; To convert this horizon data into a SoilProfileCollection, we need to identify three parameters: idname, top, and bottom. These parameters refer to the columns of unique profile IDs, top depths and bottom depths, respectively. These three define the “logical consistency” of 1-D descriptions within the collection. We specify this for your input data.frame using the depths&lt;- function. Use a formula to specify column names in the data.frame, in this case \"id\", \"top\" and \"bottom\". # # profile ID ~ top depth + bottom depth # depths(sp4) &lt;- id ~ top + bottom class(sp4) ## [1] &quot;SoilProfileCollection&quot; ## attr(,&quot;package&quot;) ## [1] &quot;aqp&quot; 4.4.1.1.1 Syntax Explanation The formula expresses the idea that a profile 1-dimensional description id has a vertical extent (Z, in profile) defined by set of top and bottom depths described for that profile. In this simplified 1-dimensional model we are concerned about keeping track of things like thickness, order, overlaps, gaps, or duplication in the data. From this, we can do a lot of “logic” checks – strictly from a perspective of basic physical representation of the horizon data. 4.4.1.2 Extracting Site and Horizon Data The SoilProfileCollection is an S4 R object. S4 objects have slots. Of primary importance, it has slots for site-level data and horizon-level data. You can extract values from these slots using the site() and horizons() functions. These create data.frame objects that are separate from the SoilProfileCollection. # extract site data from SPC into new data.frame &#39;s&#39; s &lt;- site(sp4) str(s) ## &#39;data.frame&#39;:\t10 obs. of 1 variable: ## $ id: chr &quot;colusa&quot; &quot;glenn&quot; &quot;kings&quot; &quot;mariposa&quot; ... # extract horizon data from SPC into new data.frame &#39;h&#39; h &lt;- horizons(sp4) str(h) ## &#39;data.frame&#39;:\t30 obs. of 14 variables: ## $ id : chr &quot;colusa&quot; &quot;colusa&quot; &quot;colusa&quot; &quot;colusa&quot; ... ## $ name : chr &quot;A&quot; &quot;ABt&quot; &quot;Bt1&quot; &quot;Bt2&quot; ... ## $ top : int 0 3 8 30 0 9 0 4 13 0 ... ## $ bottom : int 3 8 30 42 9 34 4 13 40 3 ... ## $ K : num 0.3 0.2 0.1 0.1 0.2 0.3 0.2 0.6 0.8 0.6 ... ## $ Mg : num 25.7 23.7 23.2 44.3 21.9 18.9 12.1 12.1 17.7 28.3 ... ## $ Ca : num 9 5.6 1.9 0.3 4.4 4.5 1.4 7 4.4 5.8 ... ## $ CEC_7 : num 23 21.4 23.7 43 18.8 27.5 23.7 18 20 29.3 ... ## $ ex_Ca_to_Mg: num 0.35 0.23 0.08 0.01 0.2 0.2 0.58 0.51 0.25 0.2 ... ## $ sand : int 46 42 40 27 54 49 43 36 27 42 ... ## $ silt : int 33 31 28 18 20 18 55 49 45 26 ... ## $ clay : int 21 27 32 55 25 34 3 15 27 32 ... ## $ CF : num 0.12 0.27 0.27 0.16 0.55 0.84 0.5 0.75 0.67 0.25 ... ## $ hzID : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... 4.4.1.3 Methods like data.frame The base R functions for accessing and setting data.frame columns by name such as $ and [[ work for SoilProfileCollection objects, too. 4.4.1.3.1 $ and [[ # SoilProfileCollection sp4$clay ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 sp4[[&#39;clay&#39;]] ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 # horizon data.frame h$clay ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 h[[&#39;clay&#39;]] ## [1] 21 27 32 55 25 34 3 15 27 32 25 31 33 13 21 23 15 17 12 19 14 14 22 25 40 51 67 24 25 32 # use $&lt;- / [[&lt;- to set proportional clay content sp4$clay &lt;- sp4[[&#39;clay&#39;]] / 100 # undo what we did above; back to percentage sp4[[&#39;clay&#39;]] &lt;- sp4$clay * 100 # create new site variable (recycled for all sites) site(sp4)$newvar1 &lt;- &quot;numberone&quot; # create new horizon variable (recycled for all horizons) horizons(sp4)$newvar2 &lt;- &quot;numbertwo&quot; 4.4.1.3.2 [ The SoilProfileCollection also has [ – but a different interpretation from the [i, j] indexing in data.frame. In a data.frame you have object[row, column, drop=TRUE]; the result is a data.frame (or a vector with default drop). In a SoilProfileCollection you have object[site, horizon]; the result is a SoilProfileCollection. # object[i, j] # i-index: 2 profiles, 6 horizons sp4[1:2, ] # first two profiles, all horizons ## SoilProfileCollection with 2 profiles and 6 horizons ## profile ID: id | horizon ID: hzID ## Depth range: 34 - 42 cm ## ## ----- Horizons (6 / 6 rows | 10 / 15 columns) ----- ## id hzID top bottom name K Mg Ca CEC_7 ex_Ca_to_Mg ## colusa 1 0 3 A 0.3 25.7 9.0 23.0 0.35 ## colusa 2 3 8 ABt 0.2 23.7 5.6 21.4 0.23 ## colusa 3 8 30 Bt1 0.1 23.2 1.9 23.7 0.08 ## colusa 4 30 42 Bt2 0.1 44.3 0.3 43.0 0.01 ## glenn 5 0 9 A 0.2 21.9 4.4 18.8 0.20 ## glenn 6 9 34 Bt 0.3 18.9 4.5 27.5 0.20 ## ## ----- Sites (2 / 2 rows | 2 / 2 columns) ----- ## id newvar1 ## colusa numberone ## glenn numberone ## ## Spatial Data: [EMPTY] # j-index: 10 profiles; 20 horizons sp4[, 1:2] # first two horizons of each profile ## SoilProfileCollection with 10 profiles and 20 horizons ## profile ID: id | horizon ID: hzID ## Depth range: 5 - 40 cm ## ## ----- Horizons (6 / 20 rows | 10 / 15 columns) ----- ## id hzID top bottom name K Mg Ca CEC_7 ex_Ca_to_Mg ## colusa 1 0 3 A 0.3 25.7 9.0 23.0 0.35 ## colusa 2 3 8 ABt 0.2 23.7 5.6 21.4 0.23 ## glenn 5 0 9 A 0.2 21.9 4.4 18.8 0.20 ## glenn 6 9 34 Bt 0.3 18.9 4.5 27.5 0.20 ## kings 7 0 4 A 0.2 12.1 1.4 23.7 0.58 ## kings 8 4 13 Bt1 0.6 12.1 7.0 18.0 0.51 ## [... more horizons ...] ## ## ----- Sites (6 / 10 rows | 2 / 2 columns) ----- ## id newvar1 ## colusa numberone ## glenn numberone ## kings numberone ## mariposa numberone ## mendocino numberone ## napa numberone ## [... more sites ...] ## ## Spatial Data: [EMPTY] Everything has gets subset simultaneously – sites, horizons, spatial data, diagnostics, etc. in one command. # First profile, first two horizons horizons(sp4[1, 1:2]) ## id name top bottom K Mg Ca CEC_7 ex_Ca_to_Mg sand silt clay CF hzID newvar2 ## 1 colusa A 0 3 0.3 25.7 9.0 23.0 0.35 46 33 21 0.12 1 numbertwo ## 2 colusa ABt 3 8 0.2 23.7 5.6 21.4 0.23 42 31 27 0.27 2 numbertwo 4.5 Using the soilDB Package The soilDB package for R works with a variety of soil and plant resource related data sources. It provides functions for accessing data stored in in NASIS, KSSL, SDA, SoilWeb, SoilGrids and other sources. These are wrappers around an internal database interface to NASIS; the one that you set up during the pre-course. Basic data checks are run within fetch functions. get functions are a usually a step lower in terms of abstraction – and generally return single data.frame or list of data.frame. You can set up scripts to make custom queries against these or other sources on your own – there is an example at the end of this section. For now, we will start with the fetch functions and others that will get you a large variety of data you can use for soil and ecological site analyses. 4.5.1 fetchNASIS() The fetchNASIS convenience function extracts data from a NASIS selected set via Structured Query Language (SQL). Note that the import process in fetchNASIS(), and the other methods, is not comprehensive. It does not pull every column for every table related to pedon data out of NASIS. Instead, it pulls essential / commonly used pedon and horizon data. Higher level functions like fetchNASIS() bundle a series of lower-level queries to get specific parts of the Pedon or Component data structures. Much of the nested complexity of NASIS is simplified in the resulting object. You may need to make more detailed queries and joins to resolve specific questions. Many-to-one relationships are “flattened” where possible by fetchNASIS(). This aggregates the data from various tables into one “site” record with related horizon records, per profile. You can see the child tables that are aggregated using the get_extended_data_from_NASIS() method, which returns a named list of child table sources that are joined to the SoilProfileCollection made with fetchNASIS() using the internal record IDs in the respective tables. 4.5.1.1 fetchNASIS arguments fetchNASIS() has many arguments: from = ‘pedons’ or ‘components’ or ‘pedon_report’ This option allows you to select which data you want to load from NASIS. Choosing either ‘pedons’ or ‘components’ will load data from your local database. If ‘pedon_report’ is specified then it will load data from the text file generated by the NASIS report ‘fetchNASIS’. url = ‘https://nasis.sc.egov.usda.gov/OfflineReports/fetchNASIS_04e6ec7d-fab5-4a90-bb88-9b9dc56dfdd8.txt’ If from = ‘pedon_report’ this option will load data from the URL that is generated when the NASIS report ‘fetchNASIS’ is run offline against the national database. This is useful for loading more than 20,000 pedons at one time, such for an entire Soil Survey Region. SS = TRUE/FALSE The Selected Set (SS) option allows you to choose whether you want the data to load from your current selected set in NASIS or from the local database tables. The default is set to TRUE so if unspecified fetchNASIS() will always load from the data in the selected set. stringAsFactors = TRUE/FALSE This option allows you to select whether to convert strings into factors or not. The default is set to FALSE, which will handle strings as character formats. Manually set this option to TRUE if you wish to handle character strings as factors. rmHzErrors = TRUE/FALSE Setting this value to TRUE (the default) enables checks for horizon depth consistency. Consider setting this argument to FALSE if you aren’t concerned about horizon-depth errors or if you know that your selected set contains many combination horizons (e.g., consisting of E/Bt horizons or similar two-part horizons described individually for the same depth range). Note that any pedons flagged as having horizon-depth errors (rmHzErrors = TRUE) are omitted from the data returned by fetchNASIS(). nullFragsAreZero = TRUE/FALSE Setting this value to TRUE (the default) converts null entries for rock fragment volumes to 0. This is typically the right assumption because rock fragment data are typically populated only when observed. If you know that your data contain a combination of omitted information (e.g. no rock fragment volumes are populated) then consider setting this argument to FALSE. soilColorState = ‘moist’ or ‘dry’ Select dry or moist colors to be converted and placed into a horizon-level attribute called soil_color. Moist and dry colors are also stored in moist_soil_color and dry_soil_color. lab = TRUE/FALSE This option allows for loading the data associated with horizons that may be in the phlabresults table. The default is set to FALSE, which will not load records from the phlabresults table. For more information on the data checks and adjusting the default options to fetchNASIS() function, see the following resource: Tips on Getting Data from NASIS into R. 4.5.2 Open Database Connectivity (ODBC) Connection to NASIS After setting up an ODBC connection, you can use R to access data from a selected set defined in your local NASIS database. How to Create an ODBC Connection to local NASIS database for R. Does NASIS need to be open and running to query data using soilDB? No, fetchNASIS() works whether the NASIS application is running or not. You just need to make sure that the data you want has been loaded into your selected set. 4.5.3 The gopheridge soilDB Dataset The gopheridge sample data set is a sample R object returned from fetchNASIS() in a self-contained .rda file stored in soilDB. Open RStudio, and set up the environment by loading packages and the gopheridge sample dataset. library(aqp) library(soilDB) # load example dataset data(gopheridge, package = &quot;soilDB&quot;) # what kind of object is this? class(gopheridge) # what does the internal structure look like? str(gopheridge, 2) # the fields at the site and horizon levels within the SPC siteNames(gopheridge) horizonNames(gopheridge) 4.5.3.1 Make profile sketches The plot() function applied to a SoilProfileCollection object generates sketches based on horizon depths, designations, and colors. The fetchNASIS() function automatically converts moist Munsell colors into R-style colors. Multiple colors per horizon are mixed to make the ones that are shown by default in the calculated soil_color fields. See ?plotSPC for a detailed list of arguments and examples. par(mar = c(1, 1, 1, 1)) # omitting pedon IDs and horizon designations plot(gopheridge, print.id = FALSE, name = &#39;&#39;, width = 0.3)s title(&#39;Pedons from the `gopheridge` sample dataset&#39;, line = -0.5) 4.5.4 Prepare Example Data Take a moment to open the NASIS client, create a selected set with some site/pedon objects that will be used in the following sections. Using a query that includes both Site and Pedon tables, download to your local database. Get sites with MT647 in the user site ID. NOTE: Depending on the query, you may need to include wild card characters such as: %MT647%. 4.5.4.1 Data Checks Run by fetchNASIS() When you load pedons using the fetchNASIS() function, the following data checks are performed: Presence of multiple map datums. Results reported to the user and the data are not modified. Inconsistent horizon boundaries. Pedons with inconsistent horizon boundaries are not loaded. In most cases, this occurs when the bottom depth of a horizon is not the same as the upper depth of the next lower horizon. Note the issue above. The bottom depth of the A horizon and the upper depth of the Bt1 horizon should be the same: either 30 or 38 cm. The correct depth needs to be determined and fixed in the database Missing lower horizon depths. Offending horizons are fixed by replacing the missing bottom depth with the top depth plus 2 cm. In the case of the profile shown above, a bottom depth of 137 cm would be inserted where the depth is missing. Sites missing pedon records. Data without corresponding horizons are not loaded. 4.5.4.2 Example: Find sites where errors occur to fix in NASIS If errors in the pedon data are detected when loading data using fetchNASIS(), the following “get” commands can trace them back to the corresponding records in NASIS. These access an option that is stored in the package enviroment that contains a vector of IDs. get(‘sites.missing.pedons’, envir = soilDB.env) Returns user site ID for sites missing pedons. get(‘dup.pedon.ids’, envir = soilDB.env) Returns user pedon ID for sites with duplicate pedon ID get(‘bad.pedon.ids’, envir = soilDB.env) Returns user pedon ID for pedons with inconsistent horizon depths. get(‘bad.horizons’, envir = soilDB.env) Returns a data.frame of horizon-level information for pedons with inconsistent horizon depths. For more information on the design of soilDB functions, see the following documentation: Introduction to soilDB. Additional documentation and examples can be found in: http://ncss-tech.github.io/AQP/aqp/aqp-intro.html http://ncss-tech.github.io/AQP/aqp/merged-legend-plot.html http://ncss-tech.github.io/AQP/aqp/profile-summary.html http://ncss-tech.github.io/AQP/aqp/SPC-plotting-ideas.html 4.5.4.3 Follow along with your own data Explore the site- and horizon-level data in your own SPC using the following code. NOTE: You must have pedons in your local NASIS selected set. # load required libraries library(aqp) library(soilDB) # load data from a NASIS selected set pedons &lt;- fetchNASIS(from = &#39;pedons&#39;) # what kind of object is this? class(pedons) # how many pedons? length(pedons) # look at variables in site and horizon tables within the SPC siteNames(pedons) horizonNames(pedons) # look at the first 2 rows of site and horizon data head(site(pedons), 2) head(horizons(pedons), 2) How can you find out how many site and horizon records are in the data you just loaded? 4.6 Viewing Pedon Locations 4.6.1 Plotting Geographic Data Plotting the data as an R graphic can give you some idea of how data look spatially and whether their distribution is what you expect. Typos are relatively common when coordinates are manually entered. Viewing the data spatially is a quick way to see if any points plot far outside of the geographic area of interest and therefore clearly have an error. # plot the locations of the gopheridge pedons within R # Steps: # 1) create and inspect an sf data.frame object # 2) plot the data with mapview # load libraries library(aqp) library(soilDB) library(sf) library(mapview) # this creates sample gopheridge object in your environment data(&quot;gopheridge&quot;, package = &quot;soilDB&quot;) # replace gopheridge object with fetchNASIS() (your data) #gopheridge &lt;- NULL #gopheridge &lt;- fetchNASIS() # create simple features POINT geometry data.frame # st_as_sf(): convert data.frame to spatial simple features collection in $geometry # st_crs(): set EPSG:4326 Coordinate Reference System (CRS) as Well-Known Text (WKT) gopher.locations &lt;- st_as_sf(site(gopheridge), coords = c(&#39;x_std&#39;,&#39;y_std&#39;), crs = st_crs(4326)) # inspect first 6 points head(gopher.locations) # inspect CRS st_crs(gopher.locations) # create interactive map with sfc_POINT object # use site_id in sf data.frame as labels mapview(gopher.locations, legend = FALSE, map.types = &#39;OpenStreetMap&#39;, label = gopher.locations$site_id) 4.6.2 Displaying Pedon Data in Google Earth Google Earth is a powerful viewer for point data. Geographic data is displayed in Google Earth using the Keyhole Markup Language (KML) format. Using the plotKML package, you can easily create a KML file to inspect and view in Google Earth. See the related material in this tutorial: Export Pedons to Google Earth. 4.6.3 Exporting Pedon Data to an ESRI Shapefile Another way you can view the data is to export a shapefile from R. For further information, see this tutorial: Export Pedons to Shapefile. 4.6.4 Follow along with your own data Use the script below to make an R plot of pedon data loaded from your NASIS selected set. The following script plots the standard WGS84 longitude/latitude decimal degrees fields from Site table of NASIS. In some cases, these fields might be incomplete due to insufficient data or to not having been calculated from UTM coordinates in NASIS. Run the following script on the data loaded from your local NASIS selected set. Note that you need to filter out any pedons that are missing their standard WGS84 longitude/latitude coordinates. library(aqp) library(soilDB) library(sf) library(mapview) # get pedons from the selected set pedons &lt;- fetchNASIS(from = &#39;pedons&#39;) # missing values in coordinates not allowed pedons.sp &lt;- subset(pedons, !is.na(x_std) &amp; !is.na(y_std)) # create sf object pedon.locations &lt;- st_as_sf(site(pedons.sp), coords = c(&#39;x_std&#39;,&#39;y_std&#39;), crs = st_crs(4326)) # interactive map mapview(pedon.locations, legend = FALSE, map.types = &#39;OpenStreetMap&#39;, label = pedon.locations$site_id) 4.7 Working with Data in R 4.7.1 Summaries Now that you’ve loaded some data, you can look at additional ways to summarize and interact with data elements. 4.7.1.1 table() The base R table() function is very useful for quick summary operations. It returns a named vector with the amount of each unique level of the a given vector. The numeric vector of “counts” is commonly combined with other functions such as sort(), order(), prop.table(), is.na() or !is.na() (is not NA) to identify abundance, proportions, or missing data (NA). # load required packages library(aqp) library(soilDB) data(&quot;gopheridge&quot;, package = &quot;soilDB&quot;) # for these examples, we use the gopheridge dataset as our &quot;selected set&quot; pedons &lt;- gopheridge # fetchNASIS() # you can use fetchNASIS # summarize which soil taxa we have loaded table(pedons$taxonname) # sort results in descending order sort(table(pedons$taxonname), decreasing = TRUE) # could do the same thing for taxonomic subgroups or any column of the SPC at the site or horizon levels table(pedons$taxsubgrp) sort(table(pedons$taxsubgrp), decreasing = TRUE) 4.7.1.2 dput() Another very useful function is dput(), which prints a string-representation of the output of an R expression as code that generates that output so that you can re-use it. It is also good short-hand for concatenating a comma-delimited list. Here, we select the first four pedon_id values in pedons site table, and print out a comma-separated c() expression showing those values as a static R expression. data(&quot;gopheridge&quot;, package = &quot;soilDB&quot;) pedons &lt;- gopheridge dput(site(pedons)$pedon_id[1:4]) ## c(&quot;08DWB028&quot;, &quot;07RJV098&quot;, &quot;07RJV099&quot;, &quot;S2007CA009002&quot;) This result string can be copy-pasted as a comma-delimited string, used as string for NASIS list queries or other things. The dput() function is helpful when sending questions or example data to colleagues. Here, we see that the results of dput() are equivalent to the input after evaluation by R. all(c(&quot;08DWB028&quot;, &quot;07RJV098&quot;, &quot;07RJV099&quot;, &quot;S2007CA009002&quot;) == site(pedons)$pedon_id[1:4]) 4.7.1.3 The dplyr package This is a stub to talk about dplyr some links to tidyverse-centric examples of working with data.frame/tibble and summarizing them. Talk about cool things like rowwise(), across() and translation to high-performance code (dtplyr) and SQL (dbplyr). 4.7.2 Missing Values table(pedons$taxsubgrp, useNA = &quot;ifany&quot;) ## ## mollic haploxeralfs typic haploxerepts ultic haploxeralfs ultic haploxerolls ## 1 6 44 1 # is.na(...) table(is.na(pedons$taxsubgrp)) ## ## FALSE ## 52 # is NOT NA !is.na(...) table(!is.na(pedons$taxsubgrp)) ## ## TRUE ## 52 # it can also be applied to horizon level columns in the SPC sort(table(pedons$texture), decreasing=TRUE) ## ## BR L GR-L GRV-L CBV-L SPM GRX-L SIL GRV-CL CBV-CL ## 58 37 33 24 18 14 12 11 9 8 ## GR-SIL CBX-L GRX-CL CBX-CL GRV-SIL CL GRV-SCL GRX-C MPM SL ## 7 6 5 4 4 3 3 3 3 3 ## CB-L GR-CL GRX-SCL PGR-C PGRX-L SICL STV-CL STV-L STX-C STX-L ## 2 2 2 2 2 2 2 2 2 2 ## C CB-C CB-CL CB-SCL CB-SIL CBV-SIL CBX-SCL CN-L CN-SICL CNX-L ## 1 1 1 1 1 1 1 1 1 1 ## CNX-SICL FLV-L GR-C GR-SIC GRV-SICL GRX-SIC GRX-SIL PCB-SICL PCBV-SICL PCN-C ## 1 1 1 1 1 1 1 1 1 1 ## PCNX-CL PGRV-C PGRV-CL PGRX-SCL PGRX-SIL ST-L STV-C STX-CL STX-SICL ## 1 1 1 1 1 1 1 1 1 4.7.3 Logical Operators Less than &lt;, greater than &gt;, less than or equal to &lt;=, and greater than or equal to &gt;=. %in% Equivalent to IN () in SQL; same logic as match() Example: pedons$taxpartsize %in% c('loamy-skeletal', 'sandy-skeletal') Returns a vector of TRUE/FALSE equal in length to left-hand side. != Not-equal-to character “string.” == Note in the example above that R uses a double equal sign as “equal to.” 4.7.4 Pattern Matching The following examples use the grep() function to pattern match within the data, create an index of the SoilProfileCollection for records that match the specified pattern within that column, and then use that index to filter to specific sites and their corresponding profiles. Patterns are specified using regular expression (REGEX) syntax. This process can be applied to many different columns in the SPC based on how you need to filter the data. This example pattern matches on the tax_subgroup column, but another useful application might be to pattern match on geomorphology or parent material. Say we want to see what the variation of particle size classes are within a specific subgroup? We can use grep() to create a row index, then apply that index to the SoilProfileCollection. # create a numeric index for pedons with taxsubgroup containing &#39;typic&#39; idx &lt;- grep(&#39;typic&#39;, pedons$taxsubgrp) idx ## [1] 11 12 13 14 26 50 # use square bracket notation to subset &#39;typic&#39; soils in `subset1` object subset1 &lt;- pedons[idx,] subset1 ## SoilProfileCollection with 6 profiles and 37 horizons ## profile ID: peiid | horizon ID: phiid ## Depth range: 200 - 200 cm ## ## ----- Horizons (6 / 37 rows | 10 / 69 columns) ----- ## peiid phiid hzdept hzdepb hzname genhz clay silt sand fragvoltot ## 351685 1567471 0 10 A1 A 20 35 45 20 ## 351685 1567472 10 18 A2 A 20 35 45 40 ## 351685 1567473 18 28 A3 A 20 35 45 10 ## 351685 1567474 28 43 Bw1 Bt1 20 35 45 80 ## 351685 1567475 43 51 Bw2 Bt2 20 35 45 20 ## 351685 1567476 51 58 BC Bt2 20 35 45 82 ## [... more horizons ...] ## ## ----- Sites (6 / 6 rows | 10 / 88 columns) ----- ## peiid pedon_id siteiid site_id obs_date utmzone utmeasting utmnorthing x y ## 351685 07RJV100 352173 07CA630RJV100 2007-09-17 10 731914.0 4180918 -120.3678 37.74611 ## 351686 07RJV101 352174 07CA630RJV101 2007-09-17 10 732398.3 4180564 -120.3625 37.74278 ## 351687 07RJV102 352175 07CA630RJV102 2007-09-17 10 732298.0 4181093 -120.3633 37.74778 ## 351688 07RJV103 352176 07CA630RJV103 2007-09-24 10 731470.3 4177508 -120.3739 37.71556 ## 352495 07RJV093 352964 07CA630RJV093 2007-08-28 10 733216.8 4181168 -120.3531 37.74805 ## 640650 07JCR008 646344 07CA630JCR008 2007-06-14 10 700696.7 4201941 -120.7161 37.94278 ## ## Spatial Data: [EMPTY] # or use the index directly to summarize taxpartsize for &#39;typic&#39; soils sort(table(pedons$taxpartsize[idx]), decreasing = TRUE) ## ## loamy-skeletal clayey-skeletal ## 5 1 Note: grep() below has an invert argument (default FALSE). This option is very useful for excluding the results of the pattern matching process by inverting whatever the result is. grepl() is the logical version of grep(), so you can invert it using the logical NOT operator: !. Another method is to create an index using which() function. which() takes any logical vector (or expression), and it returns the indices (positions) where that expression returns TRUE. Do a graphical check to see the “typic” profiles are selected. Plot them in R using the SoilProfileCollection “plot” method (e.g., specialized version of the generic plot() function). # adjust margins par(mar=c(1,0,0,1)) # plot the first 10 profiles of subset1 plot(subset1[1:10, ], label = &#39;taxsubgrp&#39;, max.depth = 60) title(&#39;Pedons with the word &quot;typic&quot; at subgroup-level of Soil Taxonomy&#39;, line=-2) For more information on using regular expressions in grep() for pattern matching operations, see: Regular-expression-syntax. Quick check: Compare or run these commands with some code, and review the documentation, to answer the questions. True or False: grepl() returns a numeric vector True or False: which(grepl('typic', pedons$taxsubgrp)) is the same as grep('typic', pedons$taxsubgrp). 4.7.4.1 Important syntax options for REGEX pattern matching | OR operator for multiple expressions: Example: grep('loamy|sandy', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) ==&gt; “loamy OR sandy” ^ Anchor to beginning of string / line: Example: grep('^sandy', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) ==&gt; “STARTS WITH sandy” $ Anchor to end of string / line: Example: grep('skeletal$', c(\"loamy-skeletal\",\"sandy\",\"sandy-skeletal\")) ==&gt; “ENDS WITH skeletal” \\\\b Anchor to word boundary: Example: grep('\\\\bmesic', c(\"mesic\",\"thermic\",\"isomesic\")) ==&gt; “WORD STARTS WITH mesic” (e.g. not “isomesic”) 4.7.4.2 Resources for learning about regular expressions https://regex101.com/ &amp; https://regexr.com/ - Online regular expression testers http://www.regular-expressions.info/quickstart.html - One-page regular expression quick start guide 4.7.5 Filtering A variety of methods are available to subset or “filter” R data objects, from data.frame or vector, to something more complex like a Spatial object or a SoilProfileCollection. You can index many R objects using numeric or logical expressions as above. There are also methods that make this process a little easier. The base R method for this is subset() and it works on data.frame objects. It is nice because you can specify column names without explicitly referencing the data set, since subset uses non-standard evaluation of expressions passed as arguments. 4.7.6 Filtering Data by Logical Criteria with aqp::subset We use the SoilProfileCollection subset method, where we first specify a data (pedons) object then we can write expressions for the columns that exist in that object. Here, we combine two logical expressions to find taxsubgrp containing \"alfs\" (Alfisols) with obsdate before January 1st, 2010. subset2 &lt;- subset(pedons, grepl(&quot;alfs&quot;, taxsubgrp) &amp; obs_date &lt; as.POSIXlt(&quot;2010-01-01&quot;)) # check taxonomic range of particle size classes in the data # overwhelmingly these are described as loamy-skeletal ultic haploxeralfs sort(table(subset2$taxsubgrp), decreasing=TRUE) ## ## ultic haploxeralfs mollic haploxeralfs ## 28 1 sort(table(subset2$taxpartsize), decreasing=TRUE) ## ## loamy-skeletal clayey-skeletal ## 27 2 # a double equal sign &#39;==&#39; is used for exact character or numeric criteria subset3 &lt;- subset(subset2, taxpartsize == &#39;loamy-skeletal&#39;) table(subset3$taxpartsize) ## ## loamy-skeletal ## 27 par(mar=c(0,0,2,1)) plotSPC(subset3[1:12,], print.id = FALSE) title(&#39;Loamy-skeletal Ultic Haploxeralfs&#39;) 4.7.7 Dates and Times Unix time is a system for describing a point in time. It is the number of seconds that have elapsed since the Unix epoch, minus leap seconds; the Unix epoch is 00:00:00 UTC on 1 January 1970. Above, we use the fact that you can logically compare dates and times if their string representation is converted to a common base R UNIX time representation known as POSIXlt. This conversion accounts for important things like timezone, using your current locale – which is important to keep in mind. 4.7.8 Review of Data Checks Run by fetchNASIS() Now that you’ve loaded some data and learned a little about how to filter data in the SPC, you can quickly review some of the get() functions used to track data issues detected in the process of loading data back to the NASIS records in your selected set. get(&#39;sites.missing.pedons&#39;, envir = soilDB.env) get(&#39;dup.pedon.ids&#39;, envir = soilDB.env) get(&#39;bad.pedon.ids&#39;, envir = soilDB.env) get(&#39;bad.horizons&#39;, envir = soilDB.env) idx &lt;- pedons$pedon_id %in% get(&#39;bad.pedon.ids&#39;, envir = soilDB.env) good.pedons &lt;- pedons[which(idx),] bad.pedons &lt;- pedons[which(!idx),] 4.8 Extended Data Functions Additional data related to both site and horizon information can be fetched using the get_extended_data_from_NASIS() function. This data is not automatically brought into a SoilProfileCollection because these data elements are typically related to the site or horizon data in one-to-many relationships. 4.8.1 Thicknesses of Diagnostic Features 4.8.1.1 fetchNASIS: Boolean Diagnostic Feature Columns If diagnostic features are populated in the pedon diagnostic features table in NASIS, then Boolean (TRUE or FALSE) fields are created for each diagnostic feature in the data brought in by soilDB. These fields can be readily used to model the presence or absence of a diagnostic soil feature by extracting the site data from the SoilProfileCollection with site(). The following is an example of how you could use the diagnostic features (if populated!) from the extended data to determine the thickness of a diagnostic feature of interest. # rename gopheridge data data(&quot;gopheridge&quot;) f &lt;- gopheridge # get diagnostic features associated with pedons loaded from selected set d &lt;- diagnostic_hz(f) # summary of the diagnostic features in your data! unique(d$featkind) ## [1] ochric epipedon argillic horizon lithic contact cambic horizon ## [5] paralithic contact mollic epipedon paralithic materials &lt;NA&gt; ## 84 Levels: anthropic epipedon abrupt textural change andic soil properties ... manufactured layer contact sort(table(droplevels(factor(d$featkind))), decreasing = TRUE) ## ## ochric epipedon argillic horizon lithic contact paralithic contact ## 51 44 33 19 ## cambic horizon mollic epipedon paralithic materials ## 12 1 1 # subset argillic horizons d &lt;- d[d$featkind == &#39;argillic horizon&#39;, ] # create a new column and subtract the upper from the lower depth d$argillic_thickness_cm &lt;- d$featdepb - d$featdept # create another new column with the upper depth to the diagnostic feature d$depth_to_argillic_cm &lt;- d$featdept # omit NA values d &lt;- na.omit(d) # subset to pedon records IDs and calculated thickness d &lt;- d[, c(&#39;peiid&#39;, &#39;argillic_thickness_cm&#39;, &#39;depth_to_argillic_cm&#39;)] head(d) ## peiid argillic_thickness_cm depth_to_argillic_cm ## 2 242808 63 18 ## 5 252851 50 18 ## 7 268791 43 15 ## 10 268793 71 10 ## 13 268794 50 5 ## 16 268795 46 15 # left-join with existing site data site(f) &lt;- d # plot as histogram par(mar = c(4.5, 4.5, 1, 1)) hist(f$argillic_thickness_cm, xlab = &#39;Thickness of argillic top depth (cm)&#39;, main = &#39;&#39;) hist(f$depth_to_argillic_cm, xlab = &#39;Depth to argillic top depth (cm)&#39;, main = &#39;&#39;) Quick check: What can you do with the boolean diagnostic feature data stored in the site table of a fetchNASIS SoilProfileCollection? s ### Diagnostic Feature Diagrams ## work up diagnostic plot based on gopheridge dataset library(aqp) library(soilDB) library(sharpshootR) # load data data(gopheridge) # can limit which diagnostic features to show by setting &#39;v&#39; manually v &lt;- c(&#39;ochric.epipedon&#39;, &#39;cambic.horizon&#39;, &#39;argillic.horizon&#39;, &#39;paralithic.contact&#39;, &#39;lithic.contact&#39;) # generate diagnostic property diagram diagnosticPropertyPlot(gopheridge, v, k = 5, grid.label = &#39;site_id&#39;, dend.label = &#39;taxonname&#39;, sort.vars = FALSE) # plot with diagnostic features ordered according to co-occurrence diagnosticPropertyPlot(gopheridge, v, k = 5, grid.label = &#39;site_id&#39;, dend.label = &#39;taxonname&#39;, sort.vars = TRUE) 4.8.1.2 Follow along with your own data Use the following script to generate a diagnostic-feature diagram for the pedon data you’ve loaded from your NASIS selected set. Note: If the data includes more than about 20 pedons, the script might generate figures that are very hard to read. You also need to be certain that pedon diagnostic feature were populated in your data. Select a series of diagnostic properties or automatically pull diagnostic feature columns. library(aqp) library(soilDB) library(sharpshootR) # Load data f &lt;- fetchNASIS(from = &#39;pedons&#39;) # May need to subset to a particular series or taxon here... # to reduce the number of pedons! # get all diagnostic feature columns from site data by pattern matching on &#39;[.]&#39; in the colnames idx &lt;- grep(&#39;[.]&#39;, colnames(site(f))) v &lt;- colnames(site(f))[idx] v # or insert diagnostics of interest # from the list of possible diagnostics in &#39;v&#39; v &lt;- c(&#39;ochric.epipedon&#39;, &#39;cambic.horizon&#39;, &#39;argillic.horizon&#39;, &#39;paralithic.contact&#39;, &#39;lithic.contact&#39;) # generate diagnostic property diagram diagnosticPropertyPlot(f, v, k = 5, grid.label = &#39;site_id&#39;, dend.label = &#39;taxonname&#39;) For more information on generating diagnostic feature diagrams, see the following tutorial: Diagnostic Feature Property Plots. 4.9 Soil Reports One of the strengths of NASIS is that it that has many queries and reports to access the complex data. This makes it easy for the average user to load their data, process it and run numerous reports. soilReports contains a small collection of reports mostly designed to summarize data into consistent report formats. This collection of reports automates many methods we have reviewed so far, but are no substitute for interacting with your data and asking questions. Example report output can be found at the following link: https://github.com/ncss-tech/soilReports#example-output. Detailed instructions are provided for each report: https://github.com/ncss-tech/soilReports#choose-an-available-report Running and interpreting one of these reports would be a fine class project. You can convert any of your R-based analyses to the soilReports format. The soilReports R package is essentially just a collection of R Markdown (.Rmd) reports, each with a “manifest” that describes any required dependencies, configuration files or inputs. R Markdown is a markup format for creating reproducible, dynamic reports with R. It allows R code and text to be mingled in the same document and executed like an R script. This allows R to generate reports similar to, and even more powerful than, NASIS. Let’s demonstrate how to run an existing .Rmd to summarize laboratory data for a soil series. 4.9.1 Requirements Data are properly populated, otherwise the report may fail. Common examples include: - Horizon depths don’t lineup - Either the Pedon or Site tables isn’t loaded ODBC connection to NASIS is setup Beware each report has a unique configuration file that may need to be edited. 4.9.2 Instructions Load your NASIS selected set. Run a query such as “POINT - Pedon/Site/NCSSlabdata by upedonid and Taxon Name” from the Region 11 report folder to load your selected set. Be sure to target both the site, pedon and lab layer tables. Remove from your selected set the pedons and sites you wish to exclude from the report. Install/re-install the soilReports package. This package is updated regularly (e.g. weekly), and should be installed from GitHub regularly. # Install the soilReports package from GitHub remotes::install_github(&quot;ncss-tech/soilReports&quot;, dependencies=FALSE, build=FALSE) View the list of available reports. # Load the soilReports library(soilReports) # List reports listReports() ## name version ## 1 region11/component_summary_by_project 0.1 ## 2 region11/lab_summary_by_taxonname 1.0 ## 3 region11/mupolygon_summary_by_project 0.1 ## 4 region11/pedon_summary_by_taxonname 1 ## 5 region2/dmu-diff 0.7 ## 6 region2/mlra-comparison-dynamic 0.1 ## 7 region2/mlra-comparison 1.0 ## 8 region2/mu-comparison-dashboard 0.0.0 ## 9 region2/mu-comparison 3.4.0 ## 10 region2/mu-summary 1 ## 11 region2/pedon-summary 0.9 ## 12 region2/QA-summary 0.5 ## 13 region2/shiny-pedon-summary 1.0 ## description ## 1 summarize component data for an MLRA project ## 2 summarize lab data from NASIS Lab Layer table ## 3 summarize mupolygon layer from a geodatabase ## 4 summarize field pedons from NASIS pedon table ## 5 Differences between select DMU ## 6 compare MLRA/LRU-scale delineations, based on mu-comparison report ## 7 compare MLRA using pre-made, raster sample databases ## 8 interactively subset and summarize SSURGO data for input to `region2/mu-comparison` report ## 9 compare stack of raster data, sampled from polygons associated with 1-8 map units ## 10 summarize raster data for a large collection of map unit polygons ## 11 Generate summaries from pedons (NASIS) and associated GIS data. ## 12 QA Summary Report ## 13 Interactively subset and summarize NASIS pedon data from one or more map units Copy the lab summary to your working directory. copyReport(reportName = &quot;region11/lab_summary_by_taxonname&quot;, outputDir = &quot;C:/workspace2/lab_sum&quot;) Examine the report folder contents. The report is titled report.Rmd. Notice there are several other support files. The parameters for the report are contained in the config.R file. Check or create a genhz_rules file for a soil series. In order to aggregate the pedons by horizon designation, a genhz_rules file (e.g., Miami_rules.R) is needed. See above. If none exists see the following job aid on how to create one, Assigning Generalized Horizon Labels. Pay special attention to how caret ^ and dollar $ symbols are used in REGEX. They function as anchors for the beginning and end of the string, respectively. A ^ placed before an A horizon, ^A, will match any horizon designation that starts with A, such as Ap, Ap1, but not something merely containing A, such as BA. Placing a $ after a Bt horizon, Bt$, will match any horizon designation that ends with Bt, such as 2Bt or 3Bt, but not something with a vertical subdivision, such as Bt2. Wrapping pattern with both ^ and $ symbols will result only in exact matches – i.e. that start and end with the contents between ^ and $. For example ^[AC]$, will only match A or C, not Ap, Ap2, or Cg. Execute the report. Command-line approach # Set report parameters series &lt;- &quot;Miami&quot; genhz_rules &lt;- &quot;C:/workspace2/lab_sum/Miami_rules.R&quot; # report file path report_path &lt;- &quot;C:/workspace2/lab_sum/report.Rmd&quot; # Run the report render(input = report_path, output_dir = &quot;C:/workspace2&quot;, output_file = &quot;C:/workspace2/lab_sum.html&quot;, envir = new.env(), params = list(series = series, genhz_rules = genhz_rules ) ) Manual approach Open the report.Rmd, hit the Knit drop down arrow, and select Knit with Parameters. Save the report. The report is automatically saved upon creation in the same folder as the R report. However, it is given the same generic name as the R report (i.e., “C:/workspace/lab_sum/report.html”), and will be overwritten the next time the report is run. Therefore, if you wish to save the report, rename the .html file to a name of your choosing and/or convert it to a PDF. Also, beware when opening the .html file with Internet Explorer – be sure to click on “Allow blocked content” if prompted. Otherwise, Internet Explorer will alter the formatting of tables etc. within the document. Sample pedon report 4.9.3 Exercise: run the region11/lab_summary_by_taxonname Load your selected set with the pedon and site table for an existing GHL file, or make your own (highly encouraged) Run the lab_summary_by_taxonname.Rmd report on a soil series of your choice. Show your work and submit the results to your mentor. 4.9.4 Exercise: Run Mapunit Comparison Another popular report in soilReports is the region2/mu-comparison report. This report uses constant density sampling (sharpshootR::constantDensitySampling()) to extract numeric and categorical values from multiple raster data sources that overlap a set of user-supplied polygons. In this example, we clip a small portion of SSURGO polygons from the CA630 soil survey area extent. We then select a small set of mapunit symbols (5012, 3046, 7083, 7085, 7088) that occur within the clipping extent. These mapunits have soil forming factors we expect to contrast with one another in several ways. You can inspect other mapunit symbols by changing mu.set in config.R. Download the demo data: # set up ch4 path and path for report ch4.data.path &lt;- &quot;C:/workspace2/chapter4&quot; ch4.mucomp.path &lt;- paste0(ch4.data.path,&quot;/mucomp&quot;) # create any directories that may be missing if(!dir.exists(ch4.mucomp.path)) { dir.create(ch4.mucomp.path, recursive = TRUE) } # download raster data, SSURGO clip from CA630, and sample script for clipping your own raster data download.file(&#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_4-mucomp-data/ch4_mucomp-data.zip&#39;, paste0(ch4.mucomp.path, &#39;/chapter_4-mucomp-data.zip&#39;)) unzip(paste0(ch4.mucomp.path, &#39;/chapter_4-mucomp-data.zip&#39;), exdir = ch4.mucomp.path, overwrite = TRUE) Create an instance of the region2/mu-comparison report with soilReports: # create new instance of reports library(soilReports) # get report dependencies reportSetup(&#39;region2/mu-comparison&#39;) # create report instance copyReport(&#39;region2/mu-comparison&#39;, outputDir = ch4.mucomp.path, overwrite = TRUE) If you want, you can now set up the default config.R that is created by copyReport() to work with your own data. OR you can use the “sample” config.R file (called new_config.R) in the ZIP file downloaded above. Run the code below to replace the default config.R with the sample config.R: # copy config file containing relative paths to rasters downloaded above file.copy(paste0(ch4.mucomp.path, &quot;/new_config.R&quot;), paste0(ch4.mucomp.path,&quot;/config.R&quot;), overwrite = TRUE) Open report.Rmd in the C:/workspace2/chapter4/mucomp folder and click the “Knit” button at the top of the RStudio source pane to run the report. Inspect the report output HTML file, as well as the spatial and tabular data output in the output folder. Question: What are the major differences that you can see, based on the report, between the five different mapunit symbols that were analysed? 4.9.5 Exercise: Run Shiny Pedon Summary The region2/shiny-pedon-summary report is an interactive Shiny-based report that uses flexdashboard to help the user subset and summarize NASIS pedons from a graphical interface. This allows one to rapidly generate reports from a large set of pedons in their NASIS selected set. The left INPUT sidebar has numerous options for subsetting pedon data. Specifically, you can change REGEX patterns for mapunit symbol, taxon name, local phase, and User Pedon ID. Also, you can use the drop down boxes to filter on taxon kind or compare different “modal”/RV pedons. Example: Analyzing the Loafercreek Taxadjuncts Create an instance of the region2/shiny-pedon-summary report with soilReports: # create new instance of reports library(soilReports) # set path for shiny-pedon-summary report instance ch4.shinyped.path &lt;- &quot;C:/workspace2/chapter4/shiny-pedon&quot; # create directories (if needed) if(!dir.exists(ch4.shinyped.path)) dir.create(ch4.shinyped.path, recursive = TRUE) # get report dependencies reportSetup(&#39;region2/shiny-pedon-summary&#39;) # copy report contents to target path copyReport(&#39;region2/shiny-pedon-summary&#39;, outputDir = ch4.shinyped.path, overwrite = TRUE) Update the config.R file You can update the config.R file in “C:/workspace2/chapter4/shiny-pedon” (or wherever you installed the report) to use the soilDB datasets loafercreek and gopheridge by setting demo_mode &lt;- TRUE. This is the simplest way to demonstrate how this report works. Alternately, when demo_mode &lt;- FALSE, pedons will be loaded from your NASIS selected set. config.R also allows you to specify a shapefile for overlaying the points on – to determine mapunit symbol – as well as several raster data sources whose values will be extracted at point locations and summarized. The demo dataset does not use either of these by default, due to large file sizes. Furthermore, a default (very general) set of REGEX generalized horizon patterns is provided to assign generalized horizon labels for provisional grouping. These provided patterns are unlikely to cover ALL cases, and essentially always need to be modified for final correlation. That said, they do a decent job of making a first-pass correlation for diverse types of soils. The default config.R settings use the general patterns: use_regex_ghz &lt;- TRUE. You are welcome to modify the defaults. If you want to use the values you have populated in NASIS Pedon Horizon Component Layer ID, set use_regex_ghz &lt;- FALSE. Run the report via shiny.Rmd This report uses the Shiny flexdashboard interface. Open up shiny.Rmd and click the “Run Document” button to start the report. This will load the pedon and spatial data specified in config.R. NOTE: If a Viewer Window does not pop-up right away, click the gear icon to the right of the “Run Document” button. Be sure the option “Preview in Window” is checked, then click “Run Document” again. All of the subsetting parameters are in the left-hand sidebar. Play around with all of these options – the graphs and plots in the tabs to the right will automatically update as you make changes. When you like what you have, you can export a non-interactive HTML file for your records. To do this, first, set the “Report name:” box to something informative – this will be your report output file name. Then, scroll down to the bottom of the INPUT sidebar and click “Export Report” button. Check the “output” folder (subdirectory of where you installed the report) for your results. 4.10 Customized Queries to Local NASIS Database Queries against the NASIS local database can be written in T-SQL which is the dialect of SQL used to communicate with Microsoft SQL Server that you set up a connection to for the pre-course. The fetchNASIS and related convenience functions are essentially wrappers around commonly used chunks of SQL. The following example will return all records from the sitesoiltemp table, along with a couple of fields from the site, siteobs, and pedon tables. This is a convenient way to collect all of the field-based soil temperature data associated with the pedons in your selected set for further analysis. 4.10.1 RODBC package This example uses RODBC. library(soilDB) library(RODBC) # write query as a character string q &lt;- &quot;SELECT siteiid as siteiid, peiid, usiteid as site_id, upedonid as pedon_id, obsdate as obs_date, soitemp, soitempdep FROM site_View_1 INNER JOIN siteobs_View_1 ON site_View_1.siteiid = siteobs_View_1.siteiidref LEFT OUTER JOIN sitesoiltemp_View_1 ON siteobs_View_1.siteobsiid = sitesoiltemp_View_1.siteobsiidref LEFT OUTER JOIN pedon_View_1 ON siteobs_View_1.siteobsiid = pedon_View_1.siteobsiidref ORDER BY obs_date, siteiid;&quot; # setup connection local NASIS channel &lt;- odbcDriverConnect(connection = getOption(&quot;soilDB.NASIS.credentials&quot;)) # exec query d &lt;- sqlQuery(channel, q, stringsAsFactors = FALSE) # close connection odbcClose(channel) # check results str(d) # remove records missing values d &lt;- na.omit(d) # tabulate unique soil depths table(d$soitempdep) # extract doy of year d$doy &lt;- as.integer(format(d$obs_date, &quot;%j&quot;)) # when where measurements collected? hist( d$doy, xlim = c(1, 366), breaks = 30, las = 1, main = &#39;Soil Temperature Measurements&#39;, xlab = &#39;Day of Year&#39; ) # soil temperature by day of year plot( soitemp ~ doy, data = d, type = &#39;p&#39;, xlim = c(1, 366), ylim = c(-1, 25), xlab = &#39;Day of Year&#39;, ylab = &#39;Soil Temperature at 50cm (deg C)&#39;, las = 1 ) 4.10.2 DBI You should know that the DBI and odbc packages provide same functions as RODBC via a more generic interface. There are many other drivers available for DBI, for example RSQLite for SQLite databases. This will soon be the database framework used in soilDB for NASIS access. "],
["exploratory-data-analysis.html", "Chapter 5 Exploratory Data Analysis 5.1 Objectives 5.2 Statistics 5.3 Data Inspection 5.4 Descriptive Statistics 5.5 Graphical Methods 5.6 Transformations 5.7 References 5.8 Additional Reading", " Chapter 5 Exploratory Data Analysis Before embarking on developing statistical models and generating predictions, it is essential to understand your data. This is typically done using conventional numerical and graphical methods. John Tukey (Tukey, 1977) advocated the practice of exploratory data analysis (EDA) as a critical part of the scientific process. “No catalog of techniques can convey a willingness to look for what can be seen, whether or not anticipated. Yet this is at the heart of exploratory data analysis. The graph paper and transparencies are there, not as a technique, but rather as a recognition that the picture examining eye is the best finder we have of the wholly unanticipated.” Fortunately, we can dispense with the graph paper and transparencies and use software that makes routine work of developing the ‘pictures’ (i.e., graphical output) and descriptive statistics needed to explore our data. 5.1 Objectives Review methods for estimating Low, RV, and High values Review different methods for visualing soil data Review data transformations 5.2 Statistics Descriptive statistics include: Mean - arithmetic average Median - middle value Mode - most frequent value Standard Deviation - variation around the mean Interquartile Range - range encompasses 50% of the values Kurtosis - peakedness of the data distribution Skewness - symmetry of the data distribution Graphical methods include: Histogram - a bar plot where each bar represents the frequency of observations for a given range of values Density estimation - an estimation of the frequency distribution based on the sample data Quantile-quantile plot - a plot of the actual data values against a normal distribution Box plots - a visual representation of median, quartiles, symmetry, skewness, and outliers Scatter plots - a graphical display of one variable plotted on the x axis and another on the y axis Radial plots - plots formatted for the representation of circular data 5.3 Data Inspection Before you start an EDA, you should inspect your data and correct all typos and blatent errors. EDA can then be used to identify additional errors such as outliers and help you determine appropriate statistical analyses. For this chapter we’ll use the loafercreek dataset from the CA630 Soil Survey Area. library(aqp) library(soilDB) # Load from the the loakercreek dataset data(&quot;loafercreek&quot;) # Construct generalized horizon designations n &lt;- c(&quot;A&quot;, &quot;BAt&quot;, &quot;Bt1&quot;, &quot;Bt2&quot;, &quot;Cr&quot;, &quot;R&quot;) # REGEX rules p &lt;- c(&quot;A&quot;, &quot;BA|AB&quot;, &quot;Bt|Bw&quot;, &quot;Bt3|Bt4|2B|C&quot;, &quot;Cr&quot;, &quot;R&quot;) # Compute genhz labels and add to loafercreek dataset loafercreek$genhz &lt;- generalize.hz(loafercreek$hzname, n, p) # Extract the horizon table h &lt;- horizons(loafercreek) # Examine the matching of pairing of the genhz label to the hznames table(h$genhz, h$hzname) As noted in Chapter 1, a visual examination of the raw data is possible by clicking on the dataset in the environment tab, or via commandline: View(h) This view is fine for a small dataset, but can be cumbersome for larger ones. The summary() function can be used to quickly summarize a dataset however, even for our small example dataset, the output can be voluminous. Therefore in the interest of saving space we’ll only look at a sample of columns. vars &lt;- c(&quot;genhz&quot;, &quot;clay&quot;, &quot;total_frags_pct&quot;, &quot;phfield&quot;, &quot;effclass&quot;) summary(h[, vars]) The summary() function is known as a generic R function. It will return a preprogrammed summary for any R object. Because h is a data frame, we get a summary of each column. Factors will be summarized by their frequency (i.e., number of observations), while numeric or integer variables will print out a five number summary, and characters simply print their length. The number of missing observations for any variable will also be printed if they are present. If any of these metrics look unfamiliar to you, don’t worry we’ll cover them shortly. When you do have missing data and the function you want to run will not run with missing values, the following options are available: Exclude all rows or columns that contain missing values using the function na.exclude(), such as h2 &lt;- na.exclude(h). However this can be wasteful because it removes all rows (e.g., horizons), regardless if the row only has 1 missing value. Instead it’s sometimes best to create a temporary copy of the variable in question and then remove the missing variables, such as clay &lt;- na.exclude(h$clay). Replace missing values with another value, such as zero, a global constant, or the mean or median value for that column, such as h$clay &lt;- ifelse(is.na(h$clay), 0, h$clay) # or h[is.na(h$clay), ] &lt;- 0. Read the help file for the function you’re attempting to use. Many functions have additional arguments for dealing with missing values, such as na.rm. A quick check for typos would be to examine the list of levels for a factor or character, such as: # just for factors levels(h$genhz) # for characters and factors sort(unique(h$hzname)) If the unique() function returned typos such as “BT” or “B t”, you could either fix your original dataset or you could make an adjustment in R, such as: h$hzname &lt;- ifelse(h$hzname == &quot;BT&quot;, &quot;Bt&quot;, h$hzname) # or h$hzname[h$hzname == &quot;BT&quot;] &lt;- &quot;Bt&quot; # or as a last resort we could manually edit the spreadsheet in R edit(h) Typo errors such as these are a common problem with old pedon data in NASIS. 5.3.1 Exercise: fetch and inspect Load the gopheridge dataset found within the soilDB package or use your own data (highly encouraged) and inspect the dataset Apply the generalized horizon rules below or develop your own, see the following job-aid Summarize the depths, genhz, texture class, sand, and fine gravel. Show your work and submit the results to your mentor. # gopheridge rules n &lt;- c(&#39;A&#39;, &#39;Bt1&#39;, &#39;Bt2&#39;, &#39;Bt3&#39;,&#39;Cr&#39;,&#39;R&#39;) p &lt;- c(&#39;^A|BA$&#39;, &#39;Bt1|Bw&#39;,&#39;Bt$|Bt2&#39;, &#39;Bt3|CBt$|BCt&#39;,&#39;Cr&#39;,&#39;R&#39;) 5.4 Descriptive Statistics 5.4.1 Measures of Central Tendency These measures are used to determine the mid-point of the range of observed values. In NASIS speak this should ideally be equivalent to the representative value (RV) for numeric and integer data. The mean and median are the most commonly used measures for our purposes. Mean - is the arithmetic average all are familiar with, formally expressed as: \\(\\bar{x} =\\frac{\\sum_{i=1}^{n}x_i}{n}\\) which sums ( \\(\\sum\\) ) all the X values in the sample and divides by the number (n) of samples. It is assumed that all references in this document refer to samples rather than a population. The mean clay content from the loafercreek dataset may be determined: # first remove missing values and create a new vector clay &lt;- na.exclude(h$clay) mean(clay) # or use the additional na.rm argument mean(h$clay, na.rm = TRUE) Median is the middle measurement of a sample set, and as such is a more robust estimate of central tendency than the mean. This is known as the middle or 50th quantile, meaning there are an equal number of samples with values less than and greater than the median. For example, assuming there are 21 samples, sorted in ascending order, the median would be the 11th sample. The median from the sample dataset may be determined: median(clay) Mode - is the most frequent measurement in the sample. The use of mode is typically reserved for factors, which we will discuss shortly. One issue with using the mode for numeric data is that the data need to be rounded to the level of desired precision. R does not include a function for calculating the mode, but we can calculate it using the following example. sort(table(round(h$clay)), decreasing = TRUE)[1] # sort and select the 1st value, which will be the mode Frequencies To summarize factors and characters we can examine their frequency or number of observations. This is accomplished using the table() or summary() functions. table(h$genhz) # or summary(h$genhz) This gives us a count of the number of observations for each horizon. If we want to see the comparison between two different factors or characters, we can include two variables. table(h$genhz, h$texcl) We can also add margin totals to the table or convert the table frequencies to proportions. # append the table with row and column sums addmargins(table(h$genhz, h$texcl)) # calculate the proportions relative to the rows, margin = 1 calculates for rows, margin = 2 calculates for columns, margin = NULL calculates for total observations round(prop.table(table(h$genhz, h$texture_class), margin = 1) * 100) knitr::kable(addmargins(table(h$genhz, h$texcl))) knitr::kable(round(prop.table(table(h$genhz, h$texture_class), margin = 1) * 100)) To determine the mean by a group or category, use the aggregate function: aggregate(clay ~ genhz, data = h, mean) To determine the median by group or category, use the aggregate command again: aggregate(clay ~ genhz, data = h, median) # or we could use the summary() function to get both the mean and median aggregate(clay ~ genhz, data = h, summary) 5.4.2 Measures of Dispersion These are measures used to determine the spread of values around the mid-point. This is useful to determine if the samples are spread widely across the range of observations or concentrated near the mid-point. In NASIS speak these values might equate to the low (L) and high (H) values for numeric and integer data. Variance is a positive value indicating deviation from the mean: \\(s^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} {n - 1}\\) This is the square of the sum of the deviations from the mean, divided by the number of samples minus 1. It is commonly referred to as the sum of squares. As the deviation increases, the variance increases. Conversely, if there is no deviation, the variance will equal 0. As a squared value, variance is always positive. Variance is an important component for many statistical analyses including the most commonly referred to measure of dispersion, the standard deviation. Variance for the sample dataset is: var(h$clay, na.rm=TRUE) Standard Deviation is the square root of the variance: \\(s = \\sqrt\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} {n - 1}\\) The units of the standard deviation are the same as the units measured. From the formula you can see that the standard deviation is simply the square root of the variance. Standard deviation for the sample dataset is: sd(h$clay, na.rm = TRUE) # or # sqrt(var(clay)) Coefficient of Variation (CV) is a relative (i.e., unitless) measure of standard deviation: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\) CV is calculated by dividing the standard deviation by the mean and multiplying by 100. Since standard deviation varies in magnitude with the value of the mean, the CV is useful for comparing relative variation amongst different datasets. However Webster (2001) discourages using CV to compare different variables. Webster (2001) also stresses that CV is reserved for variables that have an absolute 0, like clay content. CV may be calculated for the sample dataset as: cv &lt;- sd(clay) / mean(clay) * 100 cv Quantiles (a.k.a. Percentiles) - the percentile is the value that cuts off the first nth percent of the data values when sorted in ascending order. The default for the quantile() function returns the min, 25th percentile, median or 50th percentile, 75th percentile, and max, known as the five number summary originally proposed by Tukey. Other probabilities however can be used. At present the 5th, 50th, and 95th are being proposed for determining the range in characteristics (RIC) for a given soil property. quantile(clay) # or quantile(clay, c(0.05, 0.5, 0.95)) Thus, for the five number summary 25% of the observations fall between each of the intervals. Quantiles are a useful metric because they are largely unaffected by the distribution of the data, and have a simple interpretation. Range is the difference between the highest and lowest measurement of a group. Using the sample data it may be determined as: range(clay) which returns the minimum and maximum values observed, or: diff(range(clay)) # or max(clay) - min(clay) Interquartile Range (IQR) is the range from the upper (75%) quartile to the lower (25%) quartile. This represents 50% of the observations occurring in the mid-range of a sample. IQR is a robust measure of dispersion, unaffected by the distribution of data. In soil survey lingo you could consider the IQR to estimate the central concept of a soil property. IQR may be calculated for the sample dataset as: IQR(clay) # or # diff(quantile(clay, c(0.25, 0.75))) 5.4.3 Correlation A correlation matrix is a table of the calculated correlation coefficients of all variables. This provides a quantitative measure to guide the decision making process. The following will produce a correlation matrix for the sp4 dataset: h$hzdepm &lt;- (h$hzdepb + h$hzdept) / 2 # Compute the middle horizon depth vars &lt;- c(&quot;hzdepm&quot;, &quot;clay&quot;, &quot;sand&quot;, &quot;total_frags_pct&quot;, &quot;phfield&quot;) round(cor(h[, vars], use = &quot;complete.obs&quot;), 2) As seen in the output, variables are perfectly correlated with themselves and have a correlation coefficient of 1.0. Negative values indicate a negative relationship between variables. What is considered highly correlated? A good rule of thumb is anything with a value of 0.7 or greater is considered highly correlated. 5.5 Graphical Methods Now that we’ve checked for missing values and typos and made corrections, we can graphically examine the sample data distribution of our data. Frequency distributions are useful because they can help us visualize the center (e.g., RV) and spread or dispersion (e.g., low and high) of our data. Typically in introductory statistics the normal (i.e., Gaussian) distribution is emphasized. 5.5.1 Distributions 5.5.2 Bar Plot A bar plot is a graphical display of the frequency (i.e. number of observations (count or n)), such as soil texture, that fall within a given class. It is a graphical alternative to to the table() function. library(ggplot2) # bar plot ggplot(h, aes(x = texcl)) + geom_bar() 5.5.3 Histogram A histogram is similar to a bar plot, except that instead of summarizing categorical data, it categorizes a continuous variable like clay content into non-overlappying intervals for the sake of display. The number of intervals can be specified by the user, or can be automatically determined using an algorithm, such as nclass.Sturges(). Since histograms are dependent on the number of bins, for small datasets they’re not the best method of determining the shape of a distribution. ggplot(h, aes(x = clay)) + geom_histogram(bins = nclass.Sturges(h$clay)) 5.5.4 Density Curve A density estimation, also known as a Kernel density plot, generally provides a better visualization of the shape of the distribution in comparison to the histogram. Compared to the histogram where the y-axis represents the number or percent (i.e., frequency) of observations, the y-axis for the density plot represents the probability of observing any given value, such that the area under the curve equals one. One curious feature of the density curve is the hint of a two peaks (i.e. bimodal distribution?). Given that our sample includes a mixture of surface and subsurface horizons, we may have two different populations. However considering how much the two distributions overlap, it seems impractical to separate them in this instance. ggplot(h, aes(x = clay)) + geom_density() 5.5.5 Box plots Box plots are a graphical representation of the five number summary, depicting quartiles (i.e. the 25%, 50%, and 75% quantiles), minimum, maximum and outliers (if present). Boxplots convey the shape of the data distribution, the presence of extreme values, and the ability to compare with other variables using the same scale, providing an excellent tool for screening data, determining thresholds for variables and developing working hypotheses. The parts of the boxplot are shown in the figure below. The “box” of the boxplot is defined as the 1st quartile, (Q1 in the figure) and the 3rd quartile, (Q3 in the figure). The median, or 2nd quartile, is the dark line in the box. The whiskers (typically) show data that is 1.5 * IQR above and below the 3rd and 1st quartile. Any data point that is beyond a whisker is considered an outlier. That is not to say the outlier points are in error, just that they are extreme compared to the rest of the data set. However, you may want to evaluate these points to ensure that they are correct. Boxplot description (Seltman, 2009) ggplot(h, (aes(x = genhz, y = clay))) + geom_boxplot() The above box plot shows a steady increase in clay content with depth. Notice the outliers in the box plots, identified by the individual circles. 5.5.6 Quantile comparison plots (QQplot) A QQ plot is a plot of the actual data values against a normal distribution (which has a mean of 0 and standard deviation of 1). # QQ Plot for Clay ggplot(h, aes(sample = clay)) + geom_qq() + geom_qq_line() # QQ Plot for Frags ggplot(h, aes(sample = total_frags_pct)) + geom_qq() + geom_qq_line() If the data set is perfectly symmetric (i.e. normal), the data points will form a straight line. Overall this plot shows that our clay example is more or less symmetric. However the second plot shows that our rock fragments are far from evenly distributed. A more detailed explanation of QQ plots may be found on Wikipedia: https://en.wikipedia.org/wiki/QQ_plot 5.5.7 The ‘Normal’ distribution What is a normal distribution and why should you care? Many statistical methods are based on the properties of a normal distribution. Applying certain methods to data that are not normally distributed can give misleading or incorrect results. Most methods that assume normality are robust enough for all data except the very abnormal. This section is not meant to be a recipe for decision making, but more an extension of tools available to help you examine your data and proceed accordingly. The impact of normality is most commonly seen for parameters used by pedologists for documenting the ranges of a variable (i.e., Low, RV and High values). Often a rule-of thumb similar to: “two standard deviations” is used to define the low and high values of a variable. This is fine if the data are normally distributed. However, if the data are skewed, using the standard deviation as a parameter does not provide useful information of the data distribution. The quantitative measures of Kurtosis (peakedness) and Skewness (symmetry) can be used to assist in accessing normality and can be found in the fBasics package, but Webster (2001) cautions against using significance tests for assessing normality. The preceding sections and chapters will demonstrate various methods to cope with alternative distributions. A Gaussian distribution is often referred to as “Bell Curve”, and has the following properties (Lane): Gaussian distributions are symmetric around their mean The mean, median, and mode of a Gaussian distribution are equal The area under the curve is equal to 1.0 Gaussian distributions are denser in the center and less dense in the tails Gaussian distributions are defined by two parameters, the mean and the standard deviation 68% of the area under the curve is within one standard deviation of the mean Approximately 95% of the area of a Gaussian distribution is within two standard deviations of the mean Viewing a histogram or density plot of your data provides a quick visual reference for determining normality. Distributions are typically normal, Bimodal or Skewed: Examples of different types of distributions Occasionally distributions are Uniform, or nearly so: With the loafercreek dataset the mean and median for clay were only slightly different, so we can safely assume that we have a normal distribution. However many soil variables often have a non-normal distribution. For example, let’s look at graphical examination of the mean vs. median for clay and rock fragments: The solid lines represent the breakpoint for the mean and standard deviations. The dashed lines represents the median and quantiles. The median is a more robust measure of central tendency compared to the mean. In order for the mean to be a useful measure, the data distribution must be approximately normal. The further the data departs from normality, the less meaningful the mean becomes. The median always represents the same thing independent of the data distribution, namely, 50% of the samples are below and 50% are above the median. The example for clay again indicates that distribution is approximately normal. However for rock fragments, we commonly see a long tailed distribution (e.g., skewed). Using the mean in this instance would overestimate the rock fragments. Although in this instance the difference between the mean and median is only percent. 5.5.8 Scatterplots and Line Plots Plotting points of one ratio or interval variable against another is a scatter plot. Plots can be produced for a single or multiple pairs of variables. Many independent variables are often under consideration in soil survey work. This is especially common when GIS is used, which offers the potential to correlate soil attributes with a large variety of raster datasets. The purpose of a scatterplot is to see how one variable relates to another. With modeling in general the goal is parsimony (i.e., simple). The goal is to determine the fewest number of variables required to explain or describe a relationship. If two variables explain the same thing, i.e., they are highly correlated, only one variable is needed. The scatterplot provides a perfect visual reference for this. Create a basic scatter plot using the loafercreek dataset. # scatter plot ggplot(h, aes(x = clay, y = hzdepm)) + geom_point() + ylim(100, 0) # line plot ggplot(h, aes(y = clay, x = hzdepm, group = peiid)) + geom_line() + coord_flip() + xlim(100, 0) This plots clay on the X axis and depth on the X axis. As shown in the scatterplot above, there is a moderate correlation between these variables. The function below produces a scatterplot matrix for all the numeric variables in the dataset. This is a good command to use for determining rough linear correlations for continuous variables. # Load the GGally package library(GGally) # Create a scatter plot matrix vars &lt;- c(&quot;hzdepm&quot;, &quot;clay&quot;, &quot;phfield&quot;, &quot;total_frags_pct&quot;) ggpairs(h[vars]) 5.5.9 3rd Dimension - Color, Shape, Size, Layers, etc… 5.5.9.1 Color and Groups # scatter plot ggplot(h, aes(x = clay, y = hzdepm, color = genhz)) + geom_point(size = 3) + ylim(100, 0) # density plot ggplot(h, aes(x = clay, color = genhz)) + geom_density(size = 2) # bar plot ggplot(h, aes(x = genhz, fill = texture_class)) + geom_bar() # box plot ggplot(h, aes(x = genhz, y = clay)) + geom_boxplot() # heat map (pseudo bar plot) s &lt;- site(loafercreek) ggplot(s, aes(x = landform_string, y = pmkind)) + geom_tile(alpha = 0.2) 5.5.9.2 Facets - box plots # convert to long format df &lt;- reshape2::melt(h, id.vars = c(&quot;peiid&quot;, &quot;genhz&quot;, &quot;hzdepm&quot;), measure.vars = c(&quot;clay&quot;, &quot;phfield&quot;, &quot;total_frags_pct&quot;) ) ggplot(df, aes(x = genhz, y = value)) + geom_boxplot() + xlab(&quot;genhz&quot;) + facet_wrap(~ variable, scales = &quot;free_y&quot;) 5.5.9.3 Facets - depth plots library(aqp) s &lt;- slice(loafercreek, 0:100 ~ clay + phfield + total_frags_pct) s &lt;- slab(s, fm = ~ clay + phfield + total_frags_pct, slab.fun = function(x) quantile(x, c(0.1, 0.5, 0.9), na.rm = TRUE) ) ggplot(s, aes(x = top, y = X50.)) + geom_line() + geom_ribbon(aes(ymin = X10., ymax = X90., x = top), alpha = 0.2) + xlim(c(100, 0)) + coord_flip() + facet_wrap(~ variable, scales = &quot;free_x&quot;) 5.6 Transformations Slope aspect and pH are two common variables warranting special consideration for pedologists. 5.6.1 pH Since pH has a logarithmic distribution, the use of median and quantile ranges are the preferred measures when summarizing pH. Remember, pHs of 6 and 5 correspond to hydrogen ion concentrations of 0.000001 and 0.00001 respectively. The actual average is 5.26; -log10((0.000001 + 0.00001) / 2). If no conversions are made for pH, the mean and sd in the summary are considered the geometric mean and sd, not the arithmetic. The wider the pH range, the greater the difference between the geometric and arithmetic mean. The difference between the correct average of 5.26 and the incorrect of 5.5 is small, but proper handling of data types is a best practice. If you have a table with pH values and wish to calculate the arithmetic mean using R, this example will work: # arithmetic mean log10(mean(1/10^-h$phfield, na.rm = TRUE)) # geometric mean mean(h$phfield, na.rm = TRUE) 5.6.2 Circular data Slope aspect - requires the use of circular statistics for summarizing numerically, or graphical interpretation using circular plots. For example, if soil map units being summarized have a uniform distribution of slope aspects ranging from 335 degrees to 25 degrees, the Zonal Statistics tool in ArcGIS would return a mean of 180. The most intuitive means available for evaluating and describing slope aspect are circular plots available with the circular package in R and the radial plot option in the TEUI Toolkit. The circular package in R will also calculate circular statistics like mean, median, quartiles etc. library(circular) # Extract the site table s &lt;- site(loafercreek) aspect &lt;- s$aspect_field aspect &lt;- circular(aspect, template=&quot;geographic&quot;, units=&quot;degrees&quot;, modulo=&quot;2pi&quot;) summary(aspect) The numeric output is fine, but a following graphic is more revealing, which shows the dominant Southwest slope aspect. rose.diag(aspect, bins = 8, col=&quot;grey&quot;) 5.7 References FAO Corporate Document Repository. http://www.fao.org/docrep/field/003/AC175E/AC175E07.htm Lane, D.M. Online Statistics Education: A Multimedia Course of Study (http://onlinestatbook.com/ Project Leader: David M. Lane, Rice University Seltman, H. 2009. Experimental Design and Analysis. Chapter 4: Exploratory Data Analysis. Carnegie Mellon University. http://www.stat.cmu.edu/~hseltman/309/Book/ Tukey, John. 1977. Exploratory Data Analysis, Addison-Wesley Tukey, J. 1980. We need both exploratory and confirmatory. The American Statistician, 34:1, 23-25. Webster, R. 2001. Statistics to support soil research and their presentation. European Journal of Soil Science. 52:331-340. http://onlinelibrary.wiley.com/doi/10.1046/j.1365-2389.2001.00383.x/abstract 5.8 Additional Reading Healy, K., 2018. Data Visualization: a practical introduction. Princeton University Press. http://socviz.co/ Helsel, D.R., and R.M. Hirsch, 2002. Statistical Methods in Water Resources Techniques of Water Resources Investigations, Book 4, chapter A3. U.S. Geological Survey. 522 pages. http://pubs.usgs.gov/twri/twri4a3/ Kabacoff, R.I., 2015. R in Action. Manning Publications Co. Shelter Island, NY. https://www.statmethods.net/ Kabacoff, R.I., 2018. Data Visualization in R. https://rkabacoff.github.io/datavis/ Peng, R. D., 2016. Exploratory Data Analysis with R. Leanpub. https://bookdown.org/rdpeng/exdata/ Wilke, C.O., 2019. Fundamentals of Data Visualization. O’Reily Media, Inc. https://serialmentor.com/dataviz/ "],
["spatial-data-in-r.html", "Chapter 6 Spatial Data in R 6.1 Objectives 6.2 Spatial Data Sources 6.3 Many Packages, Many Spatial Representations 6.4 Coordinate Reference Systems 6.5 Exercises 6.6 Additional Reading", " Chapter 6 Spatial Data in R Most of us are familiar with spatial data types, sources, and the jargon used to describe interaction with these data. GIS software provides a convenient framework for most of the spatial analysis that we do, however, the combination of statistical routines, advanced graphics, and data access functionality make R an ideal environment for soil science. For example, with a couple of lines of R code, it is possible to quickly integrate soil morphology (NASIS), lab data (KSSL), map unit polygons (SSURGO), and climate data (PRISM raster files). This chapter is a very brief demonstration of several possible ways to process spatial data in R. 6.1 Objectives Gain experience with creating, editing, and exporting spatial data objects in R. Learn the basics of the sp classes and functions. Learn the basics of the raster classes and functions. Learn about some interfaces to NCSS spatial data sources. Develop a strategy for navigating the many possible spatial data processing methods. Learn how to integrate multiple data sources to create something new. There are many packages available for working with spatial data, however we only have time to cover the below libraries. The next couple of sections will require loading these libraries into the R session. library(aqp) library(soilDB) library(sf) library(sp) library(rgdal) library(raster) 6.2 Spatial Data Sources Conventional spatial data sources: raster data sources (elevation, PRISM, etc.): GeoTIFF, ERDAS, BIL, ASCII grid, WMS, … vector data sources (points/lines/polygons): Shape File, “file” geodatabase, KML, GeoJSON, GML, WFS, … Conventional data sources that can be upgraded to spatial data: NASIS/LIMS reports: typically site coordinates web pages: GeoJSON, WKT, or point coordinates Excel file: typically point coordinates CSV files: typically point coordinates R-based interfaces to NCSS data sources via soilDB package: functions that return tabular data which can be upgraded to spatial data: fetchKSSL(): KSSL “site” data contain x,y coordinates fetchNASIS(): NASIS “site” data contain x,y, coordinates fetchRaCA(): RaCA central pedon x,y coordinates functions that return spatial data: seriesExtent(): simplified series extent as polygons fetchHenry(): sensor / weather station locations as points SDA_query(): SSURGO data as points, lines, polygons (via SDA) mapunit_geom_by_ll_bbox(): SSURGO data as polygons (via WFS) NOTE: currently, there is no way to read raster data from ESRI “file” geodatabases. except for this: https://github.com/r-barnes/ArcRasterRescue, which requires ability to compile C code with CMAKE (non-government computer). Vector data (and associated attribute tables) can be accessed. 6.3 Many Packages, Many Spatial Representations 6.3.1 The sf package Simple Features Access is a set of standards that specify a common storage and access model of geographic features. It is used mostly for two-dimensional geometries such as point, line, polygon, multi-point, multi-line, etc. This is one of many ways of modeling the geometry of shapes in the real world. This model happens to be widely adopted in the R ecosystem via the sf package, and very convenient for typical data encountered by soil survey operations. The sf package represents the latest and greatest in spatial data processing within the comfort of an R session. It provides a “main” object class sf to contain geometric data and associated tabular data in a familiar data.frame format. sf methods work on a variety of different levels of abstraction and manipulation of those geometries. 6.3.1.1 sf vignettes Simple Features for R Reading, Writing and Converting Simple Features Manipulating Simple Feature Geometries Manipulating Simple Features Plotting Simple Features Miscellaneous Spherical geometry in sf using s2geometry 6.3.2 The sp Package The data structures (“classes”) and functions provided by the sp package have served a foundational role in the handling of spatial data in R for years. Many of the following examples will reference names such as SpatialPoints, SpatialPointsDataFrame, and SpatialPolygonsDataFrame. These are specialized (S4) classes, implemented by the sp package. Objects of these classes maintain linkages between all of the components of spatial data. For example, a point, line, or polygon feature will typically be associated with: coordinate geometry bounding box coordinate reference system attribute table 6.3.3 Converting sp and sf sp provides access to the same compiled code libraries (PROJ, GDAL, GEOS) as sf, but mostly via the interfaces in the separate rgdal package. For certain applications, such as some packages we demonstrate below, there are no sp “interfaces” to the methods – only sf, or vice-versa. The two different categories of object types are interchangeable, and you may find yourself having to do this for a variety of reasons. You can convert between objects using sf::as_Spatial or sf::st_as_sf. Check the documentation (?functionname) to figure out what object types different methods need as input; and check an input object’s class with class() or inherits(). 6.3.4 Importing / Exporting Vector Data Import a feature class from a ESRI File Geodatabase or shape file. If you have a .shp file, you can specify the whole path, including the file extension in the dsn argument, or just the folder. For a Geodatabase, you should specify the feature class using the layer argument. Note that a trailing “/” is omitted from the dsn (data source name) and the “.shp” suffix is omitted from the layer. 6.3.4.1 sf x &lt;- sf::st_read(dsn = &#39;E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb&#39;, layer = &#39;ca630_a&#39;) sf::write_sf(x, dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;) 6.3.4.2 sp / rgdal Export object x to shapefile. x &lt;- rgdal::readOGR(dsn = &#39;E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb&#39;, layer = &#39;ca630_a&#39;) rgdal::writeOGR(x, dsn = &#39;E:/gis_data/ca630/pedon_locations.shp&#39;, driver = &#39;ESRI Shapefile&#39;) The st_read() / read_sf() / write_sf() and readOGR(), writeOGR(), readGDAL(), writeGDAL() functions have many arguments, so it is worth spending some time with the associated manual pages. If you have a .shp file, you can specify the whole path, including the file extension in the dsn argument. 6.3.5 Interactive mapping with mapview and leaflet These packages make it possible to display interactive maps of sf objects in RStudio, or within an HTML document generated via R Markdown (e.g. this document). mapview package leaflet package leafem: ‘leaflet’ Extensions for ‘mapview’ The seriesExtent method in soilDB returns results as an sp object showing generalized extent polygons for a soil series. # load required packages library(sf) library(mapview) library(leafem) # get series extents from SoilWeb pentz &lt;- seriesExtent(&#39;pentz&#39;) redding &lt;- seriesExtent(&#39;redding&#39;) # make a simple map m &lt;- mapview(st_as_sf(pentz)) # add more data to the map and display addFeatures(m, st_as_sf(redding), color=&#39;black&#39;, fillColor=&#39;red&#39;, weight=1) 6.3.6 The raster Package The raster package package provides most of the commonly used grid processing functionality that one might find in a conventional GIS: re-sampling / interpolation warping (coordinate system transformations of gridded data) cropping, mosaicing, masking local and focal functions raster algebra contouring raster/vector conversions terrain analysis model-based prediction (more on this in later chapters) Introduction to the raster package vignette 6.3.6.1 Importing / Exporting Rasters # use an example from the raster package f &lt;- system.file(&quot;external/test.grd&quot;, package = &quot;raster&quot;) # create a reference to this raster r &lt;- raster(f) # print the details print(r) # default plot method plot(r) The disk-based reference can be converted to an in-memory RasterLayer with the readAll() function. Processing of raster data in memory is always faster than processing on disk, as long as there is sufficient memory. # check: file is on disk inMemory(r) # load into memory, if possible r &lt;- readAll(r) # check: file is in memory inMemory(r) Exporting data requires consideration of the output format, datatype, encoding of NODATA, and other options such as compression. See the manual pages for writeRaster(), writeFormats(), and dataType() for details. For example, suppose you had a RasterLayer object that you wanted to save to disk as an internally-compressed GeoTIFF: # using previous example data set writeRaster(r, filename=&#39;r.tif&#39;, options = c(&quot;COMPRESS=LZW&quot;)) The writeRaster() function interprets the given (and missing) arguments as: ‘.tif’ suffix interpreted as format=GTiff creation options of “LZW compression” passed to GeoTiff driver default datatype default NAflag 6.3.6.2 Object Properties RasterLayer objects are similar to sf and sp objects in that they keep track of the linkages between data, coordinate reference system, and optional attribute tables. Getting and setting the contents of RasterLayer objects should be performed using functions such as: NAvalue(r): get / set the NODATA value crs(r) or proj4string(r): get / set the coordinate reference system res(r): get / set the resolution extent(r): get / set the extent dataType(r): get / set the data type … many more, see the raster package manual 6.3.6.3 Fast Raster Sampling and Aggregation with exactextractr This example shows how to determine the distribution of Frost-Free Days across a soil series extent. The data are extracted from the raster data source very rapidly using the exactextractr package. library(sf) library(soilDB) library(raster) library(lattice) library(exactextractr) # 5-10 seconds to download Series Extent Explorer data s &lt;- seriesExtent(&#39;san joaquin&#39;) # load pointer to PRISM data r &lt;- raster(&#39;C:/workspace/chapter-2b/FFD.tif&#39;) # transform extent to CRS of raster s &lt;- st_transform(st_as_sf(s), crs = st_crs(r)) # inspect s # use `st_union(s)` to create a MULTI- POINT/LINE/POLYGON from single # use `sf::st_cast(s, &#39;POLYGON&#39;)` to create other types # &lt;0.4 seconds for sampling, including coverage fractions! system.time({ ex &lt;- exact_extract(r, s) }) # ex is a list(), with data.frame [value, coverage_fraction] # for each polygon in s, we only have one, so we don&#39;t need this # combine all list elements into single data.frame (row-wise) # ex.all &lt;- do.call(&#39;rbind&#39;, ex) # or, just take the first with MULTIPOLYGON ex.all &lt;- ex[[1]] # simple summary densityplot(ex.all$value, plot.points=FALSE, bw = 2, lwd = 2, col = &#39;RoyalBlue&#39;, ylab = &#39;Density&#39;, from = 0, to = 400, xlab = &#39;Frost-Free Days (50% chance)\\n800m PRISM Data (1981-2010)&#39;, main = &#39;FFD Estimate for Extent of San Joaquin Series&#39;) 6.4 Coordinate Reference Systems Spatial data aren’t all that useful without an accurate description of the coordinate reference system (CRS). This type of information is typically stored within the “.prj” component of a shapefile, or in the header of a GeoTIFF. Without a CRS it is not possible to perform coordinate transformations (e.g. conversion of geographic coordinates to projected coordinates), spatial overlay (e.g. intersection), or geometric calculations (e.g. distance or area). The “old” way (PROJ.4) of specifying coordinate reference systems is using character strings containing, for example: +proj or +init arguments. In general, this still “works,” so you may encounter it and need to know about it. But you also may encounter cases where CRS are specified using integer EPSG codes, OGC codes or well-known text (WKT). Some common examples of coordinate system EPSG codes and their legacy PROJ.4 strings: EPSG: 4269 / PROJ.4:+proj=longlat +datum=NAD83 - geographic, NAD83 datum EPSG: 4267 / PROJ.4:+proj=longlat +datum=NAD27 - geographic, NAD27 datum EPSG: 26910 / PROJ.4:+proj=utm +zone=10 +datum=NAD83 - projected (UTM zone 10), NAD83 datum EPSG: 6350 / PROJ.4: +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23.0 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs - Albers Equal Area CONUS (Used for gSSURGO) While you may encounter PROJ.4 strings, these are no longer considered the preferred method of referencing Coordinate Reference Systems – and, in general, newer methods are “easier.” Well-known text (WKT) is a human- machine-readable standard format for geometry, so storing the Coordinate Reference System information in a similar format makes sense. This format is returned by the sf::st_crs method. For example: the WKT representation of EPSG:4326: st_crs(4326) This is using the OGC WKT CRS standard. Adoption of this standard caused some significant changes in packages in the R ecosystem. So you can get familiar, what follows are several examples of doing the same thing: setting the CRS of spatial objects with WGS84 longitude/latitude geographic coordinates. If you have another target coordinate system, it is just a matter of using the correct codes to identify it. 6.4.1 Assigning and Transforming Coordinate Systems Returning to the example from above, lets assign a CRS to our series extent s using different methods. s &lt;- seriesExtent(&#39;san joaquin&#39;) The following are equivalent sf versus sp/rgdal syntax. 6.4.1.1 sf Use st_crs&lt;-/st_crs to set or get CRS of sf objects. Supply the target EPSG code as an integer. # s is an sf object s &lt;- st_as_sf(s) st_crs(s) &lt;- sf::st_crs(4326) Transformation of points, lines, and polygons with sf just requires that origin be defined in the object that is the argument x, and target CRS defined in crs argument as an integer, or output of st_crs(). # transform to UTM zone 10 s.utm &lt;- st_transform(x = s, crs = 26910) # transform to GCS NAD27 s.nad27 &lt;- st_transform(x = s, crs = st_crs(4267)) 6.4.1.2 sp / rgdal Equivalent EPSG, OGC and PROJ.4 can be set or get using proj4string&lt;-/proj4string and either a sp CRS object or a PROJ.4 string for Spatial objects. # s is a Spatial* object s &lt;- as_Spatial(s) # these all create the same internal sp::CRS object proj4string(s) &lt;- sp::CRS(&#39;EPSG:4326&#39;) # proj &gt;6; EPSG proj4string(s) &lt;- sp::CRS(&#39;OGC:CRS84&#39;) # proj &gt;6; OGC proj4string(s) &lt;- &#39;+init=epsg:4326&#39; # proj4 style +init string proj4string(s) &lt;- &#39;+proj=longlat +datum=WGS84&#39; # proj4 style +proj string Here, we do the same transformations we did above only using sp: spTransform(). # transform to UTM zone 10 s.utm &lt;- spTransform(s, CRS(&#39;+proj=utm +zone=10 +datum=NAD83&#39;)) # transform to GCS NAD27 s.nad27 &lt;- spTransform(s, CRS(&#39;+proj=longlat +datum=NAD27&#39;)) 6.4.1.3 raster Use crs&lt;-/crs for raster or Spatial objects, it takes as argument and returns a sp CRS object. # r is a raster object; set CRS as the CRS of itself crs(r) &lt;- raster::crs(sp::CRS(r)) “Transforming” or warping a raster is a different matter than a vector as it requires interpolation of pixels to a defined target resolution and CRS. The method provided by raster to do this is projectRaster(). It works the same as the above transform methods in that you specify an object to transform, and the target reference system or a template for the object. r.wgs84 &lt;- projectRaster(r, CRS(&quot;EPSG:4326&quot;)) Note that the default projectRaster uses bilinear interpolation (method='bilinear'), which is appropriate for continuous variables. You also have the option of using nearest-neighbor (method='ngb') for categorical variables (class maps) where interpolation does not make sense. If we want to save this transformed raster to file, we can use something like this: writeRaster(r, filename=&#39;r.tif&#39;, options=c(&quot;COMPRESS=LZW&quot;)) 6.4.1.4 Related Links sf package website rspatial.org - Spatial Data Science with R Goodbye PROJ.4 strings! How to specify a coordinate reference system in R? R Advanced Spatial Lessons by Ben Best Spatial Data in R by Pierre Roudier 6.4.2 Spatial Overlay Operations Spatial data are lot more useful when “combined” (overlay, intersect, spatial query, etc.) to generate something new. For simplicity, we will refer to this kind of operation as an “extraction”. The attribute data in one sp object can be extracted from another using the over() function, as long as the CRS are the same. Access the associated vignette by pasting vignette(\"over\") in the console. # hand make a SpatialPoints object # note that this is GCS p &lt;- SpatialPoints(coords = cbind(-120, 37.5), proj4string = CRS(&#39;+proj=longlat +datum=WGS84&#39;)) # spatial extraction of MLRA data requires a CRS transformation p.aea &lt;- spTransform(p, proj4string(mlra)) over(p.aea, mlra) The values stored in a RasterLayer or RasterStack object can be extracted using the extract() function. As long a the “query” feature has a valid CRS defined, the extract() function will automatically perform any required CRS transformation. # extract from a single RasterLayer extract(maat, p) # extract from a RasterStack extract(rs, p) The extract() function can perform several operations in one pass, such as buffering (in projected units) then extracting. See the manual page for an extensive listing of optional arguments and what they do. # extract using a buffer with radius specified in meters (1000m) extract(rs, p, buffer=1000) 6.4.3 Raster Data Sampling Typically, spatial queries of raster data by polygon features are performed in two ways: for each polygon, collect all pixels that overlap for each polygon, collect a subset of pixels defined by sampling points The first method ensures that all data are included in the analysis, however, processing is very slow and the results may not fit into memory. The second method can be far more computationally efficient (10-100x faster), require less memory, and remain statistically sound–as long as a reasonable sampling strategy is applied. A detailed description of sampling strategies in chapter 3. 6.5 Exercises 6.5.1 Example Data Load required packages into the current R Session. library(aqp) library(sp) library(raster) library(rgdal) library(soilDB) 6.5.1.1 Download Example Data If you haven’t yet done so, please setup the sample data sets for this module. It will not take too long to download. # store path as a variable, in case you want to keep it somewhere else ch2b.data.path &lt;- &#39;C:/workspace/chapter-2b&#39; # make a place to store chapter 2b example data dir.create(ch2b.data.path, recursive = TRUE) # download polygon example data from github download.file(&#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_2b-spatial-data/chapter-2b-mu-polygons.zip&#39;, file.path(ch2b.data.path, &#39;chapter-2b-mu-polygons.zip&#39;)) # download raster example data from github download.file(&#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/chapter_2b-spatial-data/chapter-2b-PRISM.zip&#39;, file.path(ch2b.data.path, &#39;chapter-2b-PRISM.zip&#39;)) # unzip unzip(file.path(ch2b.data.path, &#39;chapter-2b-mu-polygons.zip&#39;), exdir = ch2b.data.path, overwrite = TRUE) unzip(file.path(ch2b.data.path, &#39;chapter-2b-PRISM.zip&#39;), exdir = ch2b.data.path, overwrite = TRUE) 6.5.2 Load the data We will be using polygons associated with MLRAs 15 and 18 as part of this demonstration. Import these data now with readOGR(); recall the somewhat strange syntax. # set path to example data ch2b.data.path &lt;- &#39;C:/workspace/chapter-2b&#39; Next, load in the example raster data with the raster() function. These are 800m PRISM data that have been cropped to the extent of the MLRA 15 and 18 polygons. # mean annual air temperature, Deg C maat &lt;- raster(file.path(ch2b.data.path, &#39;MAAT.tif&#39;)) # mean annual precipitation, mm map &lt;- raster(file.path(ch2b.data.path, &#39;MAP.tif&#39;)) # frost-free days ffd &lt;- raster(file.path(ch2b.data.path, &#39;FFD.tif&#39;)) # growing degree days gdd &lt;- raster(file.path(ch2b.data.path, &#39;GDD.tif&#39;)) # percent of annual PPT as rain rain_fraction &lt;- raster(file.path(ch2b.data.path, &#39;rain_fraction.tif&#39;)) # annual sum of monthly PPT - ET_p ppt_eff &lt;- raster(file.path(ch2b.data.path, &#39;effective_preciptitation.tif&#39;)) Sometimes it is convenient to “stack” raster data that share a common grid size, extent, and coordinate reference system into a single RasterStack object. # create a raster stack (multiple rasters aligned) rs &lt;- stack(maat, map, ffd, gdd, rain_fraction, ppt_eff) # reset layer names names(rs) &lt;- c(&#39;MAAT&#39;, &#39;MAP&#39;, &#39;FFD&#39;, &#39;GDD&#39;, &#39;rain.fraction&#39;, &#39;eff.PPT&#39;) Quick inspection of the data. # object class class(mlra) class(maat) class(rs) # the raster package provides a nice &quot;print&quot; method for raster and sp classes print(maat) # coordinate reference systems: note that they are not all the same proj4string(mlra) proj4string(maat) proj4string(rs) Basic plot methods (class-specific functions) for the data. Note that this approach requires that all layers in the “map” are in the same coordinate refrence system (CRS). # MLRA polygons in native coordinate system # recall that mlra is a SpatialPolygonsDataFrame plot(mlra, main = &#39;MLRA 15 and 18&#39;) box() # MAAT raster # recall that maat is a raster object plot(maat, main = &#39;PRISM Mean Annual Air Temperature (deg C)&#39;) # plot MAAT raster with MLRA polygons on top # this requires transforming to CRS of MAAT mlra.gcs &lt;- spTransform(mlra, CRS(proj4string(maat))) plot(maat, main = &#39;PRISM Mean Annual Air Temperature (deg C)&#39;) plot(mlra.gcs, main = &#39;MLRA 15 and 18&#39;, add = TRUE) 6.5.3 Basic sampling and extraction Sampling and extraction, result is a matrix object. # sampling single RasterLayer sampleRegular(maat, size = 10) # sampling RasterStack sampleRegular(rs, size = 10) Sampling and extract, result is a SpatialPointsDataFrame object. par(mfcol = c(1, 2), mar = c(1, 1, 3, 1)) # regular sampling + extraction of raster values x.regular &lt;- sampleRegular(maat, size = 100, sp = TRUE) plot(maat, axes = FALSE, legend = FALSE, main = &#39;Regular Sampling&#39;) points(x.regular) # random sample + extraction of raster values # note that NULL values are removed x.random &lt;- sampleRandom(maat, size = 100, sp = TRUE, na.rm = TRUE) plot(maat, axes = FALSE, legend = FALSE, main = &#39;Random Sampling with NA Removal&#39;) points(x.random) Note that the mean can be efficiently estimated, even with a relatively small number of samples. # all values: slow for large grids mean(values(maat), na.rm = TRUE) # regular sampling: efficient, central tendency comparable to above mean(x.regular$MAAT, na.rm = TRUE) # this value will be pseudorandom # depends on number of samples, pattern of NA mean(x.random$MAAT, na.rm = TRUE) Just how much variation can we expect when collecting 100, randomly-located samples over such a large area? This is better covered in chapter 4 (Sampling), but a quick experiment might be fun. Do this 100 times: compute the mean MAAT from 100 randomly-located samples. # takes a couple of seconds z &lt;- replicate(100, mean(sampleRandom(maat, size = 100, na.rm = TRUE), na.rm = TRUE)) # 90% of the time the mean MAAT values were within: quantile(z, probs = c(0.05, 0.95)) 6.5.3.1 Extracting Raster Data at KSSL Pedon Locations Extract PRISM data at the coordinates associated with KSSL pedons that have been correlated to the AUBURN series. We will use the fetchKSSL() function from the soilDB package to get KSSL data from the most recent snapshot. This example can be easily adapted to pedon data extracted from NASIS using fetchNASIS(). Get some KSSL data and upgrade the “site” data to a SpatialPointsDataFrame. # result is a SoilProfileCollection object auburn &lt;- fetchKSSL(series = &#39;auburn&#39;) # extract site data s &lt;- site(auburn) # these are GCS WGS84 coordinates from NASIS coordinates(s) &lt;- ~ x + y proj4string(s) &lt;- &#39;+proj=longlat +datum=WGS84&#39; Extract PRISM data (the RasterStack object we made earlier) at the Auburn KSSL locations and summarize. # return the result as a data.frame object e &lt;- extract(rs, s, df=TRUE) # summarize: remove first (ID) column using [, -1] j index summary(e[, -1]) Join the extracted PRISM data with the original SoilProfileCollection object. More information on SoilProfileCollection objects here. # don&#39;t convert character data into factors options(stringsAsFactors = FALSE) # combine site data with extracted raster values, row-order is identical res &lt;- cbind(as(s, &#39;data.frame&#39;), e) # extract unique IDs and PRISM data res &lt;- res[, c(&#39;pedon_key&#39;, &#39;MAAT&#39;, &#39;MAP&#39;, &#39;FFD&#39;, &#39;GDD&#39;, &#39;rain.fraction&#39;, &#39;eff.PPT&#39;)] # join with original SoilProfileCollection object via pedon_key site(auburn) &lt;- res The extracted values are now part of the “auburn” SoilProfileCollection object. Does there appear to be a relationship between soil morphology and “effective precipitation”? Not really. # create an ordering of pedons based on the extracted effective PPT new.order &lt;- order(auburn$eff.PPT) # setup figure margins, 1x1 row*column layout par(mar = c(5, 2, 4, 2), mfcol = c(1, 1)) # plot profile sketches plot(auburn, name = &#39;hzn_desgn&#39;, print.id = FALSE, color = &#39;clay&#39;, plot.order = new.order, cex.names = 0.85) # add an axis with extracted raster values axis(side = 1, at = 1:length(auburn), labels = round(auburn$eff.PPT[new.order]), cex.axis = 0.75) mtext(&#39;Annual Sum of Monthly (PPT - ET_p) (mm)&#39;, side = 1, line = 2.5) Note that negative values are associated with a net deficit in monthly precipitation vs. estimated ET. 6.5.3.2 Summarizing PRISM Data by Series Extent The seriesExtent() function from the soilDB package provides a simple interface to Series Extent Explorer data files. Note that these series extents have been generalized for rapid display at regional to continental scales. A more precise representation of “series extent” can be generated from SSURGO polygons and queried from SDA. Get an approximate extent for the Amador soil series from SEE. See the manual page for seriesExtent for additional options and related functions. amador &lt;- seriesExtent(s = &#39;amador&#39;) class(amador) Generate 100 sampling points within the extent, using a hexagonal grid. These will be used to extract raster values from our RasterStack of PRISM data. s &lt;- spsample(amador, n = 100, type = &#39;hexagonal&#39;) For comparison, extract a single point from each SSURGO map unit delineation that contains Amador as a major component. This will require a query to SDA for the set of matching map unit keys (mukey), followed by a second request to SDA for the geometry. The SDA_query function is used to send arbitrary queries written in SQL to SDA, the results may be a data.frame or list, depending on the complexity of the query. The fetchSDA_spatial function returns map unit geometry as either polygons, polygon envelopes, or a single point per polygon and selected by mukey or nationalmusym. # result is a data.frame mukeys &lt;- SDA_query(&quot;SELECT DISTINCT mukey FROM component WHERE compname = &#39;Amador&#39; AND majcompflag = &#39;Yes&#39;;&quot;) # result is a SpatialPointsDataFrame amador.pts &lt;- fetchSDA_spatial(mukeys$mukey, by.col = &#39;mukey&#39;, method = &#39;point&#39;, chunk.size = 2) Graphically check both methods # adjust margins and setup plot device for two columns par(mar = c(1, 1, 3, 1), mfcol = c(1, 2)) # first figure plot(maat, ext = extent(s), main = &#39;PRISM MAAT\\n100 Sampling Points from Extent&#39;, axes = FALSE) plot(amador, add = TRUE) points(s, cex = 0.25) plot(maat, ext = extent(s), main = &#39;PRISM MAAT\\nPolygon Centroids&#39;, axes = FALSE) points(amador.pts, cex = 0.25) Extract PRISM data (the RasterStack object we made earlier) at the sampling locations (100 regularly-spaced and from MU polygon centroids) and summarize. Note that CRS transformations are automatic (when possible). # return the result as a data.frame object e &lt;- extract(rs, s, df = TRUE) e.pts &lt;- extract(rs, amador.pts, df = TRUE) # check out the extracted data summary(e[,-1]) # all pair-wise correlations knitr::kable(cor(e[,-1]), digits = 2) Quickly compare the two sets of samples. More on this in the Sampling module. # compile results into a list maat.comparison &lt;- list(&#39;regular samples&#39; = e$MAAT, &#39;polygon centroids&#39; = e.pts$MAAT) # number of samples per method lapply(maat.comparison, length) # summary() applied by group lapply(maat.comparison, summary) # box-whisker plot par(mar = c(4.5, 8, 3, 1), mfcol = c(1, 1)) boxplot( maat.comparison, horizontal = TRUE, las = 1, xlab = &#39;MAAT (deg C)&#39;, varwidth = TRUE, boxwex = 0.5, main = &#39;MAAT Comparison&#39; ) Basic climate summaries from a standardized source (e.g. PRISM) might be a useful addition to an OSD. Think about how you could adapt this example to compare climate summaries derived from NASIS pedons to similar summaries derived from map unit polygons and generalized soil series extents. 6.5.3.3 Summarizing PRISM Data by MLRA Polygon We will be using polygons associated with MLRAs 15 and 18 as part of this demonstration. Import these data now with readOGR(); recall the somewhat strange syntax. You will need the data and RasterStack object rs we created in the examples above. # load MLRA polygons mlra &lt;- readOGR(dsn = ch2b.data.path, layer = &#39;mlra-18-15-AEA&#39;) The following example is a simplified version of what is available in the soilReports package, reports on the ncss-tech GitHub repository, or in the TEUI suite of map unit summary tools. The Computing GIS Summaries from Map Unit Polygons tutorial is an expanded version of this example. Example output from the soilReports package: summary of select CA630 map units summary of select MLRA polygons Efficient summary of large raster data sources can be accomplished using: internally-compressed raster data sources, stored on a local disk, can be in any coordinate system polygons stored in an equal-area or UTM coordinate system, with CRS units of meters fixed-density sampling of polygons estimation of quantiles from collected raster samples Back to our example data. The first step is to check the MLRA polygons (mlra); how many features per MLRA symbol? Note that some MLRA have more than one polygon. table(mlra$MLRARSYM) Convert polygon area from square meters to acres and summarize. Note that this will only make sense when using a projected CRS with units of meters! poly.area &lt;- round(sapply(mlra@polygons, slot, &#39;area&#39;) * 0.000247105) summary(poly.area) sum(poly.area) Sample each polygon at a constant sampling density of 0.001 samples per acre (1 sample for every 1,000 ac.). At this sampling density we should expect approximately 16,700 samples–more than enough for our simple example. library(sharpshootR) # the next function requires a polygon ID: each polygon gets a unique number 1--number of polygons mlra$pID &lt;- 1:nrow(mlra) s &lt;- constantDensitySampling(mlra, n.pts.per.ac = 0.001) Extract MLRA symbol at sample points using the over() function. The result will be a data.frame object with all attributes from our MLRA polygons that intersect sampling points s. # spatial overlay: sampling points and MLRA polygons res &lt;- over(s, mlra) # row / feature order is preserved, so we can directly copy s$mlra &lt;- res$MLRARSYM # tabulate number of samples per MLRA table(s$mlra) Extract values from the RasterStack of PRISM data as a data.frame. # raster stack extraction at sampling points e &lt;- extract(rs, s, df=TRUE) # convert sampling points from SpatialPointsDataFrame to data.frame s.df &lt;- as(s, &#39;data.frame&#39;) # join columns from extracted values and sampling points s.df &lt;- cbind(s.df, e) # check results head(s.df) Summarizing multivariate data by group (MLRA) is usually much simpler after reshaping data from “wide” to “long” format. library(lattice) library(reshape2) # reshape from wide to long format m &lt;- melt(s.df, id.vars = c(&#39;mlra&#39;), measure.vars = c(&#39;MAAT&#39;, &#39;MAP&#39;, &#39;FFD&#39;, &#39;GDD&#39;, &#39;rain.fraction&#39;, &#39;eff.PPT&#39;)) # check &quot;wide&quot; format head(m) A simple tabular summary of means by MLRA and PRISM variable using tapply(). # tabular summary of mean values tapply(m$value, list(m$mlra, m$variable), mean) Lattice graphics are useful for summarizing grouped comparisons. The syntax is a little rough, but there is a lot of documentation online. Box and whisker plots are usually the first step in a graphical comparison of samples. We will go over the interpretation of these kind of figures in chapter 4. tps &lt;- list( box.rectangle = list(col = &#39;black&#39;), box.umbrella = list(col = &#39;black&#39;, lty = 1), box.dot = list(cex = 0.75), plot.symbol = list( col = rgb(0.1, 0.1, 0.1, alpha = 0.25, maxColorValue = 1), cex = 0.25 ) ) bwplot(mlra ~ value | variable, data=m, # setup plot and data source as.table=TRUE, # start panels in top/left corner varwidth=TRUE, # scale width of box by number of obs scales=list(alternating=3, relation=&#39;free&#39;), # setup scales strip=strip.custom(bg=grey(0.9)), # styling for strips par.settings=tps, # apply box/line/point styling panel=function(...) { # within in panel, do the following panel.grid(-1, -1) # make grid lines at all tick marks panel.bwplot(...) # make box-whisker plot } ) 6.6 Additional Reading Ahmed, Zia. 2020. Geospatial Data Science with R. Gimond, M., 2019. Intro to GIS and Spatial Analysis https://mgimond.github.io/Spatial/ Hijmans, R.J. 2019. Spatial Data Science with R. https://rspatial.org/ Lovelace, R., J. Nowosad, and J. Muenchow, 2019. Geocomputation with R. CRC Press. https://bookdown.org/robinlovelace/geocompr/ Pebesma, E., and R.S. Bivand. 2005. Classes and methods for spatial data: The sp package. https://cran.r-project.org/web/packages/sp/vignettes/intro_sp.pdf. Pebesma, E. and R. Bivand, 2019. Spatial Data Science. https://keen-swartz-3146c4.netlify.com/ "],
["sampling.html", "Chapter 7 Sampling 7.1 Introduction 7.2 Sampling Strategies 7.3 Evaluating a Sampling Strategy 7.4 Other Tools for Selecting Random Features 7.5 References 7.6 Additional Reading", " Chapter 7 Sampling 7.1 Introduction Sampling is a fundamental part of statistics. Samples are collected to achieve an understanding of a population because it is typically not feasible to observe all members of the population. The goal is to collect samples that provide an accurate representation of the population. Constraints on time and money dictate that the sampling effort must be efficient. More samples are needed to characterize the nature of highly variable populations than less variable populations. Define your purpose: What are you investigating? Examples include soil properties, soil classes, and plant productivity. How many samples are needed? Because you are typically interested in using field point data to derive inferences, you need enough samples to be confident that they approximate the target population. In the case of calibrating a laboratory device, you might only need two measurements, each at opposite ends of the measurement scale. This illustrates the point that sample size is closely related to the inherent variability in the data. The number of samples required increases with increasing variability. Also, the more samples you have, the greater the confidence level you can achieve. For example, sampling at an 85 percent confidence level is less intensive than sampling at a 95 percent confidence level. If you have prior knowledge about the soil attribute being sampled, you can use a t-test to estimate the number (n) of samples needed to detect a significant difference. You can use either the power.t.test() or power.prop.tes() functions. See the following example of differences in clay content. # Clay example. Test to see the number of samples necessary to detect a 3 percent difference in clay between two horizons. power.t.test(power = 0.95, sd = 2, delta = 16 - 19) # delta = the difference between the two means power.t.test(power = 0.95, sd = 3, delta = 16 - 19) From the figure above, you can see how difficult it is to separate two overlapping populations (e.g., horizons). Also, a 1-unit increase in the standard deviation doubles the number of samples required. This simplistic example, although enlightening, is hard to implement in practice and serves only as a theoretical exercise. Some rules of thumb for regression models are as follows: Use &gt; 10 observations (n) per predictor (m) (Kutner et al., 2005). Use &gt; 20 n per m and n &gt; 104 + m to test regression coefficients (Rossiter, 2015; Franklin and Miller, 2009). Never use n &lt; 5*m (Rossiter, 2015). Soil Survey Applications: Capture range of soil-forming factors Determine map unit composition 7.2 Sampling Strategies 7.2.1 Simple Random In simple random sampling, all samples within the region have an equal chance of being selected. A simple random selection of points can be made using either the spsample() function within the sp R package or the Create Random Points tool in ArcGIS. Advantages Simplicity Requires little prior knowledge of the population Disadvantages Lower accuracy Higher cost Lower efficiency Samples may be clustered spatially Samples may not be representative of the feature attribute(s) library(sp) # Create a sixteen square polygon grd &lt;- GridTopology(c(1, 1), c(1, 1), c(4, 4)) polys &lt;- as.SpatialPolygons.GridTopology(grd) plot(polys) # Generate simple random sample test &lt;- spsample(polys, n = 15, type = &quot;random&quot;) points(test, pch = 19) 7.2.2 Stratified Random In stratified random sampling, the sampling region is spatially subset into different strata, and random sampling is applied to each strata. If prior information is available about the study area, it can be used to develop the strata. Strata may be sampled equally or in proportion to area; however, if the target of interest is rare in the population, it may be preferable to sample the strata equally (Franklin and Miller, 2009). Advantages Higher accuracy Lower cost Disadvantages The existing knowledge used to construct strata may be flawed. plot(polys, main = &quot;Stratified random sample&quot;) # Generate a spatially stratified random sample test &lt;- spsample(polys, n = 15, type = &quot;stratified&quot;) points(test, pch = 19) Note that the spsample() function only stratifies the points spatially. Other more sophisticated designs can be implemented using the spsurvey, spcosa, sharpshootR, or clhs packages. 7.2.3 Multistage Stratified Random In multistage random sampling, the region is separated into different subsets that are randomly selected (i.e., first stage), and then the selected subsets are randomly sampled (i.e., second stage). This is similar to stratified random sampling, except that with stratified random sampling each strata is sampled. Advantages Greater efficiency Lower cost Disadvantages Lower precision Stronger clustering than simple random sampling plot(polys, main = &quot;Two-stage random&quot;) # Select 8 samples from each square s &lt;- sapply(slot(polys, &quot;polygons&quot;), function(x) spsample(x, n = 5, type = &quot;random&quot;)) points(sample(s, 1)[[1]], pch = 19) # randomly select 1 square and plot points(sample(s, 1)[[1]], pch = 19) # randomly select 1 square and plot 7.2.4 Systematic In systematic sampling, a sample is taken according to a regularized pattern. This approach ensures even spatial coverage. Patterns may be rectilinear, triangular, or hexagonal. This sampling strategy can be inaccurate if the variation in the population doesn’t coincide with the regular pattern (e.g., if the population exhibits periodicity). Advantages Greater efficiency Lower cost Disadvantages Lower precision plot(polys, main = &quot;Systematic sample&quot;) # Generate systematic random sample test &lt;- spsample(polys, n = 15, type = &quot;regular&quot;) points(test, pch = 19) 7.2.5 Cluster In cluster sampling, a cluster or group of points is selected at one or more sites. The transect is an example of this strategy, although other shapes are possible (e.g., square, triangle, or cross shapes). It is common to orient the transect in the direction of greatest variability. Advantages Greater efficiency Lower cost Disadvantages Lower precision plot(polys, main = &quot;Clustered (n = 3) random sample&quot;) # Generate cluster random sample test &lt;- spsample(polys, n = 15, type = &quot;clustered&quot;, nclusters = 3, iter = 10) points(test, pch = 19) 7.2.6 Conditioned Latin Hypercube (cLHS) Conditioned Latin hypercube sampling is a stratified random sampling technique to obtain representative samples from feature (attribute) space (Minasny and McBratney, 2006). For example, assume you have prior knowledge of a study area and have the time and resources to collect 120 points. You also know the following variables (strata), which are represented as coregistered raster datasets, to be of importance to the soil property or class being investigated: Normalized Difference Vegetation Index (NDVI), Topographic Wetness Index (a.k.a. Wetness Index, compound topographic index), Solar insolation (potential incoming solar radiation), and Relative elevation (a.k.a. relative position, normalized slope height). The cLHS procedure iteratively selects samples from the strata variables such that they replicate the range of values from each stratum. Without a technique such as cLHS, obtaining a sample that is representative of the feature space becomes increasingly difficult as the number of variables (strata) increases. To perform cLHS using R, you can use the clhs package (Roudier, 2011). library(clhs) library(raster) # import volcano DEM, details at http://geomorphometry.org/content/volcano-maungawhau data(volcano) volcano_r &lt;- raster(as.matrix(volcano[87:1, 61:1]), crs = CRS(&quot;+init=epsg:27200&quot;), xmn = 2667405, xmx = 2667405 + 61 * 10, ymn = 6478705, ymx = 6478705 + 87 * 10) names(volcano_r) &lt;- &quot;elev&quot; # calculate slope from the DEM slope_r &lt;- terrain(volcano_r, opt = &quot;slope&quot;, unit = &quot;degrees&quot;) # Stack Elevation and Slope rs &lt;- stack(volcano_r, slope_r) # generate cLHS design cs &lt;- clhs(rs, size = 20, progress = FALSE, simple = FALSE) # Plot cLHS Samples par(mar=c(1,1,1,4)) plot(volcano_r, axes=FALSE) points(cs$sampled_data) # Summary of clhs object summary(cs$sampled_data)$data # Summary of raster objects cbind(summary(volcano_r), summary(slope_r)) Although the above example works well on our small volcano dataset, the clhs package is inefficient if you are working with large raster datasets. To overcome this limitation, you can first take a large random sample and then subsample it using cLHS. sub_s &lt;- sampleRandom(volcano_r, size = 200, sp = TRUE) # random sample function from the raster package # s &lt;- clhs(sub_s, size = 20, progress = FALSE, simple = FALSE) 7.3 Evaluating a Sampling Strategy To gauge the representativeness of a sampling strategy, you can compare the results it produces to the results for other variables you think might coincide with the soil properties or classes of interest (Hengl, 2009). Examples include slope gradient, slope aspect, and vegetative cover. These other variables may be used to stratify the sampling design or to assess the representativeness of our existing samples (e.g., NASIS pedons). The simple example below demonstrates how to compare several sampling strategies by evaluating how well they replicate the distribution of elevation. # create a polygon from the spatial extent of the volcano dataset test &lt;- as(extent(volcano_r), &quot;SpatialPolygons&quot;) # take a large random sample sr400 &lt;- spsample(test, n = 400, type = &quot;random&quot;) # take a small random sample sr &lt;- spsample(test, n = 20, type = &quot;random&quot;) # take a small stratified random sample str &lt;- spsample(test, n = 23, type = &quot;stratified&quot;, iter = 1000)[1:20] # take a cLHS sample # cs &lt;- clhs(rs, size = 20, progress = FALSE, simple = FALSE) # Combind and Extract Samples s &lt;- rbind(data.frame(method = &quot;Simple Random 400&quot;, extract(rs, sr400)), data.frame(method = &quot;Simple Random&quot;, extract(rs, sr)), data.frame(method = &quot;Stratified Random&quot;, extract(rs, str)), data.frame(method = &quot;cLHS&quot;, cs$sampled_data@data)) # Summarize the sample values aggregate(slope ~ method, data = s, function(x) round(summary(x))) # Plot overlapping density plots to compare the distributions between the large and small samples library(ggplot2) ggplot(s, aes(x = slope, col = method)) + geom_density(cex = 2) # plot the spatial locations par(mfrow = c(1, 3), mar=c(1,2,4,5)) plot(volcano_r, main = &quot;Simple random&quot;, cex.main = 2, axes=FALSE) points(sr, pch = 3, cex = 1.2) plot(volcano_r, main = &quot;Stratified random&quot;, cex.main = 2, axes=FALSE) points(str, pch = 3, cex = 1.2) plot(volcano_r, main = &quot;cLHS&quot;, cex.main = 2, axes=FALSE) points(cs$sampled_data, pch = 3, cex = 1.2) The overlapping density plots above illustrate the differences between large and small sets of samples using several sampling designs. Clearly the cLHS approach best duplicates the distribution of elevation (because elevation is explicitly used in the stratification process). The contrast is less severe in the summary metrics, but again cLHS more closely resembles the larger sample. Other comparisons are possible using the approaches in the following chapters. 7.3.1 Exercise: Design a Sampling Strategy Using the “tahoe_lidar_highesthit.tif” dataset in the gdalUtils package or using your own dataset (highly encouraged), compare two or more sampling approaches. Show your work and submit the results to your coach. 7.4 Other Tools for Selecting Random Features An ArcGIS tool for selecting random features is available from the Job Aids page. This tool randomly selects the specified number of features from a dataset or set of selected features in ArcGIS. It is an ideal tool for the first stage of a two-stage random sample. 7.4.1 cLHS Using TEUI The TEUI toolkit includes a tool for generating cLHS samples on large raster datasets. The tool is based on the clhs R package (Roudier, 2011). Relative Elevation (a.k.a. relative position) Northwestness Normalized Difference Vegetation Difference (a.k.a. NDVI) R GUI image Open ArcGIS. Add the TEUI Toolkit Toolbar by clicking the Customize tab, pointing to Toolbars, and checking TEUI. R GUI image The TEUI Toolbar looks like the following. R GUI image Click the Cube icon to open the Latin Hyper Cube Generator Tool. R GUI image R GUI image The Tool requires that all raster data be in Imagine format (i.e., have the “img” extension) and share a common projection and resolution. The tool adds all raster layers in the Table of Contents to the Layers section. The layers to be used are checked. An exclusion layer is used in this example. An exclusion layer is a binary raster with values of 0 and 1. Using an exclusion layer confines the selection of points to those areas with a raster value of 1. The output file is a shapefile named “samples.shp.” The Number of Points is 30. The Number of Iterations is increased from the default of 100 to 300. Increasing the number of iterations increases the processing time, but it also increases the likelihood that the samples selected are representative of the selected strata. Click Generate and let the routine process. This can take from several minutes to several hours, depending on how large the area is in terms of columns and rows and how many layers are used. The resulting output shows 30 points confined to the watershed of interest. R GUI image Comparing the frequency distribution of the samples to the population shows a reasonable representation, especially considering the small sample size. R GUI image 7.4.2 Two-Stage Stratified Random Sample Design Using ArcGIS The following example examines a sampling design executed by Wendy Noll, USDA-NRCS, Morgantown, WV and used by NRCS and USFS investigators in the Monongahela National Forest. They were interested in quantifying the depth of organic surface horizons in soils that correlated to the Mandy soil series and formed under red spruce canopy on backslopes in the Upper Greenbrier Watershed (HUC 8 -05050003). This project ultimately led to the development of new soil series and the update of thousands of acres of forest soils in West Virginia. Stage 1: Sub-watersheds in the study area were randomly selected. R GUI image Strata: Sampling was based on three stratum. Mandy soil map units (MfE, MfF, MfG, and other) Red spruce canopy cover (&gt;30% canopy, other) Slope (&lt;= 35%, &gt;= 35%) Data layers: Input layers included coregistered raster data of each stratum, reclassed as follows. R GUI image The Raster Calculator was used to add data layers together. R GUI image The resulting raster file had the following combinations. R GUI image The following guide was used to verify the selection of sample numbers allocated according to the proportionate extent of the strata. R GUI image The Create Random Points tool was used within each sub-watershed. The tool is in Data Management Tools &gt; Feature Class toolbox. R GUI image The sub-watershed layer was specified as the Constraining Feature Class. The number of points selected was 50. R GUI image The resulting point file had 50 points per polygon. R GUI image The following image was used to check if the sample points adequately represented the proportionate extent of the data made by summarizing the GRIDCODE of the points. R GUI image The results compared well to the extent of the population. R GUI image 7.5 References Franklin, J., and J.A. Miller. 2009. Mapping species distributions: Spatial inference and prediction. Cambridge: Cambridge University Press. http://www.cambridge.org/us/academic/subjects/life-sciences/ecology-and-conservation/mapping-species-distributions-spatial-inference-and-prediction. Hengl, T. 2009. A practical guide to geostatistical mapping, 2nd Ed. University of Amsterdam. ISBN 978-90-9024981-0. http://spatial-analyst.net/book/system/files/Hengl_2009_GEOSTATe2c0w.pdf. Kutner, M.H., C.J. Nachtsheim, J. Neter, and W. Li (Eds.). 2005. Applied linear statistical models, 5th edition. McGraw-Hill. Roudier, P. 2011. cLHS: A R package for conditioned Latin hypercube sampling. https://cran.r-project.org/web/packages/clhs/index.html. Rossiter, D. 2015. Spatial modelling and analysis applied to agronomic and environmental systems [Lecture notes]. http://www.css.cornell.edu/faculty/dgr2/teach/CSS6200.html. Minasny, B., and A.B. McBratney. 2006. A conditioned Latin hypercube method for sampling in the presence of ancillary information. Computers &amp; Geosciences. 32(9):1378-1388. http://www.sciencedirect.com/science/article/pii/S009830040500292X. Vaughan, R., and K. Megown. 2015. The Terrestrial Ecological Unit Inventory (TEUI) Geospatial Toolkit: User guide v5.2. RSAC-10117-MAN1. Salt Lake City, UT: U.S. Department of Agriculture, Forest Service, Remote Sensing Applications Center. http://www.fs.fed.us/eng/rsac/programs/teui/about.html. 7.6 Additional Reading Brungard, C., &amp; Johanson, J. (2015). The gate’s locked! I can’t get to the exact sampling spot … can I sample nearby? Pedometron(37), 8-9. http://www.pedometrics.org/Pedometron/Pedometron37.pdf Brus, D.J. and J.J. de Gruijter. “Random sampling or geostatistical modelling? Choosing between design-based and model-based sampling strategies for soil (with discussion).” Geoderma. Vol. 80. 1. Elsevier, 1997. 1-44. https://www.sciencedirect.com/science/article/pii/S0016706197000724 de Gruijter, J., D.J. Brus, M.F.P. Bierkens, and M. Knotters. 2006. Sampling for natural resource monitoring: Springer. http://www.springer.com/us/book/9783540224860. Schreuder, H.T., R. Ernst, and H. Ramirez-Maldonado. 2004. Statistical techniques for sampling and monitoring natural resources. Gen. Tech. Rep. RMRS-GTR-126. Fort Collins, CO: U.S. Department of Agriculture, Forest Service, Rocky Mountain Research Station. http://www.fs.fed.us/rm/pubs/rmrs_gtr126.html. U.S. Environmental Protection Agency. 2002. Guidance for choosing a sampling design for environmental data collection. Washington, DC: US EPA. http://www.epa.gov/quality/guidance-choosing-sampling-design-environmental-data-collection-use-developing-quality. Viscarra Rossel, R.A., et al. “Baseline estimates of soil organic carbon by proximal sensing: Comparing design-based, model-assisted and model-based inference.” Geoderma 265 (2016): 152-163. https://www.sciencedirect.com/science/article/pii/S0016706115301312 "]
]
