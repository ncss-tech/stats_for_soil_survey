[["index.html", "Statistics for Soil Survey - Part 2 Pre-course Assignment 0.1 Setup 0.2 Connect to Local NASIS 0.3 Additional Support/Optional Readings", " Statistics for Soil Survey - Part 2 Soil Survey Staff 2021-05-10 Pre-course Assignment 0.1 Setup 0.1.1 Create Workspace Make a local folder C:\\workspace2 to use as a working directory for this course. Use all lower case letters please. 0.1.2 Configure RStudio Open RStudio, and edit the Global Options (Main menu: Tools  Global Options). 0.1.2.1 Essentials These options are important for pleasant, reproducible and efficient use of the RStudio environment: Change the default working directory to C:\\workspace2 (R General Tab) Uncheck Restore .Rdata into workspace at startup (R General Tab) VERY IMPORTANT Figure 1: Example of RStudio General settings. RStudio detects the available R installations on your computer. Individual versions are certified for the Software Center as they become available, and sometimes there is a more recent version available for download. It is worth taking the time before installing packages to get the latest version of R available to you. This is to minimize compatibility issues which arise over time. 0.1.2.2 Personalization Figure 2: Example of RStudio Code/Editing settings. Optional: Check Soft-wrap R source files (Code/Editing Tab) Optional: Show help tooltips, control auto-completion and diagnostics (Code/Completion and Diagnostics Tabs) Optional: Update code font size, colors and theme (Appearance) Optional: Use RStudio Projects (top-right corner) to manage working directories 0.1.3 Install .RProfile The code you run next will establish a safe location for your R package library. Your package library should ideally be on a local disk with about 1 - 2 GB of free space. We want to prevent installs to ~ (your $HOME directory) which is typically on a network share (such as H:/), not a local disk. Copy the following code in the box below and paste into the R console panel after the command prompt (&gt;) and press enter. Hint: the R console is the lower left or left window in RStudio with a tab labeled Console. source(&#39;https://raw.githubusercontent.com/ncss-tech/soilReports/master/R/installRprofile.R&#39;) installRprofile(overwrite=TRUE) An updated set of library paths will be printed. Close and re-open RStudio, or Restart R (Main menu: Session  Restart R; or Ctrl+Shift+F10), before continuing to the next steps. Figure 3: Example of RStudio Console - the R library paths are on a local drive C:/ When your .Rprofile is set up correctly you will see output in a new R console/session confirming your library paths are: on a local drive (such as C:/) specific to the version number of R installed (such as 4.0) 0.1.4 Install Required Packages Packages can be installed by name from the Comprehensive R Archive Network (CRAN) using the base R function install.packages There are a lot of packages out there  many more than you will download here, and many of which are useful for Soil Survey work. The first time you install packages, R will ask you if you want to create a local repository in your User Documents folder. Click Yes. For example, to download and install the remotes package from CRAN: install.packages(&quot;remotes&quot;) To install the R packages used in this class copy all of the code from the box below and paste into the R console window. Paste after the command prompt (&gt;) and press enter. Downloading and configuring the packages will take a while if you are installing or upgrading all of the packages in the list below. # ipkCRAN: a helper function for installing required packages from CRAN # - p: vector of package names # - up: logical - upgrade installed packages? Default: TRUE ipkCRAN &lt;- function(p, up = TRUE){ message(&#39;installing packages from CRAN...&#39;) if (up) { # default is to re-install everything install.packages(p, dependencies = TRUE) } else { # but if up != TRUE install just what is needed new.pkg &lt;- p[! (p %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg) &gt; 0) { install.packages(new.pkg, dependencies = TRUE) } } # finally, check and see if any failed missing.pkg &lt;- p[! (p %in% installed.packages()[, &quot;Package&quot;])] if (length(missing.pkg) &gt; 0) { warning(sprintf(&#39;\\033[31mOne or more packages failed to install!\\033[39m\\n%s&#39;, sprintf(&quot;Restart R then try `\\033[35minstall.packages(c(%s))\\033[39m`&quot;, paste0(sprintf(&#39;&quot;%s&quot;&#39;, missing.pkg), collapse = &quot;,&quot;))), call. = FALSE) } } ## character vector of packages packages &lt;- c( # soil &quot;aqp&quot;, &quot;soilDB&quot;, &quot;sharpshootR&quot;, &quot;soiltexture&quot;, # gis &quot;rgdal&quot;, &quot;raster&quot;, &quot;sp&quot;, &quot;sf&quot;, &quot;terra&quot;, &quot;gdalUtils&quot;, &quot;rgrass7&quot;, &quot;RSAGA&quot;, &quot;exactextractr&quot;, &quot;fasterize&quot;, # data management &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;devtools&quot;, &quot;roxygen2&quot;, &quot;Hmisc&quot;, &quot;RODBC&quot;, &quot;circular&quot;, &quot;DT&quot;, &quot;remotes&quot;, # graphics &quot;ggplot2&quot;, &quot;latticeExtra&quot;, &quot;maps&quot;, &quot;spData&quot;, &quot;tmap&quot;, &quot;mapview&quot;, &quot;plotrix&quot;, &quot;rpart.plot&quot;, &quot;visreg&quot;, # modeling &quot;car&quot;, &quot;rms&quot;, &quot;randomForest&quot;, &quot;ranger&quot;, &quot;party&quot;, &quot;caret&quot;, &quot;vegan&quot;, &quot;ape&quot;, &quot;shape&quot;, # sampling &quot;clhs&quot; ) ## install vector of CRAN packages &quot;safely&quot; ## up = TRUE to download all packages ## up = FALSE to download only packages you don&#39;t already have installed ipkCRAN(packages, up = TRUE) The ipkCRAN function will let you know if any of the above packages fail to install. Any time you run code, always check the console output for warnings and errors before continuing. If a lot of output is produced by a command you should scroll up and sift through. It may be best early on to send commands individually to learn about and inspect their output. 0.1.5 Common Errors 0.1.5.1 No output is produced after pasting into console If you do not have a new command prompt (&gt;) and a blinking cursor on the left hand side of your console, but instead see a + after you run a command, R may think you are still in the middle of submitting input to the read-eval-print-loop (REPL). If this is not expected you are possibly missing closing quotes, braces, brackets or parentheses. R needs to know you were done with your expression, so you may need to supply some input to get the command to be complete. You can use the shortcut Ctrl+C to get out of a partially-complete command. Pasting code line-by-line is useful but prone to input errors with multi-line expressions. Alternately, you can run commands or an entire file using the GUI or keyboard shortcuts such as Ctrl+Enter. You have a chance to try this in the example at the end. 0.1.5.2 SOMEPACKAGE is not available (for R version X.Y.Z) This means either: A package named SOMEPACKAGE exists but it is not available for your version of R CRAN does not have a package with that name You can try again, but first check for spelling and case-sensitivity. When in doubt search the package name on Google or CRAN to make sure you have it right. Note that not all R packages are available on CRAN: there are many other ways that you can deliver packages (including GitHub described below). 0.1.6 Packages not on CRAN To install the latest version of packages from the Algorithms for Quantitative Pedology (AQP) suite off GitHub we use the remotes package. The AQP packages are updated much more frequently on GitHub than they are on CRAN. Generally, the CRAN versions (installed above) are the stable releases whereas the GitHub repositories have new features and bug fixes. remotes::install_github(&quot;ncss-tech/aqp&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/soilDB&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/soilReports&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) remotes::install_github(&quot;ncss-tech/sharpshootR&quot;, dependencies=FALSE, upgrade=FALSE, build=FALSE) 0.2 Connect to Local NASIS Establish an ODBC connection to NASIS by following the directions at the following hyperlink (ODBC Connection to NASIS). Once youve successfully established a ODBC connection, prove it by loading your NASIS selected set with the site and pedon tables for any pedons from your local area. You only need a few pedons at a minimum for this demo  too many (say, &gt;20) will make the example profile plot cluttered. Paste the below code at the command prompt (&gt;) and press enter, as you did above. Or create a new R script (Main menu: File  New File  R Script) and paste code into the Source pane (script editor window). Then, click the Run button in the top-right corner of the Script Editor or use Ctrl+Enter to run code at the cursor location / any selected code. This will execute the code in the Console. Submit the resulting plot to your mentor (from Plot pane (bottom-right): Export  Save as PDF) # load packages into the current session library(aqp) # provides &quot;SoilProfileCollection&quot; object &amp; more library(soilDB) # provides database access methods # get pedons from NASIS selected set test &lt;- fetchNASIS(from = &#39;pedons&#39;) # inspect the result str(test, max.level = 2) # make a profile plot # set margins smaller than default par(mar=c(1,1,1,1)) # make profile plot of selected set, with userpedonid as label plot(test, label=&#39;pedon_id&#39;) 0.2.1 Demonstrate a Working Connection Follow the one line example below, copy the output, and submit the results to your mentor. This will help us to verify that all of the required packages have been installed. # dump list of packages that are loaded into the current session sessionInfo() 0.3 Additional Support/Optional Readings Spatial Data Analysis and Modeling with R (highly recommended) R-Intro R for Beginners The R Inferno AQP Website and Tutorials Stats for Soil Survey Webinar Soil Data Aggregation using R Webinar "],["intro.html", "Chapter 1 Introduction 1.1 Outline", " Chapter 1 Introduction Finish this. 1.1 Outline "],["uncertainty.html", "Chapter 2 Uncertainty 2.1 Introduction 2.2 Theory of Uncertainty 2.3 Resampling to Estimate Uncertainty 2.4 Performance Metrics 2.5 Validation 2.6 Apparent Validation 2.7 Inherent Validation 2.8 Additional reading 2.9 References (Uncertainty)", " Chapter 2 Uncertainty 2.1 Introduction Validating and assessing the uncertainty of a model is just as, if not more important, than generating the model itself. Validation quantifies the models ability to explain variance in the data while uncertainty quantifies the confidence of model prediction. Uncertainty and validation assessments enable the end user to better understand model error, the nature and distribution of the input data, and the overall accuracy and spatial applicability of the model. Within a soil model, there are several sources of error: Measurement errors Interpretation errors Digitization errors Classification errors Generalization errors Interpolation errors Errors are simply the difference between reality and our representation of reality. Assessing the data structure with simple statistical measures such as mean, median and mode can be useful for understanding the central tendency of the data, but more complicated calculations are needed to get at dispersion or the variation of a property within a population to further assess error and uncertainty (Zar, 1999). Measure of Dispersion Range: The difference between the highest and lowest values measured or observed. Not always reliable because it can include outliers, error, or misclassified data. Quantiles: These refer to 25% increments in the rank of observations. Typically, the 25th and 75th quantiles are used to represent the spread of the most typical values around the central tendency. Measure of Variation * Variance: The deviation of from the mean is calculated as sum of squares (SS) to use absolute deviation (eliminate any distinction between negative and positive correlation). \\(variance (sample) = \\frac{SS}{n-1}\\) \\(SS = \\sum{(X - x)^2}\\) Standard deviation: Used to return variance to the original units \\(sd = \\sqrt{\\frac{SS}{n-1}}\\) Coefficient of variation: Scale standard deviation with mean so that multiple properties can be compared \\(CV = \\frac{SD}{x}\\) Measures of Certainty Standard Error: represents the variance of the mean that would be found with repeated sampling. Estimated by dividing standard deviation by the square root of n. The concept of standard error is important for hypothesis testing. Confidence interval: Interval in which you are confident that a given percentage (known as the confidence limit 95, 80, 75%) of the population lie. If a normal distribution is assumed, for a 95% confidence interval, it can be estimated that the value is 95% likely to fall between as SD * 1.96 +/- mean. 2.1.1 Exercise - Dispersion Create an example data-set and evaluate dispersion. library(ggplot2) # set random seed, so that we all get the same results set.seed(123) # create a sample dataset with 10 values between 20 - 60 d1 &lt;- data.frame( soil = &quot;alpha&quot;, depth = sample(20:60, size = 10, replace = TRUE) ) # create a sample dataset with 100 values between 20 - 60 d2 &lt;- data.frame( soil = &quot;beta&quot;, depth = sample(20:60, size = 100, replace = TRUE) ) # combine d1 and d2 d3 &lt;- rbind(d1, d2) aggregate(depth ~ soil, data = d3, quantile) ## soil depth.0% depth.25% depth.50% depth.75% depth.100% ## 1 alpha 22.00 33.00 39.00 45.75 56.00 ## 2 beta 22.00 32.00 42.00 50.00 60.00 # examine box plots ggplot(d3, aes(x = soil, y = depth)) + geom_boxplot() Dispersion or variance is a characteristic of the population being evaluated. While more or better sample collection might give you better precision of those estimates, we would not expect them to change the dispersion calculated. Conversely, measures of certainty of the central tendency (how sure are you of the typical value reported) depends both on the characteristic dispersion/variance and the number of samples collected. 2.1.2 Exercise - Variation and Certainty Create an example data-set and and evaluate variation and certainty. # calculate the mean of depth m &lt;- mean(d2$depth) # subtract mean from each value and square (i.e. residuals) d2$S &lt;- (d2$depth - m)^2 #calculate overall sum of squares SS &lt;- sum(d2$S) #calculate sample variance (length gives us the total number of sample/observations) SS / (length(d2$depth) - 1) ## [1] 113.3425 Note the differences in range and variance calculated for Depth in both examples (10 samples vs. 100) aggregate(depth ~ soil, data = d3, var) ## soil depth ## 1 alpha 125.5667 ## 2 beta 113.3425 Now Compare Standard Error (standard deviation / square root of n) SE &lt;- function(x) sd(x) / sqrt(length(x)) aggregate(depth ~ soil, data = d3, SE) ## soil depth ## 1 alpha 3.543539 ## 2 beta 1.064624 Why are the standard errors different? 2.2 Theory of Uncertainty At its most basic level, uncertainty is simply a lack of certainty. In soil survey, uncertainty encompasses both of these aspects: youve gathered multiple observations and you need to describe them in relation to one another, and you must predict a property or characteristic at unobserved locations. It is difficult to quantify the knowledge we have about data and information uncertainty. While we may have good data of the accuracy of our GPS, how likely are we to include that in our estimates of model error? How important is it? In other disciplines, they spend a lot of time quantifying and tracking measurement error. In soil science, we tend to treat measurement as having an exact known location and value. Given the unknowns in mapping and predicting soil properties, this is a reasonable treatment of relatively small levels of error. When using secondary information as data (or data that is actually a prior prediction or result of a model, including soil components), considering incorporated error can be crucial. One way to deal with this is through re-sampling an alternate way is to through error propagation theory. The most common way to deal with this in soil survey and digital soil mapping is to assess error through model validation. Explanatory vs. Predictive Modelling While explanatory and predictive modeling can use the same types of models, data and even questions, the errors and uncertainty are important for different reasons Explanatory or Descriptive - data are collected and analyzed in order to test causal hypothesis and observe correlations and relationships between data element. Often used at the beginning phases of soil-landscape exploration. How does the soil relate to each of the soil forming factors? Predictive - applying a model or algorithm to data for the purpose of making a prediction (in new or unknown locations) (Shueli, 2010). 2.3 Resampling to Estimate Uncertainty When calculating many basic statistical parameters, the assumption is that the data is parametric. That implies that the data is continuous (ratio or interval) and normally distributed. This is rarely the case with soil data. Soil properties are often not normally distributed (you cannot have less that 0% organic matter, for instance) and often we are trying to predict soil taxa or other nominal classes. Re-sampling is a general term that defines any procedure to repeatedly draw samples form a given data-set. You are essentially pretending to collect a series of separate samples from your sample set then calculating a statistic on that sample. Re-sampling techniques can be used on known and unknown data distributions for uncertainty estimation and validation (Good, 2001). 2.3.1 Exercise - Resampling # this bootstrap is estimating the uncertainty associated with the variance of d2$depth # an example of getting a confidence interval through bootstrapping (no assumption of a normal distribution) # abbreviate our data to simply the commands d &lt;- d2$depth n &lt;- length(d) # set number of iterations N &lt;- 50 # create a data frame to store the results boot_stats &lt;- data.frame( vars = numeric(N), means = numeric(N) ) # for each instance (i) in the set from 1 to N (50 in this case) for (i in 1:N) { # create a new variable dB from each bootstrap sample of d boot.sample = sample(d, n, replace = TRUE) boot_stats$means[i] = mean(boot.sample) boot_stats$vars[i] = var(boot.sample) } quantile(boot_stats$vars) ## 0% 25% 50% 75% 100% ## 91.54293 107.14240 112.61833 120.47970 139.81818 stripchart(boot_stats$vars) # Traditional Approach ci &lt;- c( # lower 5th l = mean(d) - 1.96 * sd(d) / sqrt(n), # upper 95th u = mean(d) + 1.96 * sd(d) / sqrt(n) ) # Compare Bootstrap to Confidence Interval quantile(boot_stats$means, c(0.05, 0.95)) ## 5% 95% ## 39.663 43.005 ci ## l u ## 39.38334 43.55666 2.4 Performance Metrics Numerical Coefficient of variation (\\(R^2\\)): % of variance explained Mean Error (ME): average error Root Mean Square Error (RMSE): average residual Categorical (derivatives of confusion matrix) Overall Accuracy: % of observations that were correctly classified, for all classes Precision (i.e. users accuracy, errors of commission (Type II): % of observations that were correctly classified, for an individual class Sensitivity (i.e. producers accuracy, errors of omission (Type I)): % of predictions that were correctly classified, for an individual class Confusion Matrix Observed Predicted No Yes No True Negative (TN) False Negative (FN) Yes False Positive (FP) True Positive (TP) 2.4.1 Exercise - Numeric Metrics ### Numeric error metrics ### Linear model example # Create a Ficticous Data-set d_num &lt;- data.frame( depth = 21:60 + rnorm(40, mean = 0, sd = 10), slope = 60:21 ) num_lm &lt;- lm(depth ~ slope, data = d_num, y = TRUE) # R2 summary(num_lm)$r.squared ## [1] 0.5795156 # or predicted &lt;- num_lm$fitted.values observed &lt;- num_lm$y cor(predicted, observed)^2 ## [1] 0.5795156 # ME mean(predicted - observed) ## [1] 2.220446e-16 # RMSE sqrt(mean((predicted - observed)^2)) ## [1] 11.48239 2.4.2 Exercise - Categorical Metrics # Create a Ficticous Data-set r &lt;- runif(175) idx &lt;- sample(which(r &gt; 0.5), 75) r &lt;- r[-idx] d_cat = data.frame( predicted = r, observed = (r + rnorm(100, sd = 0.2)) &gt; 0.5 ) # Compute Confusion Matrix cm &lt;- table(predicted = d_cat$predicted &gt; 0.5, observed = d_cat$observed) print(cm) ## observed ## predicted FALSE TRUE ## FALSE 66 14 ## TRUE 2 18 # or library(caret) ## Loading required package: lattice confusionMatrix(cm, positive = &quot;TRUE&quot;) ## Confusion Matrix and Statistics ## ## observed ## predicted FALSE TRUE ## FALSE 66 14 ## TRUE 2 18 ## ## Accuracy : 0.84 ## 95% CI : (0.7532, 0.9057) ## No Information Rate : 0.68 ## P-Value [Acc &gt; NIR] : 0.0002249 ## ## Kappa : 0.5918 ## ## Mcnemar&#39;s Test P-Value : 0.0059595 ## ## Sensitivity : 0.5625 ## Specificity : 0.9706 ## Pos Pred Value : 0.9000 ## Neg Pred Value : 0.8250 ## Prevalence : 0.3200 ## Detection Rate : 0.1800 ## Detection Prevalence : 0.2000 ## Balanced Accuracy : 0.7665 ## ## &#39;Positive&#39; Class : TRUE ## # Examine threshoold ggplot(d_cat, aes(x = predicted, color = observed)) + geom_density(lwd = 1.5) # Trade Precision for Sensitivity by Varying the Threshold cm &lt;- table(predicted = d_cat$predicted &gt; 0.4, observed = d_cat$observed) confusionMatrix(cm, positive = &quot;TRUE&quot;) ## Confusion Matrix and Statistics ## ## observed ## predicted FALSE TRUE ## FALSE 56 8 ## TRUE 12 24 ## ## Accuracy : 0.8 ## 95% CI : (0.7082, 0.8733) ## No Information Rate : 0.68 ## P-Value [Acc &gt; NIR] : 0.005378 ## ## Kappa : 0.5552 ## ## Mcnemar&#39;s Test P-Value : 0.502335 ## ## Sensitivity : 0.7500 ## Specificity : 0.8235 ## Pos Pred Value : 0.6667 ## Neg Pred Value : 0.8750 ## Prevalence : 0.3200 ## Detection Rate : 0.2400 ## Detection Prevalence : 0.3600 ## Balanced Accuracy : 0.7868 ## ## &#39;Positive&#39; Class : TRUE ## 2.5 Validation Validation refers to the process and the result of a process where the validity of a model is tested. That is, how well does the model represent reality? There are varying degrees of formality and thoroughness that can be used in validation. While multiple stages of the modeling process can be validated, usually its the output of the model that is investigated and reported. You can group initial validation into three broad groups: Expert evaluation, Theoretical Analysis and Prediction Accuracy. Expert Evaluation: In this case, the model output is inspected by an expert user. The first evaluator will be you (the developer), but ideally an outside expert will be utilized. This is often a step in an iterative process. Evaluate the model output, does it make sense, do you see things that need to be improved? Then make changes to the model to improve the output. Theoretical Analysis: compare the results of the model to what is theoretically possible. In systems modeling, this might include diagnostics statistics including residual analysis, cross-correlation of variables and outputs, sensitivity analysis and model analysis such as Akaike Information Criterion (AIC). This can also include simple comparison of output to known possible values. This is especially important for linear regression where the slope of the model is assumed to be steady no matter the values of the dependent variables. Prediction Accuracy: The correctness of the parameter being predicted by the model (soil taxa, property etc.). Ideally this is done with an independent set of data. In soil science, we typically use the term model validation to refer to a statistical analysis that assesses how well a model will predict at an unknown location. A complete model should have a formal statistical evaluation that can be reported and stored as model and output meta-data. That is the portion of validation we will focus. For this discussion, validation can be thought of as an assessment of prediction error and variance. Three types of validation used in the course Apparent - Performance on sample used to develop model Internal - Performance on population underlying the sample External - Performance on related (similar/adjacent) but independent population 2.6 Apparent Validation In this exploratory and explanatory phase you are looking for relationships that can be used later for predictive purposes. Use Goodness of fit tests on all the data in your sample Correlation (R2, rho, etc.) P-values (test questions about individual or combinations of variables) Analyze Residuals (distribution of model errors) to diagnose modeling problems. One or more of these issues indicate that one or more important variables was omitted from the model or that an incorrect functional form was used (linear when the function should be non-linear) Heteroscedasticity Normality Spatial distribution (auto-correlation) 2.7 Inherent Validation 2.7.1 Split-sample - A single partition of the data into a learning and a calibration set. Achieve an independent validation by partitioning the samples into calibration or training and validation data-sets (70% of the samples available are recommended for calibration) Build model on calibration (training) data-set Test model on validation (test) data-set Report accuracy on the validation data-set This method is relatively simple (conceptually and computationally). Results depend on having an adequate sample size to both develop and test the model. 2.7.2 Cross-validation - Alternate development and validation Leave-One-Out Cross-Validation (LOOCV) One observation is used for testing and all others are used to develop model Repeat n (total number of observations) times Average error over n The mean of the accuracy is the expected accuracy of the model (this assumes that new data is from the same population) (Efron, 1983) k-fold Cross-Validation (k-fold) CV Randomly divide observations into calibration and validation sets. Repeat k times, each time one k group is used for error estimates Average error of k Less computationally intensive than LOOCV, but it is more robust and can be done with smaller sample sizes than a simple split. Several R packages have tools to cross-validate predictions, including: DAAG and boot for lm() and glm() objects, caret, rms, ### Linear model example # Create folds folds &lt;- createFolds(d_num$depth, k = 10) # Cross validate lm_cv &lt;- lapply(folds, function(x) { train = d_num[-x,] test = d_num[x,] model = lm(depth ~ ., data = train, y = TRUE) actual = test$depth predict = predict(model, test) RMSE = sqrt(mean((actual - predict)^2, na.rm = TRUE)) R2 = cor(actual, predict, use = &quot;pairwise&quot;)^2 return(c(RMSE = RMSE, R2 = R2)) } ) # Convert to a data.frame lm_cv &lt;- do.call(rbind, lm_cv) # Summarize results summary(lm_cv) ## RMSE R2 ## Min. : 5.115 Min. :0.07494 ## 1st Qu.: 8.806 1st Qu.:0.76935 ## Median :11.283 Median :0.82930 ## Mean :11.552 Mean :0.74810 ## 3rd Qu.:15.006 3rd Qu.:0.93548 ## Max. :17.183 Max. :0.98633 2.7.3 Subsample (Resampling or sample simulation) In this method, the leave-out method can be random (Bootstrap) or observation selection can use a more sophisticated method to select observations to represent the population including Monte Carlo (Molarino, 2005) and .632+bootstrap of Efron &amp; Tibshirani (1997). The details of those arent important, except to know that they can give you a better idea of the robustness of your model. As with re-sampling for uncertainty estimation, observations are repeatedly sampled Select a number of samples (Randomly or from known distribution). Develop the model Estimate model accuracy on unselected samples Repeat the process (with independent sample) a large number of times, 500 - 5,000. The expected model accuracy is then the mean of the estimates. NOTE: The BEST model should not be assumed to be the one that makes the truest predictions. Beware of over-fitting. When a model is over-fit, it predicts due to very specific quirks in the calibration data set and not due to explanatory relationships that will apply to validation and independent data-sets. One strategy to avoid this situation is to build models with as few variables as possible. Parsimonious models (those that use the least amount of information possible to obtain the same result or convey the same meaning) often have higher predicative validity. The use of metrics such as Akaikes Information Criterion (AIC) can be helpful for balancing error and parameter minimization. 2.7.4 External Validation In this case, an independent data-set is used as the test case. Independent observations predicted with model Errors (ME, RMSE) calculated on predicted vs. actual Some exploratory analysis can be helpful to diagnose and explain model performance. The use of validation will be demonstrated as part of each modeling section. The size of the data-set used, understanding of the variables involved and the nature of the statistical models and algorithms used all influence which validation techniques are most convenient and appropriate. 2.8 Additional reading James, G., D. Witten, T. Hastie, and R. Tibshirani, 2014. An Introduction to Statistical Learning: with Applications in R. Springer, New York. http://www-bcf.usc.edu/~gareth/ISL/ Hastie, T., R. Tibshirani, and J. Friedman 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, New York. http://statweb.stanford.edu/~tibs/ElemStatLearn/ 2.9 References (Uncertainty) Efron, B., Tibshirani, R.J., 1993. An introduction to the bootstrap. Monographs on Statistics and Applied Probability, vol. 57. Chapman &amp; Hall, London, UK. Good, P.I., 2001. Resampling methods. Birkhäuser. James, G., D. Witten, T. Hastie, and R. Tibshirani, 2014. An Introduction to Statistical Learning: with Applications in R. Springer, New York. http://www-bcf.usc.edu/~gareth/ISL/ Molinaro, A. M. (2005). Prediction error estimation: a comparison of resampling methods. Bioinformatics, 21(15), 3301-3307. doi:10.1093/bioinformatics/bti499 Shmueli, G.. (2010). To Explain or to Predict?. Statistical Science, 25(3), 289:310. Retrieved from http://www.jstor.org/stable/41058949 Zar, J.H., 1999. Biostatistical analysis. Pearson Education India. "],["numerical-tax.html", "Chapter 3 Numerical Taxonomy and Ordination 3.1 Introduction 3.2 Whirlwind Tour 3.3 Excercises 3.4 Practical Applications 3.5 References (Numerical Taxonomy and Ordination)", " Chapter 3 Numerical Taxonomy and Ordination 3.1 Introduction Nearly every aspect of soil survey involves the question: Is X more similar to Y or to Z? The quantification of similarity within a collection of horizons, pedons, components, map units, or even landscapes represents an exciting new way to enhance the precision and accuracy of the day-to-day work of soil scientists. After completing this module, you should be able to quantitatively organize objects based on measured or observed characteristics in a consistent and repeatable manner. Perhaps you will find a solution to the long-standing similar or dissimilar question. 3.1.1 Objectives Learn essential vocabulary used in the field of numerical taxonomy. Review some of the literature. Gain experience with R functions and packages commonly used for clustering and ordination. Learn how to create and interpret a distance matrix and appropriate distance metrics. Learn how to create and interpret a dendrogram. Lean the basics and application of hierarchical clustering methods. Lean the basics and application of partitioning clustering methods. Learn the basics and application of ordination methods. Apply skills to a range of data sources for soils and vegetation. Apply techniques from numerical taxonomy to addressing the similar or dissimilar question. Learn some strategies for coping with missing data. 3.2 Whirlwind Tour Most of the examples featured in this whirlwind tour are based on soil data from McGahan, D.G., Southard, R.J, Claassen, V.P. 2009. Plant-available calcium varies widely in soils on serpentinite landscapes. Soil Sci. Soc. Am. J. 73: 2087-2095. These data are available in the dataset sp4 that is built into aqp package for R. 3.2.1 Similarity, Disimilarty, and Distance There are shelves of books and thousands of academic articles describing the theory and applications of clustering and ordination methods. This body of knowledge is commonly described as the field of numerical taxonomy (Sneath and Sokal 1973). Central to this field is the quantification of similarity among individuals based on a relevant set of characteristics. Individuals are typically described as rows of data with a single characteristic per column, together referred to as a data matrix. For example: name clay sand Mg Ca CEC_7 A 21 46 25.7 9.0 23.0 ABt 27 42 23.7 5.6 21.4 Bt1 32 40 23.2 1.9 23.7 Bt2 55 27 44.3 0.3 43.0 Quantitative measures of similarity are more conveniently expressed as distance, or dissimilarity; in part because of convention and in part because of computational efficiency. In the simplest case, dissimilarity can be computed as the shortest distance between individuals in property-space. Another name for the shortest linear distance between points is the Euclidean distance. Evaluated in two dimensions (between individuals \\(p\\) and \\(q\\)), the Euclidean distance is calculated as follows: \\[D(p,q) = \\sqrt{(p_{1} - q_{1})^{2} + (p_{2} - q_{2})^{2}}\\] where \\(p_{1}\\) is the 1st characteristic (or dimension) of individual \\(p\\). There are many other ways to define distance (e.g. distance metrics), but they will be covered later. Using the sand and clay percentages from the data above, dissimilarity is represented as the length of the line connecting any two individuals in property space. The following is a matrix of all pair-wise distances (the distance matrix): A ABt Bt1 Bt2 A 0.0 7.2 12.5 38.9 ABt 7.2 0.0 5.4 31.8 Bt1 12.5 5.4 0.0 26.4 Bt2 38.9 31.8 26.4 0.0 Note that this is the full form of the distance matrix. In this form, zeros are on the diagonal (i.e. the distance between an individual and itself is zero) and the upper and lower triangles are symmetric. The lower triangle is commonly used by most algorithms to encode pair-wise distances. A ABt Bt1 ABt 7.2 Bt1 12.5 5.4 Bt2 38.9 31.8 26.4 Interpretation of the matrix is simple: Individual A is more like ABt than like Bt1. It is important to note that quantification of dissimilarity (distance) among individuals is always relative: X is more like Y, as compared to Z. 3.2.1.1 Distances You Can See: Perceptual Color Difference Simulated redoximorphic feature colors, constrast classes and CIE \\(\\Delta{E_{00}}\\). Details here. 3.2.2 Standardization of Characteristics Euclidean distance doesnt make much sense if the characteristics do not share a common unit of measure or range of values. Nor is it relevant when some characteristics are categorical and some are continuous. For example, distances are distorted if you compare clay (%) and exchangeable Ca (cmol/kg). In this example, exchangeable Ca contributes less to the distance between individuals than clay content, effectively down-weighting the importance of the exchangeable Ca. Typically, characteristics are given equal weight (Sneath and Sokal 1973); however, weighting is much simpler to apply after standardization. Standardization of the data matrix solves the problem of unequal ranges or units of measure, typically by subtraction of the mean and division by standard deviation (z-score transformation). \\[x_{std} = \\frac{x - mean(x)}{sd(x)}\\] There are several other standardization methods covered later. The new data matrix looks like the following: name clay sand Mg Ca CEC_7 A -0.86 0.88 -0.35 1.23 -0.47 ABt -0.45 0.40 -0.55 0.36 -0.63 Bt1 -0.12 0.15 -0.60 -0.59 -0.40 Bt2 1.43 -1.43 1.49 -1.00 1.49 Using the standardized data matrix, distances computed in the property space of clay and exchangeable calcium are unbiased by the unique central tendency or spread of each character. Rarely can the question of dissimilarity be answered with only two characteristics (dimensions). Euclidean distance, however, can be extended to an arbitrary number of \\(n\\) dimensions. \\[D(p,q) = \\sqrt{ \\sum_{i=1}^{n}{(p_{i} - q_{i})^{2}} }\\] In the equation above, \\(i\\) is one of \\(n\\) total characteristics. Imagining what distance looks like is difficult if there are more than three dimensions. Instead, examine the distance matrix calculated using all five characteristics. Rescaling to the interval {0,1}. You can now begin to describe dissimilarity between individuals using an arbitrary number of (relevant) characteristics. You can make statements like The A horizon is roughly 2x more similar to the ABt horizon than it is to the Bt horizon. Although this is a trivial example, the utility of generalizing these methods to soil survey operations should be obvious. 3.2.2.1 Review and Discuss What are the data matrix and distance matrix? What is standardization, and why is it important? What is the largest impediment to creating a distance matrix from NASIS and KSSL data? Key point: Each characteristic is its own dimension in property-space. Sand, clay, and CEC = 3 dimensions. Sand, clay, CEC, OC, and horizon depth = 5 dimensions. Multiple dimensions are simple to define in code but are hard to visualize. The curse of dimensionality. 3.2.3 Missing Data Missing data are a fact of life. Soil scientists are quite familiar with missing lab data (Why didnt we request optical grain counts?) or missing essential NASIS pedon data elements, such as horizon bottom depth, estimated clay fraction, or pH. Nearly all of the methods described in this document are very sensitive to missing data. In other words, they wont work! Following are a couple of possible solutions: Fix the missing data if at all possible, Estimate the missing data values from know relationships to other properties or a group-wise mean or median, or Remove records containing any missing data. 3.2.4 Visualizing Pair-Wise Distances: The Dendrogram Dendrograms are a convenient approximation of pair-wise distances between individuals (after application of hierarchical grouping criteria; more on this later). Dissimilarity between branches is proportional to the level at which branches merge: branching at higher levels (relative to the root of the tree) suggests greater dissimilarity; branching at lower levels suggests greater similarity. Consider the previous example in which distance between individuals was defined in terms of sand and clay percentages. Interpretation is simple. Euclidean distance in property-space is directly proportional to branching height in the corresponding dendrogram. Visualizing the geometry of pair-wise distances in more than three dimensions is difficult. A dendrogram, however, can conveniently summarize a distance matrix created from an arbitrary number of characteristics (dimensions). It is important to note that some information about pair-wise distances is lost or distorted in the dendrogram. Distortion is least near the terminal leaves of the dendrogram. This phenomena is analogous to the distortion generated by a map projection. It is impossible to flatten a higher-dimensional entity to a lower-dimensional form without causing distortion. 3.2.5 Re-arranging a Dendrogram for Clarity The branches of a dendrogram can be rotated like a mobile, so that the final ordering of the terminal leaves approximates an alternate ranking. For example, branches of the following dendrogram (right-hand side) have been rotated to approximate the expected hydrologic gradient from summit to toeslope. 3.2.5.1 Review and Discuss Do you have any questions about dendrogram interpretation? Dendrograms are used extensively in the rest of this chapter. If you were explaining how to interpret a dendrogram to someone, would you start at the roots or leaves? Why? 3.2.6 Cluster Analysis: Finding Groups in Data Cluster analysis is a massive topic that deals with the seemingly simple task of finding useful groups within a dataset. This topic and the methods used are also referred to as unsupervised classification in the fields of remote sensing and GIS. All of the available algorithms will find groups in a given dataset; however, it is up to the subject expert to determine the following: Suitable characteristics and standardization method, Appropriate clustering algorithm, Criteria to determine the right number of clusters, Limitations of the selected algorithm, Interpretation of the final grouping based on subject knowledge, and The possibility of needing to start over at step 1. 3.2.6.1 Using Color to Communicate Results of Clustering or Ordination Note that the widespread use of color in the following examples is not for aesthetic purposes. Colors are convenient for tri-variate data-spaces because you can visually integrate the information into a self-consistent set of classes. 3.2.6.2 Hierarchical Clustering Hierarchical clustering is useful when a full distance matrix is available and the optimal number of clusters is not yet known. This form of clustering creates a data structure that can encode grouping information from one cluster to as many clusters as there are individuals. The expert must determine the optimal place to cut the tree and generate a fixed set of clusters. The results from a hierarchical clustering operation are nearly always presented as a dendrogram. 3.2.6.2.1 Methods There are two main types of hierarchical clustering. Agglomerative: Start with individuals and iteratively combine into larger and larger groups. Divisive: Start with all individuals and iteratively split into smaller and smaller groups. Both methods are strongly influenced by the choice of standardization method and distance metric. Both methods require a full, pair-wise distance matrix as input. This requirement can limit the use of hierarchical clustering to datasets that can be fit into memory. The agglomerative methods also depend on the choice of linkage criterion. Some of these criteria include: Average: usually generates spherical clusters, default in agnes() and recommended by (Kaufman and Rousseeuw 2005), Single linkage: usually generates elongated clusters, Complete linkage: usually generates small, compact clusters, Wards (minimum variance) method, Weighted average (or Gowers) linkage: spherical clusters of roughly equal size, and Flexible strategy: adjustable linkage based on parameter \\(\\alpha\\). Flexible UPGMA (Belbin, Faith, and Milligan 1992): adjustable linkage based on parameter \\(\\beta\\). See (Kaufman and Rousseeuw 2005), (Arkley 1976), (Legendre and Legendre 1998), and agnes() manual page for a detailed description of these linkage criteria. Selection of linkage criteria can be quantitatively evaluated using the cophenetic correlation; and index of how well a dendrogram preserves the original pair-wise distances. This thread on StackExchange has a nice summary of various linkage criteria. I really like these slides by Brian Tibshirani on the topic of agglomerative method selection. More on this later. 3.2.6.2.1.1 Review and Discuss The simplicity and lack of decisions make the divisive method convenient for most work. The top-down approach is similar to the way in which we describe soil morphology and taxonomy. Linkage criteria should be selected based on the hypothesized shape of clusters or prior subject knowledge. When selecting a method, read/think about it; dont just go fishing. Personal opinion: I usually select divisive hierarchical clustering over the other methods: structures seem to be generally more interpretable. 3.2.6.3 Centroid and Medoid (Partitioning) Clustering Centroid and medoid cluster analyses are commonly referred to as k-means-style analysis. K-means, however, is just one of many possible clustering algorithms that partition property-space into a fixed number of groups. These type of algorithms can be applied to very large datasets because they do not rely on the distance matrix. Instead, they are based on an iterative shuffling of group centroids until some criterion is minimized, for example, the mean variance within groups. 3.2.6.3.1 Methods This section describes three (out of many) of the most important partitioning-type algorithms. K-means: Groups of individuals are partitioned around newly created centroids. Resulting clusters approximately spherical and contain an approximately equal number of individuals. K-medoids: Groups of individuals are partitioned around selected medoids. Fuzzy clustering: Individuals are assigned a fuzzy membership value for each partition of property-space. All of these methods are sensitive to the type of standardization applied to the characteristics. These methods rely on iterative minimization of one or more criteria; therefore, each clustering run may generate slightly different output. Most implementations re-run the algorithm until it stabilizes. 3.2.6.3.1.1 Review and Discuss What is the difference between a medoid and a centroid? Can you think of a way in which both concepts could be applied to the grouping of soils data? medoids are tied to individuals, centroids are hypothetical (calculated) Fuzzy clustering is also referred to as soft clustering, while the other two methods are referred to as hard clustering. Sometimes, using both can be helpful, especially if individuals straddle the line between groups. Each method has its own strength and weakness, for example here is a nice summary of the limitations of k-means. 3.2.7 Ordination: Visualization in a Reduced Space Humans are generally quite good at extracting spatial patterns, almost instantly, from two dimensional fields: faces, written language, etc. Sadly, this ability does not extend beyond two or three dimensions. The term ordination refers to a suite of methods that project coordinates in a high-dimensional space into suitable coordinates in a low-dimensional (reduced) space. Map projections are a simple form of ordination: coordinates from the curved surface of the Earth are projected to a two-dimensional plane. As with any projection, there are assumptions, limitations, and distortions. Carl Sagan gives a beautiful demonstration of this concept using shadows, in this excerpt from Cosmos. Here is another excellent demonstration based on handwriting recognition. Hole and Hironaka (1960) were some of the first pedologists to apply ordination methods to soils data. The main figure from their classic paper was hand-drawn, based on a physical model (constructed from wooden dowels and spheres!) of the ordination. 3.2.7.1 Major Types of Ordination Of the many possible ordination methods, there are two major types to keep in mind: Constrained ordination: coordinates in the reduced space are subject to some kind of constraint (more rigid, simple interpretation). Principal component analysis (PCA) is one of the simplest and most widely used ordination methods. The reduced space (principal components) are defined by linear combinations of characteristics. Unconstrained ordination: coordinates in the reduced space are not subject to any kind of constraint (more flexible, less interpretable). Non-metric multidimensional scaling (nMDS) attempts to generate a reduced space that minimizes distortion in proportional similarity; i.e., similar individuals are near each other in the reduced space, dissimilar individuals are farther apart. The non-metric adjective implies that exact distances are not preserved. See (Legendre and Legendre 1998) for a comprehensive listing of methods, associated theory, and ecological applications. 3.2.7.2 An Example of nMDS Applied to Soil Data The following example is based on a data matrix containing lab measurements of clay fraction, sand fraction, exchangeable Ca, exchangeable Mg, and CEC measured by NH4-Ac at pH 7. name clay sand Mg Ca CEC_7 A -0.41 0.21 0.06 0.44 -0.23 ABt 0.04 -0.07 -0.06 -0.13 -0.38 Bt1 0.41 -0.21 -0.09 -0.74 -0.16       Note that distances between individuals, within clusters, and between clusters is more apparent in the nMDS representation of the distance matrix. Similar information is embedded in the dendrogram but it is not as intuitive. ## species scores not available 3.2.7.2.1 Interpreting the Results We can conveniently annotate the results of an ordination with contextual information, such as the approximate trends in clay content or CEC. Note that ordination methods represent a flattening of multi-dimensional data space, and in the case of nMDS, preserve proportional (vs. exact) pair-wise distances. Therefore, it is quite common for surfaces (the contours below) fit to the 2D ordination to have complex patterns. ## species scores not available ## species scores not available Putting the clustering results into context is important: recall that we are working with individuals that represent genetic horizons that have been clustered according to 5 physical / chemical properties (characteristics). Differences in CEC by NH4-Ac are one of the strongest contributors to the pair-wise distances and overall clustering structure. 3.2.8 Review and Discuss Which visualization of the distance matrix was simpler to interpret: Dendrogram or ordination by nMDS? Do you have any questions about the figures? Do you have any guesses on what the clusters represent? 3.2.9 Pair-Wise Distances Between Soil Profiles ## Computing dissimilarity matrices from 10 profiles [0.09 Mb] This is a complex topic, described in a supplemental set of slides. If you want for more detailed information, see this relevant paper. 3.2.10 Final Discussion Missing data strategy Meaningful characteristics Standardization of characteristics Distance metric Clustering algorithm Number of clusters Application to soil survey and ESD 3.3 Excercises This is the fun part. 3.3.1 Set Up the R Session Install R packages as needed. Open a new R script file to use as you follow along. # load libraries library(aqp) library(soilDB) library(sharpshootR) library(cluster) library(ape) library(RColorBrewer) library(vegan) library(MASS) library(colorspace) library(viridis) 3.3.1.1 Data Sources Most of the examples used in the following exercises come from these sources: Built-in data sets from the aqp and soilDB packages (sp4, gopheridge, and loafercreek). Results from fetchNASIS(): pedon data from the local NASIS selected set. Results from fetchKSSL(): lab characterization data from the SoilWeb snapshot. Results from fetchOSD(): basic morphologic and taxonomic data from the SoilWeb snapshot. Results from SDA_query(): live SSURGO spatial and tabular data from Soil Data Access Data from SSR 2, as CSV, downloaded from class GitHub site In most cases, you can edit the examples and swap-in just about any data that are in a SoilProfileCollection object. For example, pedons from your local NASIS selected set can be loaded with fetchNASIS(). 3.3.1.1.1 Try it! Tinker with some SoilProfileCollection objects. Get some data using one of the methods listed above. If you need help, see the manual pages for examples (?fetchKSSL) or the SoilProfileCollection tutorial. Determine the number of profiles and horizons within the collection. View and extract some site and horizon attributes. Generate some soil profile sketches. 3.3.2 Evaluating Missing Data The aqp package provides two functions for checking the fraction of missing data within a SoilProfileCollection object. The first function (evalMissingData) generates an index that ranges from 0 (all missing) to 1 (all present) for each profile. This index can be used to subset or rank profiles for further investigation. The second function (missingDataGrid) creates a visualization of the fraction of data missing within each horizon. Both functions can optionally filter-out horizons that dont typically have data, for example Cr, Cd, and R horizons. The following examples are based on the gopheridge sample dataset. evalMissingData # example data data(&quot;gopheridge&quot;, package = &quot;soilDB&quot;) # compute data completeness gopheridge$data.complete &lt;- evalMissingData(gopheridge, vars = c(&#39;clay&#39;, &#39;sand&#39;, &#39;phfield&#39;), name = &#39;hzname&#39;, p = &#39;Cr|R|Cd&#39;) # check range in missing data summary(gopheridge$data.complete) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.2143 0.4672 1.0000 1.0000 # rank new.order &lt;- order(gopheridge$data.complete) # plot along data completeness ranking par(mar=c(3,0,1,1)) plot(gopheridge, plot.order=new.order, print.id=FALSE, name=&#39;&#39;) # add axis, note re-ordering of axis labels axis(side=1, at=1:length(gopheridge), labels = round(gopheridge$data.complete[new.order], 2), line=-2, cex.axis=0.65, las=2) title(&#39;Gopheridge pedons sorted according to data completeness (clay, sand, pH)&#39;) missingDataGrid # view missing data as a fraction res &lt;- missingDataGrid(gopheridge, max_depth=100, vars=c(&#39;clay&#39;, &#39;sand&#39;, &#39;phfield&#39;), filter.column=&#39;hzname&#39;, filter.regex = &#39;Cr|R|Cd&#39;, main=&#39;Fraction of missing data (clay, sand, pH)&#39;, cols = viridis(10)) # plot figure print(res$fig) # check results head(res$summary) ## peiid clay sand phfield ## 1 1137354 0 100 100 ## 2 1147151 0 0 100 ## 3 1147190 0 0 100 ## 4 242808 29 29 0 ## 5 252851 29 29 29 ## 6 268791 0 0 0 For now, extract those profiles that have a complete set of field-described clay, sand, or pH values for later use. # be sure to read the manual page for this function gopheridge.complete &lt;- subset(gopheridge, data.complete &gt; 0.99) # another way idx &lt;- which(gopheridge$data.complete &gt; 0.99) gopheridge.complete &lt;- gopheridge[idx, ] # looks good par(mar=c(0,0,3,1)) plot(gopheridge.complete, color=&#39;clay&#39;, id.style=&#39;side&#39;, label=&#39;pedon_id&#39;) 3.3.3 More on the Distance Matrix and How to Make One The following three functions are essential to the creation of a distance matrix: dist(): This function is in base R, is simple and fast, and has a limited number of distance metrics. daisy(): This function is in cluster package, has a better selection of distance metrics, and has simple standardization. Much more convenient than dist(). vegdist(): This function is in vegan package, has many distance metrics, and is primarily designed for species composition data. The following is a short demonstration: # get some example data from the aqp package data(&#39;sp4&#39;, package = &#39;aqp&#39;) # subset select rows and columns sp4 &lt;- sp4[1:4, c(&#39;name&#39;, &#39;clay&#39;, &#39;sand&#39;, &#39;Mg&#39;, &#39;Ca&#39;, &#39;CEC_7&#39;)] row.names(sp4) &lt;- sp4$name # compare distance functions # Euclidean distance, no standardization round(dist(sp4[, -1], method = &#39;euclidean&#39;)) ## A ABt Bt1 ## ABt 8 ## Bt1 15 7 ## Bt2 48 44 39 # Euclidean distance, standardization round(daisy(sp4[, -1], stand = TRUE, metric = &#39;euclidean&#39;), 2) ## Dissimilarities : ## A ABt Bt1 ## ABt 1.45 ## Bt1 2.73 1.36 ## Bt2 6.45 5.65 4.91 ## ## Metric : euclidean ## Number of objects : 4 # Gower&#39;s generalized distance metric (includes standardization) round(vegdist(sp4[, -1], method = &#39;gower&#39;), 2) ## A ABt Bt1 ## ABt 0.19 ## Bt1 0.32 0.16 ## Bt2 0.96 0.84 0.69 3.3.3.1 Distance Calculations with Categorical Data The following example is excerpted from A Novel Display of Categorical Pedon Data. This example illustrates an application of clustering binary data (presence or absence of a diagnostic feature). Internally, the diagnosticPropertyPlot() function uses the daisy() function to compute pair-wise distances using the general dissimilarity coefficient of Gower (Gower 1971). A concise summary of this distance metric is in (Kaufman and Rousseeuw 2005). # load some example NASIS data data(loafercreek, package=&#39;soilDB&#39;) # cut-down to a subset, first 20 pedons loafercreek &lt;- loafercreek[1:20, ] # get depth class sdc &lt;- getSoilDepthClass(loafercreek) site(loafercreek) &lt;- sdc # diagnostic properties to consider, no need to convert to factors v &lt;- c(&#39;lithic.contact&#39;, &#39;paralithic.contact&#39;, &#39;argillic.horizon&#39;, &#39;cambic.horizon&#39;, &#39;ochric.epipedon&#39;, &#39;mollic.epipedon&#39;, &#39;very.shallow&#39;, &#39;shallow&#39;, &#39;mod.deep&#39;, &#39;deep&#39;, &#39;very.deep&#39;) # do the analysis and save the results to object &#39;x&#39; x &lt;- diagnosticPropertyPlot(loafercreek, v, k=5, grid.label=&#39;bedrckkind&#39;, dend.label = &#39;taxonname&#39;) If you are wondering what is in the object x, the str() function or manual page (?diagnosticPropertyPlot) can help. 3.3.4 Hierachrical Clustering The go-to functions for hierarchical clustering are as follows: hclust(): This function is agglomerative, is in base R, requires a distance matrix, and implements most of the commonly used linkage criteria. agnes(): This function is agglomerative, is in cluster package, can perform standardization and distance calculations, and implements more linkage criteria. diana(): This function is divisive, is in cluster package, can perform standardization and distance calculations. 3.3.4.1 Basic Agglomerative Hierarchical Clustering with hclust The hclust() function and resulting hclust-class objects are simple to use, but limited. # re-make data, this time with all profiles data(&#39;sp4&#39;, package = &#39;aqp&#39;) sp4 &lt;- sp4[, c(&#39;name&#39;, &#39;clay&#39;, &#39;sand&#39;, &#39;Mg&#39;, &#39;Ca&#39;, &#39;CEC_7&#39;)] # distance matrix d &lt;- daisy(sp4[, -1], metric = &#39;euclidean&#39;, stand = TRUE) # hierachical clustering with base function hclust sp4.h &lt;- hclust(d, method = &#39;ward.D&#39;) sp4.h$labels &lt;- sp4$name # plot with basic plotting method... not many options here par(mar=c(2,4,2,2)) plot(sp4.h, font=2, cex=0.85) # ID clusters after cutting tree rect.hclust(sp4.h, 4) 3.3.4.2 Better Plots via ape Package This example uses a different approach to plotting based on functions and classes from the ape package. # re-make data, this time with all profiles data(&#39;sp4&#39;, package = &#39;aqp&#39;) sp4 &lt;- sp4[, c(&#39;name&#39;, &#39;clay&#39;, &#39;sand&#39;, &#39;Mg&#39;, &#39;Ca&#39;, &#39;CEC_7&#39;)] # distance matrix d &lt;- daisy(sp4[, -1], metric = &#39;euclidean&#39;, stand = TRUE) # divising clustering dd &lt;- diana(d) # convert to ape class, via hclust class h &lt;- as.hclust(dd) h$labels &lt;- sp4$name p &lt;- as.phylo(h) # define some nice colors col.set &lt;- brewer.pal(9, &#39;Set1&#39;) # cut tree into 4 groups groups &lt;- cutree(h, 4) # make color vector based on groups cols &lt;- col.set[groups] The plot methods for phylo class objects are quite flexible. Be sure to see the manual page ?plot.phylo. par(mar=c(1,1,1,1), mfcol=c(2,2)) plot(p, label.offset=0.125, direction=&#39;right&#39;, font=1, cex=0.85, main=&#39;dendrogram&#39;) tiplabels(pch=15, col=cols) plot(p, type=&#39;radial&#39;, font=1, cex=0.85, main=&#39;radial&#39;) tiplabels(pch=15, col=cols) plot(p, type=&#39;fan&#39;, font=1, cex=0.85, main=&#39;fan&#39;) tiplabels(pch=15, col=cols) plot(p, type=&#39;unrooted&#39;, font=1, cex=0.85, main=&#39;unrooted&#39;) tiplabels(pch=15, col=cols) 3.3.4.3 Evaluation of Dendrogram Representation Re-visiting our sample data from before, develop hierarchical clusterings using several strategies (methods / linkage criteria). # re-make data, this time with all profiles data(&#39;sp4&#39;, package = &#39;aqp&#39;) sp4 &lt;- sp4[, c(&#39;name&#39;, &#39;clay&#39;, &#39;sand&#39;, &#39;Mg&#39;, &#39;Ca&#39;, &#39;CEC_7&#39;)] # distance matrix d &lt;- daisy(sp4[, -1], metric = &#39;euclidean&#39;, stand = TRUE) # hierarchical clustering based on several strategies # agglomerative h.avg &lt;- agnes(d, method=&#39;average&#39;) h.single &lt;- agnes(d, method=&#39;single&#39;) h.complete &lt;- agnes(d, method=&#39;complete&#39;) h.ward &lt;- agnes(d, method=&#39;ward&#39;) h.flexible &lt;- agnes(d, method=&#39;gaverage&#39;, par.method = 0.01) # divisive h.div &lt;- diana(d) The correlation between original distance matrix and cophenetic distance matrix is a reasonable index of how faithfully a dendrogram preserves the original pair-wise distances. # agglomerative hierarchical clustering with various linkage criteria corr.avg &lt;- cor(d, cophenetic(h.avg)) corr.single &lt;- cor(d, cophenetic(h.single)) corr.complete &lt;- cor(d, cophenetic(h.complete)) corr.ward &lt;- cor(d, cophenetic(h.ward)) corr.flexible &lt;- cor(d, cophenetic(h.flexible)) # divisive hierarchical clustering corr.div &lt;- cor(d, cophenetic(h.div)) Combine the results into a table for quick comparison. Note that the agnes and diana functions provide an agglomerative / divisive coefficient that can be used to evaluate clustering efficiency (e.g. cluster size and compactness). Depending on the application, one metric may be more informative than the other. res &lt;- data.frame( method=c(&#39;average&#39;, &#39;single&#39;, &#39;complete&#39;, &#39;ward&#39;, &#39;flexible UPGMA&#39;, &#39;divisive&#39;), cophenetic.correlation=c(corr.avg, corr.single, corr.complete, corr.ward, corr.flexible, corr.div), agg_div.coefficient=c(h.avg$ac, h.single$ac, h.complete$ac, h.ward$ac, h.flexible$ac, h.div$dc) ) # re-order res &lt;- res[order(res$cophenetic.correlation, decreasing = TRUE), ] method cophenetic.correlation agg_div.coefficient average 0.840 0.787 flexible UPGMA 0.840 0.778 single 0.784 0.613 complete 0.759 0.879 ward 0.756 0.894 divisive 0.703 0.874 Investigate differences graphically: Ive ranked according to the frequency withwhich I use the various methods. par(mar=c(0,0.25,1.5,0.25), mfrow=c(2,3)) plot(as.phylo(as.hclust(h.div)), label.offset=0.125, direction=&#39;right&#39;, font=1, cex=0.65, main=&#39;1. Divisive&#39;) tiplabels(pch=15, col=col.set[cutree(h.div, 4)]) plot(as.phylo(as.hclust(h.ward)), label.offset=0.125, direction=&#39;right&#39;, font=1, cex=0.65, main=&#39;2. Ward&#39;) tiplabels(pch=15, col=col.set[cutree(h.ward, 4)]) plot(as.phylo(as.hclust(h.flexible)), label.offset=0.125, direction=&#39;right&#39;, font=1, cex=0.65, main=&#39;3. Flexible (0.01)&#39;) tiplabels(pch=15, col=col.set[cutree(h.flexible, 4)]) plot(as.phylo(as.hclust(h.avg)), label.offset=0.125, direction=&#39;right&#39;, font=1, cex=0.65, main=&#39;4. Average&#39;) tiplabels(pch=15, col=col.set[cutree(h.avg, 4)]) plot(as.phylo(as.hclust(h.single)), label.offset=0.125, direction=&#39;right&#39;, font=1, cex=0.65, main=&#39;(never) Single&#39;) tiplabels(pch=15, col=col.set[cutree(h.single, 4)]) plot(as.phylo(as.hclust(h.complete)), label.offset=0.125, direction=&#39;right&#39;, font=1, cex=0.65, main=&#39;(never) Complete&#39;) tiplabels(pch=15, col=col.set[cutree(h.complete, 4)]) 3.3.4.3.1 More on the Flexible UPGMA method Test the flexible UPGMA method (Belbin, Faith, and Milligan 1992) by iterating over possible values for \\(\\beta\\). Looks like a value near 0.01 would be ideal (e.g. highest cophenetic correlation). Interestingly, this is very close to the cophenetic correlation associated with the average linkage criteria. # init a sequence spanning -1 to 1 beta &lt;- seq(from=-1, to=1, length.out = 100) # init an empty vector to store results r &lt;- vector(mode=&#39;numeric&#39;, length = length(beta)) # iterate over values of beta and compute cophenetic correlation for(i in 1:length(r)) { r[i] &lt;- cor(d, cophenetic(agnes(d, method=&#39;gaverage&#39;, par.method = beta[i]))) } ## Warning in cor(d, cophenetic(agnes(d, method = &quot;gaverage&quot;, par.method = ## beta[i]))): the standard deviation is zero # plot par(mar=c(5,5,1,1)) plot(beta, r, type=&#39;l&#39;, xlab=&#39;beta parameter&#39;, ylab=&#39;cophenetic correlation&#39;, pch=16, las=1) # locate the max coph. corr. idx &lt;- which.max(r) # mark this point and label points(beta[idx], r[idx], col=&#39;red&#39;, cex=2, lwd=2) text(beta[idx], r[idx], labels = round(beta[idx], 3), pos=4) 3.3.4.4 Comparison of Dendrograms The following example is rather basic. Many more possibilities are available in the dendextend package. # load sample dataset from aqp package data(sp3) # promote to SoilProfileCollection depths(sp3) &lt;- id ~ top + bottom # compute dissimilarity using different sets of variables # note that these are rescaled to the interval [0,1] d.1 &lt;- profile_compare(sp3, vars=c(&#39;clay&#39;, &#39;L&#39;), k=0, max_d=100, rescale.result=TRUE) ## Computing dissimilarity matrices from 10 profiles [0.22 Mb] # cluster via divisive hierarchical algorithm # convert to &#39;phylo&#39; class p.1 &lt;- as.phylo(as.hclust(diana(d.1))) # graphically compare diana() to agnes() using d.2 dueling.dendrograms(as.phylo(as.hclust(diana(d.1))), as.phylo(as.hclust(agnes(d.1, method=&#39;average&#39;))), lab.1=&#39;divisive&#39;, lab.2=&#39;agglomerative: average linkage&#39;) 3.3.5 Centroid and Medoid (Partitioning) Clustering The following creates simulated data for demonstration purposes, representing two populations: mean = 0, sd = 0.3 mean = 1, sd = 0.3 # nice colors for later col.set &lt;- brewer.pal(9, &#39;Set1&#39;) # 2D example x &lt;- rbind( matrix(rnorm(100, mean = 0, sd = 0.3), ncol = 2), matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2) ) colnames(x) &lt;- c(&quot;x&quot;, &quot;y&quot;) 3.3.5.1 Hard Classes 3.3.5.1.1 K-Means It is important to note that the k-means algorithm is sensitive to the initial selection of centroid locations (typically random). The default behavior of the kmeans() function does not attempt to correct for this limitation. Note that cluster assignment and centroid vary across runs (panels in the figure below). par(mfrow=c(3,3), mar=c(1,1,1,1)) for(i in 1:9) { cl &lt;- kmeans(x, centers=3) plot(x, col = col.set[cl$cluster], axes=FALSE) grid() points(cl$centers, col = col.set, pch = 8, cex = 2, lwd=2) box() } Setting the nstart argument (number of random starts) to a value great than 1 (10 is ideal) will ensure that the final clustering configuration will remain stable between runs. Note that the cluster ID (color) will vary between runs, however, with nstart=10 the overal configuration remains the same. par(mfrow=c(3,3), mar=c(1,1,1,1)) for(i in 1:9) { cl &lt;- kmeans(x, centers=3, nstart = 10, iter.max = 100) plot(x, col = col.set[cl$cluster], axes=FALSE) grid() points(cl$centers, col = col.set, pch = 8, cex = 2, lwd=2) box() } 3.3.5.1.2 K-Medoids The cluster package provides two interfaces to the k-medoids algorithm: pam(): small to medium sized data sets clara(): optmized for larger data sets A quick example of using pam() to identify an increasing number of clusters. par(mfrow=c(2,3), mar=c(1,1,1,1)) for(i in 2:7) { cl &lt;- pam(x, k = i, stand = TRUE) plot(x, col = col.set[cl$clustering], axes=FALSE) grid() points(cl$medoids, col = col.set, pch = 0, cex = 2, lwd=2) box() } 3.3.5.2 Fuzzy Clusters Clustering results are in the form of class membership; values ranging between 0 and 1. This means that group membership is a continuum vs. the hard classes assigned by k-means or k-medoids. The mixture of class membership in the example below is conveniently expressed using proportions of red, green, and blue. # re-make data, this time with all profiles data(&#39;sp4&#39;, package = &#39;aqp&#39;) sp4.std &lt;- data.frame(sp4[, c(&#39;id&#39;, &#39;name&#39;, &#39;top&#39;, &#39;bottom&#39;)], scale( sp4[, c(&#39;Mg&#39;, &#39;Ca&#39;)])) # perform fuzzy clustering cl &lt;- fanny(sp4.std[, c(&#39;Mg&#39;, &#39;Ca&#39;)], k = 3, stand = FALSE) # get membership matrix m &lt;- cl$membership # convert to colors by interpreting membership as R,G,B proportions cols &lt;- rgb(m) # setup plot par(mar=c(4,4,0,0)) plot(sp4.std$Mg, sp4.std$Ca, asp=1, ylab=&#39;Exchangeable Mg (cmol/kg), Standardized&#39;, xlab=&#39;Exchangeable Ca (cmol/kg), Standardized&#39;, type=&#39;n&#39;) abline(h=0, v=0, col=&#39;black&#39;) grid() # add original obs points(sp4.std$Mg, sp4.std$Ca, bg=cols, col=&#39;black&#39;, cex=1.5, pch=21) Save the RGB color representation of cluster membership to the source data.frame and convert to SoilProfileCollection. sp4.std$colors &lt;- cols depths(sp4.std) &lt;- id ~ top + bottom par(mar=c(0,0,0,0)) plot(sp4.std, color=&#39;colors&#39;, cex.names=0.75) title(&#39;Fuzzy Clustering Results in Context&#39;, line=-1) From the source: Of the 11 parent materials, 9 were serpentinites. The parent materials in Napa and Tehama Counties were quite different from each other and from those of the nine other sites. Neither had parent rock that contained serpentine minerals. They were, therefore, not serpentinites. The Napa County parent material contained dominantly vermiculite and albite and had minor amounts of Ca-bearing clino-pyroxene. The Tehama County parent material was dominated by grossularite, which is a calcsilicate ugrandite garnet, and had subdominant amounts of the Ca-bearing sorosilicate, pumpellyite, and Ca-bearing clinopyroxene. The rocks from the Shasta and Kings County sites were serpentinite, dominated by serpentine minerals. They had minor amounts of Ca-bearing accessory minerals (calcic clinoamphibole [tremolite] and calcsilicate ugrandite garnet [andradite]). The seven other parent materials were serpentinites and exhibited, at most, trace amounts of Ca-bearing minerals. 3.3.5.3 How Many Clusters? There is no simple answer to the question How many clusters are in my data? Some metrics, however, can be used to help estimate a reasonable number of clusters. The mean silhouette width is a useful index of cluster compactness relative to neighbor clusters (Rousseeuw 1987). Larger silhouette widths suggest tighter grouping. # re-make data, this time with all profiles data(&#39;sp4&#39;, package = &#39;aqp&#39;) sp4.std &lt;- data.frame(sp4[, c(&#39;id&#39;, &#39;name&#39;, &#39;top&#39;, &#39;bottom&#39;)], scale( sp4[, c(&#39;Mg&#39;, &#39;Ca&#39;)])) # perform hard clustering sil.widths &lt;- vector(mode=&#39;numeric&#39;) for(i in 2:10) { cl &lt;- pam(sp4.std[, c(&#39;Mg&#39;, &#39;Ca&#39;)], k = i, stand = FALSE) sil.widths[i] &lt;- cl$silinfo$avg.width } par(mar=c(4,4,3,1)) plot(sil.widths, type=&#39;b&#39;, xlab=&#39;Number of Clusters&#39;, ylab=&#39;Average Silhouette Width&#39;, las=1, lwd=2, col=&#39;RoyalBlue&#39;, cex=1.25, main=&#39;Finding the &quot;Right&quot; Number of Clusters&#39;) grid() According to this metric, it looks like 3 clusters is reasonable. Again, this is a judgement callmost decisions related to clustering algorithm selection and the optimal number of clusters are somewhat subjective. # perform fuzzy clustering cl &lt;- pam(sp4.std[, c(&#39;Mg&#39;, &#39;Ca&#39;)], k = 3, stand = FALSE) # setup plot par(mar=c(4,4,0,0)) plot(sp4.std$Mg, sp4.std$Ca, asp=1, ylab=&#39;Exchangeable Mg (cmol/kg), Standardized&#39;, xlab=&#39;Exchangeable Ca (cmol/kg), Standardized&#39;, type=&#39;n&#39;) abline(h=0, v=0, col=&#39;black&#39;) grid() # add original obs points(sp4.std$Mg, sp4.std$Ca, bg=cl$clustering, col=&#39;black&#39;, cex=1.25, pch=21) TODO: other cluster metrics packages 3.3.5.4 Ordination 3.3.5.4.1 Principal Component Analysis A simple, constrained ordination based on variance. This method does not use the distance matrix, rather it seeks to find a new set of axes that describe maximum variance via linear combinations of characteristics. Standardization is essential. # re-make data, this time with all profiles data(&#39;sp4&#39;, package = &#39;aqp&#39;) sp4 &lt;- sp4[, c(&#39;name&#39;, &#39;clay&#39;, &#39;sand&#39;, &#39;Mg&#39;, &#39;Ca&#39;, &#39;CEC_7&#39;)] sp4.scaled &lt;- data.frame(name=sp4[, 1], round(scale( sp4[, -1]), 2)) # PCA # note that we are leaving out the first column: the horizon names # note the syntax used to extract the principal components # note that PCA doesn&#39;t use the distance matrix pca &lt;- predict(princomp(sp4.scaled[, -1])) ## perform clustering to highlight structure in the PCA # distance matrix d &lt;- dist(sp4.scaled[, -1]) m &lt;- as.matrix(d) dimnames(m) &lt;- list(sp4.scaled$name, sp4.scaled$name) d &lt;- as.dist(m) # dendrogram from divisive clustering dd &lt;- diana(d) h &lt;- as.hclust(dd) p &lt;- as.phylo(h) # define colors based on cutting a divisive hierarchical clustering into 4 groups cols &lt;- brewer.pal(9, &#39;Set1&#39;)[cutree(h, 4)] # plot first 2 PC plot(pca[, 1:2], asp=1, type=&#39;n&#39;, axes=FALSE, xlab=&#39;&#39;, ylab=&#39;&#39;, main=&quot;Principal Components 1 and 2&quot;) grid() text(pca[, 1:2], sp4.scaled$name, cex=0.75, col=cols, font=2) box() 3.3.5.5 Sammons Non-linear Mapping Simple interface to nMDS, input is a distance matrix. Note that this algorithm will fail if there are 0s or ties within the distance matrix. See ?sammon for details. # re-make data, this time with all profiles data(&#39;sp4&#39;, package = &#39;aqp&#39;) sp4 &lt;- sp4[, c(&#39;name&#39;, &#39;clay&#39;, &#39;sand&#39;, &#39;Mg&#39;, &#39;Ca&#39;, &#39;CEC_7&#39;)] sp4.scaled &lt;- data.frame(name=sp4[, 1], round(scale( sp4[, -1]), 2)) # distance matrix d &lt;- dist(sp4.scaled[, -1]) m &lt;- as.matrix(d) dimnames(m) &lt;- list(sp4.scaled$name, sp4.scaled$name) d &lt;- as.dist(m) # dendrogram from divisive clustering dd &lt;- diana(d) h &lt;- as.hclust(dd) p &lt;- as.phylo(h) # define colors based on cutting a divisive hierarchical clustering into 4 groups cols &lt;- brewer.pal(9, &#39;Set1&#39;)[cutree(h, 4)] # nMDS from distance matrix s &lt;- sammon(d) # plot par(mar=c(3,1,3,1)) plot(s$points, asp=1, type=&#39;n&#39;, axes=FALSE, xlab=&#39;&#39;, ylab=&#39;&#39;, main=&quot;nMDS by Sammon&#39;s Non-Linear Mapping&quot;) # abline(v=0, h=0, col=&#39;black&#39;) grid() text(s$points, rownames(s$points), cex=0.75, col=cols, font=2) box() 3.3.5.6 nMDS with the vegan Package The following example is quite brief. See the Introduction to ordination in vegan vignette for some excellent worked examples and ecological interpretation. The vegan package FAQ is another excellent resource. Numerical Ecology with R can be used as both reference and learning resource. The metaMDS() function from the vegan package provides a convenience function that automates most of the steps required to create an oridination. # re-make data, this time with all profiles data(&#39;sp4&#39;, package = &#39;aqp&#39;) sp4 &lt;- sp4[, c(&#39;name&#39;, &#39;clay&#39;, &#39;sand&#39;, &#39;Mg&#39;, &#39;Ca&#39;, &#39;CEC_7&#39;)] sp4.scaled &lt;- data.frame(name=sp4[, 1], round(scale( sp4[, -1]), 2)) # define colors based on natural groupings cols &lt;- brewer.pal(9, &#39;Set1&#39;) # distance calc + ordination s &lt;- metaMDS(sp4.scaled[, -1], distance = &#39;gower&#39;, autotransform = FALSE, wascores=FALSE) ## this is used to generate 4 classes from a divisive hierarchical clustering # manually compute distance matrix d &lt;- dist(sp4.scaled[, -1]) m &lt;- as.matrix(d) dimnames(m) &lt;- list(sp4.scaled$name, sp4.scaled$name) d &lt;- as.dist(m) # dendrogram from divisive clustering dd &lt;- diana(d) h &lt;- as.hclust(dd) # define colors based on cutting a divisive hierarchical clustering into 4 groups cols &lt;- brewer.pal(9, &#39;Set1&#39;)[cutree(h, 4)] # plot ordination par(mar=c(3,1,3,1)) fig &lt;- ordiplot(s, type=&#39;none&#39;, cex.axis=0.75, axes=FALSE, xlab=&#39;&#39;, ylab=&#39;&#39;, main=&#39;nMDS by metaMDS()&#39;) ## species scores not available abline(h=0, v=0, lty=2, col=&#39;grey&#39;) text(fig$sites, sp4$name, cex=0.75, font=2, col=cols) # ordicluster(fig, agnes(daisy(sp4.scaled[, -1]), method=&#39;ward&#39;), prune=3, col = &quot;orange&quot;) box() 3.3.5.7 Rotations procrustes, etc. # https://ncss-tech.github.io/AQP/aqp/color-contrast.html 3.4 Practical Applications Before you work through the following examples, you should review the SoilProfileCollection object tutorial. 3.4.1 Pair-Wise Distances Between Soil Profiles # init example data data(sp4) depths(sp4) &lt;- id ~ top + bottom # eval dissimilarity: # using Ex-Ca:Mg and CEC at pH 7 # with no depth-weighting (k=0) # to a maximum depth of 40 cm d &lt;- profile_compare(sp4, vars=c(&#39;ex_Ca_to_Mg&#39;, &#39;CEC_7&#39;), k=0, max_d=40) # check distance matrix: round(d, 1) ## Dissimilarities : ## colusa glenn kings mariposa mendocino napa san benito shasta ## glenn 13.5 ## kings 16.0 12.7 ## mariposa 8.4 11.3 16.5 ## mendocino 11.5 8.0 16.4 15.0 ## napa 30.4 24.1 29.4 29.2 21.6 ## san benito 25.7 20.6 26.3 28.2 15.8 18.0 ## shasta 17.2 13.3 8.7 17.6 17.1 33.7 22.2 ## shasta-trinity 6.4 16.6 22.3 9.6 16.5 29.8 27.2 23.3 ## tehama 28.7 22.9 27.9 27.3 20.0 8.8 15.1 31.4 ## shasta-trinity ## glenn ## kings ## mariposa ## mendocino ## napa ## san benito ## shasta ## shasta-trinity ## tehama 27.9 ## ## Metric : mixed ; Types = I, I ## Number of objects : 10 # cluster via divisive method clust &lt;- diana(d) # vizualize dissimilarity matrix via hierarchical clustering par(mar=c(0,0,3,1)) plotProfileDendrogram(sp4, clust, dend.y.scale = max(d), scaling.factor = (1/max(d) * 10), y.offset = 2.25, width=0.25, cex.names=0.6, color=&#39;ex_Ca_to_Mg&#39;, col.label=&#39;Exchageable Ca to Mg Ratio&#39;) 3.4.2 Pair-Wise Distances Between Soil Series The following figures are a preview of some new functionality planned for SoilWeb/SDE. These related tutorials cover similar material in greater detail: Competing Soil Series Querying Soil Series Data The fetchOSD function can return additional summaries tabulated from climate data, MLRA boundaries, SSURGO, and much more with the extended=TRUE argument. Lets experiment with distances computed from annual climate data and hillslope position. # soil series from around CONUS soils &lt;- c(&#39;redding&#39;, &#39;pentz&#39;, &#39;willows&#39;, &#39;yolo&#39;, &#39;hanford&#39;, &#39;cecil&#39;, &#39;sycamore&#39;, &#39;KLAMATH&#39;, &#39;drummer&#39;, &#39;musick&#39;, &#39;zook&#39;) s &lt;- fetchOSD(soils, extended = TRUE) # note additional data, packed into a list str(s, 1) ## List of 14 ## $ SPC :Formal class &#39;SoilProfileCollection&#39; [package &quot;aqp&quot;] with 9 slots ## $ competing :&#39;data.frame&#39;: 57 obs. of 3 variables: ## $ geog_assoc_soils:&#39;data.frame&#39;: 75 obs. of 2 variables: ## $ geomcomp :&#39;data.frame&#39;: 9 obs. of 9 variables: ## $ hillpos :&#39;data.frame&#39;: 10 obs. of 8 variables: ## $ mtnpos :&#39;data.frame&#39;: 2 obs. of 9 variables: ## $ terrace :&#39;data.frame&#39;: 8 obs. of 5 variables: ## $ flats :&#39;data.frame&#39;: 7 obs. of 7 variables: ## $ pmkind :&#39;data.frame&#39;: 19 obs. of 5 variables: ## $ pmorigin :&#39;data.frame&#39;: 44 obs. of 5 variables: ## $ mlra :&#39;data.frame&#39;: 75 obs. of 4 variables: ## $ climate.annual :&#39;data.frame&#39;: 88 obs. of 12 variables: ## $ climate.monthly :&#39;data.frame&#39;: 264 obs. of 14 variables: ## $ soilweb.metadata:&#39;data.frame&#39;: 17 obs. of 2 variables: 3.4.2.1 Annual Climate Data The vizAnnualClimate function (sharpshootR package) arranges percentiles of annual climate summaries according to divisive hierarchical clustering applied to median values. Climate summaries were derived from 800m, daily PRISM data spanning 1981-2010. # control color like this trellis.par.set(plot.line=list(col=&#39;RoyalBlue&#39;)) # control centers symbol and size here res &lt;- vizAnnualClimate(s$climate.annual, IQR.cex = 1.25, cex=1.1, pch=18) # plot figure, this is Lattice graphics print(res$fig) # do something with clustering par(mar=c(0,0,1,1)) # usually requires tinkering... plotProfileDendrogram(s$SPC, clust = res$clust, scaling.factor = 0.05, width = 0.2, y.offset = 1.3) mtext(&#39;sorted by annual climate summaries&#39;, side = 3, at = 0.5, adj = 0, line = -1.5, font=3) 3.4.2.2 Hillslope Position The vizHillslopePosition function (sharpshootR package) arranges hillslope position proportions (SSURGO) according to divisive hierarchical clustering. Proportions are used as characteristics for each soil series. The branches of the dendrogram are rotated so that ordering within the figure approximates the hydrologic gradient as closely as possible. Rotation is performed by the dendextend::rotate function. # result is a lattice graphics object res &lt;- vizHillslopePosition(s$hillpos) ## Registered S3 method overwritten by &#39;dendextend&#39;: ## method from ## rev.hclust vegan print(res$fig) 3.4.3 Pair-Wise Distances Between Subgroup-Level Taxa The following are demonstrations of pair-wise distances computed from categorical data and the use of a dendrogram to organize groups from Soil Taxonomy. Click here for details. # define a vector of series s.list &lt;- c(&#39;amador&#39;, &#39;redding&#39;, &#39;pentz&#39;, &#39;willows&#39;, &#39;pardee&#39;, &#39;yolo&#39;, &#39;hanford&#39;, &#39;cecil&#39;, &#39;sycamore&#39;, &#39;KLAMATH&#39;, &#39;MOGLIA&#39;, &#39;drummer&#39;, &#39;musick&#39;, &#39;zook&#39;, &#39;argonaut&#39;, &#39;PALAU&#39;) # get and SPC object with basic data on these series s &lt;- fetchOSD(s.list) # graphical check par(mar=c(0,0,2,0)) plot(s) ; title(&#39;Selected Pedons from Official Series Descriptions&#39;, line=0) # check structure of some site-level attributes # head(site(s))[, c(&#39;id&#39;, &#39;soilorder&#39;, &#39;suborder&#39;, &#39;greatgroup&#39;, &#39;subgroup&#39;)]) id soilorder suborder greatgroup subgroup AMADOR inceptisols xerepts haploxerepts typic haploxerepts ARGONAUT alfisols xeralfs haploxeralfs mollic haploxeralfs CECIL ultisols udults kanhapludults typic kanhapludults DRUMMER mollisols aquolls endoaquolls typic endoaquolls HANFORD entisols orthents xerorthents typic xerorthents KLAMATH mollisols aquolls cryaquolls cumulic cryaquolls par(mar=c(0,1,1,1)) # plot dendrogram + profiles d &lt;- SoilTaxonomyDendrogram(s, scaling.factor = 0.01, width=0.2, cex.names=0.5) Check the resulting distance matrix. print(d) 3.4.4 Soil Color Signatures See this related tutorial for additional examples. # manually convert Munsell -&gt; sRGB rgb.data &lt;- munsell2rgb(s$hue, s$value, s$chroma, return_triplets = TRUE) s$r &lt;- rgb.data$r s$g &lt;- rgb.data$g s$b &lt;- rgb.data$b # eval color signature pig &lt;- soilColorSignature(s, RescaleLightnessBy = 5, method=&#39;depthSlices&#39;) # display results as table kable_styling(knitr::kable(head(pig), digits = 3, row.names = FALSE)) id A.0.1 A.0.5 A.0.9 B.0.1 B.0.5 B.0.9 L.0.1 L.0.5 L.0.9 AMADOR 3.681 4.870 1.919 13.298 18.956 27.828 8.250 10.324 14.333 ARGONAUT 13.549 19.311 15.876 19.548 30.215 17.613 6.159 6.161 8.250 CECIL 7.378 32.895 28.722 25.999 29.497 35.807 8.254 8.251 8.254 DRUMMER 2.189 1.478 1.478 5.367 6.176 6.176 4.111 8.247 8.247 HANFORD 5.629 5.629 6.611 19.847 19.847 25.480 8.252 8.252 10.327 KLAMATH 2.189 2.189 10.371 5.367 5.367 37.715 4.111 4.111 8.257 # move row names over for distance matrix row.names(pig) &lt;- pig[, 1] d &lt;- daisy(pig[, -1]) dd &lt;- diana(d) # plot par(mar=c(0,0,0.25,1)) plotProfileDendrogram(s, dd, dend.y.scale = max(d) * 2, scaling.factor = 0.3, y.offset = 6, width=0.2, cex.names=0.5) 3.4.5 Clustering of Soil Colors Just for fun, use hierarchical clustering and nMDS on soil color data from the OSDs that were used in the previous example. # extract horizon data from select OSDs in above example h &lt;- horizons(s) # convert Munsell color notation to sRGB # these are moist colors rgb.data &lt;- munsell2rgb(h$hue, h$value, h$chroma, return_triplets = TRUE) lab.data &lt;- munsell2rgb(h$hue, h$value, h$chroma, returnLAB = TRUE) # check head(rgb.data) ## r g b ## 1 0.4360624 0.3706674 0.29697452 ## 2 0.5589675 0.4673350 0.35663875 ## 3 0.5589675 0.4673350 0.35663875 ## 4 0.7719679 0.6774631 0.48997537 ## 5 0.3940324 0.2499977 0.16682669 ## 6 0.4309729 0.2327690 0.09771028 head(lab.data) ## L A B ## 1 41.24855 3.681301 13.29762 ## 2 51.62124 4.870262 18.95563 ## 3 51.62124 4.870262 18.95563 ## 4 71.66388 1.919454 27.82850 ## 5 30.79580 13.548688 19.54789 ## 6 30.80674 19.311423 30.21539 # remove NA rgb.data &lt;- na.omit(rgb.data) lab.data &lt;- na.omit(lab.data) # retain unique colors rgb.data &lt;- unique(rgb.data) lab.data &lt;- unique(lab.data) # visualize colors in LAB coordinates pairs(lab.data, col=&#39;white&#39;, bg=rgb(rgb.data), pch=21, cex=2) # create distance matrix from LAB coordinates d &lt;- daisy(lab.data, stand=FALSE) # divisive heirarcical clustering d.hclust &lt;- as.hclust(diana(d)) # convert to phylo class for nicer plotting p &lt;- as.phylo(d.hclust) # perform nMDS on distance matrix d.sammon &lt;- sammon(d) # setup multi-figure page par(mfcol=c(1,2), mar=c(0,0,2,0), bg=grey(0.95)) # plot fan-style dendrogram plot(p, font=2, cex=0.5, type=&#39;fan&#39;, show.tip.label=FALSE, main=&#39;Dendrogram Representation&#39;) # add colors at dendrogram tips tiplabels(pch=21, cex=4, col=&#39;white&#39;, bg=rgb(rgb.data)) # plot nMDS ordination plot(d.sammon$points, type=&#39;n&#39;, axes=FALSE, xlab=&#39;&#39;, ylab=&#39;&#39;, asp=1, main=&#39;nMDS Ordination&#39;) abline(h=0, v=0, col=&#39;black&#39;, lty=3) points(d.sammon$points, bg=rgb(rgb.data), pch=21, cex=3.5, col=&#39;white&#39;) 3.4.6 How Do the Interpretations Compare? Example borrowed from this tutorial. library(reshape2) # set list of component names, same as soil color example s.list &lt;- c(&#39;amador&#39;, &#39;redding&#39;, &#39;pentz&#39;, &#39;willows&#39;, &#39;pardee&#39;, &#39;yolo&#39;, &#39;hanford&#39;, &#39;cecil&#39;, &#39;sycamore&#39;, &#39;KLAMATH&#39;, &#39;MOGLIA&#39;, &#39;drummer&#39;, &#39;musick&#39;, &#39;zook&#39;, &#39;argonaut&#39;, &#39;PALAU&#39;) # set list of relevant interpretations interp.list &lt;- c(&#39;ENG - Construction Materials; Topsoil&#39;, &#39;ENG - Sewage Lagoons&#39;, &#39;ENG - Septic Tank Absorption Fields&#39;, &#39;ENG - Unpaved Local Roads and Streets&#39;) # compose query q &lt;- paste0(&quot;SELECT UPPER(compname) as compname, mrulename, AVG(interplr) as interplr_mean FROM component INNER JOIN cointerp ON component.cokey = cointerp.cokey WHERE compname IN &quot;, format_SQL_in_statement(s.list), &quot; AND seqnum = 0 AND mrulename IN &quot;, format_SQL_in_statement(interp.list), &quot; AND interplr IS NOT NULL GROUP BY compname, mrulename;&quot;) # send query x &lt;- SDA_query(q) ## single result set, returning a data.frame # reshape long -&gt; wide x.wide &lt;- dcast(x, compname ~ mrulename, value.var = &#39;interplr_mean&#39;) knitr::kable(x.wide, digits = 3, caption=&quot;Mean Fuzzy Ratings for Select Soil Series&quot;) Table 3.1: Mean Fuzzy Ratings for Select Soil Series compname ENG - Construction Materials; Topsoil ENG - Septic Tank Absorption Fields ENG - Sewage Lagoons ENG - Unpaved Local Roads and Streets AMADOR 0.000 1.000 1.000 0.691 ARGONAUT 0.050 1.000 1.000 1.000 CECIL 0.414 0.666 0.854 0.302 DRUMMER 0.000 1.000 1.000 1.000 HANFORD 0.678 0.989 1.000 0.220 KLAMATH 0.000 1.000 1.000 1.000 MOGLIA 0.000 1.000 0.500 1.000 MUSICK 0.245 1.000 0.963 0.832 PALAU 0.011 1.000 0.864 1.000 PARDEE 0.000 1.000 1.000 1.000 PENTZ 0.003 1.000 1.000 0.714 REDDING 0.022 1.000 1.000 0.907 SYCAMORE 0.824 0.998 0.756 0.919 WILLOWS 0.000 1.000 0.947 1.000 YOLO 0.843 0.914 0.608 0.769 ZOOK 0.004 1.000 1.000 1.000 # note: component name and series name have been converted to upper case # sort rows of fuzzy ratings based on profiles from OSDs new.order &lt;- match(x.wide$compname, profile_id(s)) x.wide &lt;- x.wide[new.order, ] # copy ids to row.names so that they are preserved in distance matrix row.names(x.wide) &lt;- x.wide$compname # create distance matrix d &lt;- daisy(x.wide[, -1]) # divisive hierarchical clustering clust &lt;- diana(d) par(mar=c(2,0,2,0)) plotProfileDendrogram(s, clust, dend.y.scale = 1.5, scaling.factor = 0.004, y.offset = 0.1, width=0.25, cex.names=0.45) title(&#39;Component Similarity via Select Fuzzy Ratings&#39;) mtext(&#39;Profile Sketches are from OSDs&#39;, 1) 3.4.7 MLRA Concepts via Climate and Elevation Samples Get and process example data, originally sampled from PRISM raster and DEM within MLRA 15, 18, 22A, and 22B. Variables include: elevation mean annual air temperature mean annuap precipitation fraction of annual precipitation as rain effective precipitation frost-free days growing degree days library(MASS) library(vegan) library(cluster) library(RColorBrewer) # get example data # init a temp file tf &lt;- tempfile() # download compressed CSV to temp file download.file(&#39;https://github.com/ncss-tech/stats_for_soil_survey/raw/master/data/clustering_and_ordination/MLRA-raster-data-example.csv.gz&#39;, destfile = tf, quiet = TRUE) # read-in from compressed CSV to data.frame object d.sub &lt;- read.csv(gzfile(tf), stringsAsFactors = FALSE) # check: note that the column names are quite strange head(d.sub) # set factor levels mu.set &lt;- c(&#39;15&#39;, &#39;18&#39;, &#39;22A&#39;, &#39;22B&#39;) d.sub$.id &lt;- factor(d.sub$.id, levels = mu.set) # define some nice colors cols &lt;- brewer.pal(9, &#39;Set1&#39;) # remove light colors cols &lt;- cols[c(1:5,7,9)] 3.4.7.1 Nonmetric Multidimensional Scaling A smooh surface fit to mean annual air temperature highlights structure within a nMDS ordination. m &lt;- metaMDS(d.sub[, -c(1:3)], distance = &#39;gower&#39;) ## &#39;comm&#39; has negative data: &#39;autotransform&#39;, &#39;noshare&#39; and &#39;wascores&#39; set to FALSE # margins par(mar=c(1,1,3,1)) # setup plot o &lt;- ordiplot(m, type=&#39;n&#39;, axes=FALSE) ## species scores not available # add points, colored accoring to MLRA points(o, &#39;sites&#39;, cex=1, col=cols[as.numeric(d.sub$.id)], pch=16) # overlay smooth surface of variable used in ordination ordisurf(m, d.sub$Mean.Annual.Air.Temperature..degrees.C., add=TRUE, col=&#39;black&#39;, labcex=1) legend(&#39;topleft&#39;, legend=mu.set, pch=16, col=cols, bty=&#39;n&#39;, cex=1.25) title(&#39;nMDS with mean annual air temperature (deg C) surface&#39;) box() 3.4.7.2 Principle Coordinates This example generates an ordination (via principal coordinates) of environmental variables (PRSIM and elevation) associated with MLRAs 15, 18, 22A, and 22B. Ellipses represent 50% probability contours via multivariate homogeneity of group dispersions. See comments in the code for details. Note that this example was extracted from the Region 2 Map Unit Comparison Report. Thoughts on interpretation: The relative position of points and ellipses are meaningful; absolute position will vary each time the figure is generated. Look for diffuse vs. concentrated clusters: these suggest relatively broadly vs. narrowly defined concepts. Nesting of clusters (e.g. smaller cluster contained by larger cluster) suggests super-set/subset relationships. Overlap is proportional to similarity. ## NOTE: data with very low variability will cause warnings # eval numerical distance, removing first 3 columns of IDs d.dist &lt;- daisy(d.sub[, -c(1:3)], stand=TRUE) ## map distance matrix to 2D space via principal coordinates d.betadisper &lt;- betadisper(d.dist, group=d.sub$.id, bias.adjust = TRUE, sqrt.dist = FALSE, type=&#39;median&#39;) ## fancy plot plot( d.betadisper, hull=FALSE, ellipse=TRUE, conf=0.5, col=cols, main=&#39;Ordination of Raster Samples\\n50% Probability Ellipse&#39;, sub=&#39;MLRA 15, 18, 22A, 22B&#39; ) Pair-wise comparisons at the 90% level of confidence. ## pair-wise comparisons of variance par(mar=c(4.5, 5.5, 4.5, 1)) plot(TukeyHSD(d.betadisper, conf.level = 0.9), las=1) 3.5 References (Numerical Taxonomy and Ordination) This document is based on aqp version 1.30 and soilDB version 2.6.1 and sharpshootR version 1.8.2. References "],["linear-regression.html", "Chapter 4 Linear Regression 4.1 Introduction 4.2 Linear Regression Example 4.3 Data 4.4 Spatial data 4.5 Exploratory Data Analysis (EDA) 4.6 Linear modeling 4.7 Generate spatial predictions 4.8 Create Map 4.9 Literature 4.10 Additional reading 4.11 References (Linear Models)", " Chapter 4 Linear Regression Statistics for pedologists course banner image 4.1 Introduction Linear regression models the linear relationship between a response variable (y) and an predictor variable (x). \\(y = \\alpha + \\beta x + e\\) Where: \\(y\\) = the dependent variable \\(\\alpha\\) = the intercept of the fitted line \\(\\beta\\) = the Regression coefficient, i.e. slope of the fitted line. Strong relationships will have high values. \\(x\\) = the independent variable (aka explanatory or predictor variable(s) ) \\(e\\) = the error term Linear regression has been used for soil survey applications since the early 1900s when Briggs and McLane (1907) developed a pedotransfer function to estimate the wilting coefficient as a function of soil particle size. Wilting coefficient = 0.01(sand) + 0.12(silt) + 0.57(clay) When more than one independent variable is used in the regression, it is referred to as multiple linear regression. In regression models, the response (or dependent) variable must always be continuous. The predictor (or independent) variable(s) can be continuous or categorical. In order to use linear regression or any linear model, the errors (i.e. residuals) must be normally distributed. Most environmental data are skewed and require transformations to the response variable (such as square root or log) for use in linear models. Normality can be assessed using a QQ plot or histogram of the residuals. 4.2 Linear Regression Example Now that weve got some of the basic theory out of the way well move on to a real example, and address any additional theory where it relates to specific steps in the modeling process. The example selected for this chapter comes from the Mojave desert. The landscape is composed primarily of closed basins ringed by granitic hills and mountains (Peterson, 1981). The problem tackled here is modeling mean annual air temperature as a function of PRISM, digital elevation model (DEM) and Landsat derivatives. This climate study began in 1998 as part of a national study run by the National Lab and led by Henry Mount (Mount and Paetzold, 2002). The objective was to determine if the hyperthermic line was consistent across the southern US, which at the time was thought to be ~3000 in elevation. Up until 2015 their were 77 active MAST sites, and 12 SCAN sites throughout MLRA 30 and 31. For more details see the MLRA 30 - Soil Climate Study - Soil Temperature project in NASIS, on GitHub, or by Roecker et al., 2012. library(soilDB) prj &lt;- get_project_from_NASISWebReport(mlrassoarea = &quot;8-VIC&quot;, fiscalyear = 2015) ## Loading required namespace: rvest subset(prj, projectname == &quot;MLRA 30 - Soil Climate Study - Soil Temperature&quot;) ## # A tibble: 1 x 30 ## mlrassoarea nonmlrassaarea mlraarea projecttypename fiscalyear ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 8-VIC &lt;NA&gt; 30 MLRA 2015 ## # ... with 25 more variables: fiscalyear_goaled &lt;int&gt;, projectiid &lt;int&gt;, ## # uprojectid &lt;chr&gt;, projectname &lt;chr&gt;, projectapprovedflag &lt;lgl&gt;, ## # projectconcerntypename &lt;chr&gt;, projectdesc &lt;chr&gt;, n_projectdesc &lt;int&gt;, ## # n_areasymbol &lt;int&gt;, n_nationalmusym &lt;int&gt;, n_spatial &lt;int&gt;, ## # date_start &lt;date&gt;, date_complete &lt;date&gt;, date_qc &lt;date&gt;, ## # date_qa_start &lt;date&gt;, date_qa_complete &lt;date&gt;, date_qc_spatial &lt;date&gt;, ## # date_qa_spatial &lt;date&gt;, pl_username &lt;chr&gt;, qa_username &lt;chr&gt;, ## # muacres &lt;int&gt;, acre_landcat &lt;int&gt;, acre_goal &lt;int&gt;, acre_progress &lt;int&gt;, ## # pct_progress &lt;int&gt; In addition to the 11-IND MAST modeling efforts there has also been two published studies on the Mojave. The first was by Schmidlin et al. (1983) who examined both the Great Basin and Mojave Deserts in Nevada. The second was by Bai et al. (2010) who examined the Mojave Desert in California. Both studies developed regression models using elevation, but Schmidlin et al. (1983) also incorporated latitude. The results from Bai et al. (2010) displayed considerably larger areas of hyperthermic soils than Schmidlin et al. (1983). This made be due to the unconventional method used by Bai et al. (2010) to measure MAST. 4.3 Data 4.3.1 Tidy Raw Files Typically the data for MAST and other monitoring projets is downloaded annually. However, some sites now are online and can be accessed remotely. Eitherway prior to loading the information into a proper database it is best to store the data in txt files (Excel files are discouraged). If the data is organized and the filenames encode the metadata, an example such as the one below can be used to efficiently import them into R and combine them into a data frame. p &lt;- &quot;D:/projects/soilTemperatureMonitoring/data/rawTxtFilesClean&quot; setwd(p) # get file names of HOBO temp data files &lt;- list.files() # read files l &lt;- lapply(files, function(x) { # extract the file name fileName = strsplit(x, &#39;[.]&#39;)[[1]][1] # parse the siteid from the file name siteid = strsplit(x, &#39;_&#39;)[[1]][1] cat(paste(&quot;working on&quot;, fileName, &quot;\\n&quot;)) f = paste0(p, &quot;/&quot;, x) # read the files f = read.table(file = f, header = TRUE, sep = &quot;\\t&quot;, stringsAsFactors = FALSE) f$siteid &lt;- siteid names(f)[1:3] &lt;- c(&quot;date&quot;,&quot;tempF&quot;,&quot;tempC&quot;) f$tempF &lt;-as.numeric(f$tempF) f$tempC &lt;-as.numeric(f$tempC) vars = c(&quot;date&quot;, &quot;siteid&quot;, &quot;tempF&quot;, &quot;tempC&quot;) f = f[vars] }) mastSeries_df &lt;- do.call(&quot;rbind&quot;, l) # save cached copy save(mastSeries_df, file = &quot;D:/projects/soilTemperatureMonitoring/data/R/mastSeries.Rdata&quot;) length(unique(mastSeries_df$siteid)) 4.3.2 Henry Mount Database The Henry Mount Database already has 59 of the sites from the Mojave. The full data set however has 68 sites. library(soilDB) f &lt;- fetchHenry(sso = &quot;8-VIC&quot;) length(unique(f$sensors$user_site_id)) 4.3.3 Aggregate Time Series # load cached versions load(file = &quot;D:/projects/soilTemperatureMonitoring/data/R/mastSeries.Rdata&quot;) # Plot sites visually inspect for flat lines and spikes test &lt;- subset(mastSeries_df, site == &quot;JTNP08&quot;) test.zoo &lt;- read.zoo(test[,c(1,3)],format = &quot;%m/%d/%y %H:%M:%S&quot;, tz = &quot;GMT&quot;) plot(test.zoo, ylab = &quot;tempF&quot;) # Aggregate by Year, Month, and Julian day (i.e. 1-365, 366 for leap years) ms.df &lt;- mastSeries_df ms.df$date &lt;- as.POSIXlt(ms.df$date, format=&quot;%m/%d/%y %H:%M:%S&quot;) ms.df$day &lt;- as.character(format(ms.df$date, &quot;%m/%d/%y&quot;)) ms.df$Jday &lt;- as.integer(format(ms.df$date, &quot;%j&quot;)) # compute number of days per site ms.D.df &lt;- aggregate(tempF ~ site + day, data = ms.df, FUN = mean, na.action = na.exclude) ms.D.df &lt;- aggregate(day ~ site, data = ms.D.df, function(x) sum(!is.na(x))) names(ms.D.df) &lt;- c(&quot;siteid&quot;,&quot;numDays&quot;) # compute mast per year ms.Jd.df &lt;- aggregate(tempF ~ siteid + Jday, data = ms.df, mean) mastSites.df &lt;- aggregate(tempF ~ siteid, data = ms.Jd.df, mean) # merge mast &amp; numDays mastSites.df &lt;- merge(mastSites.df, ms.D.df, by = &quot;siteid&quot;) write.csv(mastSites.df, &quot;mastSites.csv&quot;) 4.3.4 Final Dataset Since the Henry Mount database is incomplete we will procede with the aggregation from the txt files. # Read tempC data setwd(&quot;D:/projects/soilTemperatureMonitoring/data/R&quot;) sites_df &lt;- read.csv(&quot;HOBO_List_2013_0923_master.csv&quot;) mast_df &lt;- read.csv(&quot;mastSites.csv&quot;) mast_df &lt;- merge(mast_df, sites_df, by = &quot;siteid&quot;) vars &lt;- c(&quot;siteid&quot;, &quot;tempF&quot;, &quot;numDays&quot;, &quot;utmeasting&quot;, &quot;utmnorthing&quot;) mast_df &lt;- mast_df[vars] mast_df$tempC &lt;- (mast_df$tempF - 32) * (5 / 9) 4.4 Spatial data 4.4.1 Plot Coordinates Where do our points plot? To start we need to convert them to a spatial object first. Then we can create an interactive we map using mapview. Also, if we wish we can also export the locations as a Shapefile. library(sf) library(mapview) library(dplyr) githubURL &lt;- &quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/ch7_data_v2.Rdata&quot; load(url(githubURL)) # convert to sites to a spatial object mast_sf &lt;- st_as_sf(mast_df, coords = c(&quot;utmeasting&quot;, &quot;utmnorthing&quot;), crs = 26911 ) %&gt;% # reproject st_transform(crs = 4326) # reduce precision mast_sf2 &lt;- st_as_sf(as.data.frame(round(st_coordinates(mast_sf), 1)), coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 4326 ) # reproject mast_sf &lt;- st_transform(mast_sf, 5070) # MLRAs mlra &lt;- read_sf(dsn = &quot;D:/geodata/soils/mlra_a_mbr.shp&quot;, layer = &quot;mlra_a_mbr&quot;) %&gt;% st_transform(crs = 4326) %&gt;% filter(MLRARSYM %in% 30:31) # plot mapview(mlra, fill = NA) + mapview(mast_sf2) 4.4.2 Extracting Spatial Data Prior to any spatial analysis or modeling, you will need to develop a suite of geodata files that can be intersected with your field data locations. This is, in and of itself a difficult task and should be facilitated by your Regional GIS Specialist. The geodata files typically used would consist of derivatives from a DEM or satellite imagery, and a good geology map. Prior to any prediction it is also necessary to ensure the geodata files have the same projection, extent, and cell size. Once we have the necessary files we can construct a list in R of the file names and paths, read the geodata into R, and then extract the geodata values where they intersect with field data. As you can see below their are numerous variables we could inspect. library(raster) # set file path folder &lt;- &quot;D:/geodata/project_data/R8-VIC/&quot; files &lt;- c(elev = &quot;ned30m_8VIC_elev5.tif&quot;, slope = &quot;ned30m_8VIC_slope5.tif&quot;, aspect = &quot;ned30m_8VIC_aspect5.tif&quot;, twi = &quot;ned30m_8VIC_wetness.tif&quot;, solar = &quot;ned30m_8VIC_solar.tif&quot;, solarcv = &quot;ned30m_8VIC_solarcv.tif&quot;, tc = &quot;landsat30m_8VIC_tc123.tif&quot;, precip = &quot;prism30m_8VIC_ppt_1981_2010_annual_mm.tif&quot;, temp = &quot;prism30m_8VIC_tmean_1981_2010_annual_C.tif&quot; ) # combine the folder directory and file names geodata_f &lt;- paste0(folder, files) names(geodata_f) &lt;- names(files) # Create a raster stack geodata_r &lt;- stack(geodata_f) # Extract the geodata and add to a data frame data &lt;- raster::extract(geodata_r, as(mast_sf, &quot;Spatial&quot;), sp = TRUE)@data # convert aspect data$northness &lt;- abs(180 - data$aspect) # random sample vars &lt;- c(&quot;elev&quot;, &quot;temp&quot;, &quot;precip&quot;, &quot;solar&quot;, &quot;tc_1&quot;, &quot;twi&quot;) idx &lt;- which(names(geodata_r) %in% vars) geodata_s &lt;- sampleRegular(geodata_r[[idx]], size = 5000) # cache files save(data, mast_df, mast_sf2, mlra, geodata_s, file = &quot;C:/workspace2/github/ncss-tech/stats_for_soil_survey/data/ch7_data_v2.Rdata&quot;) 4.5 Exploratory Data Analysis (EDA) Generally before we begin modeling it is good to explore the data. By examining a simple summary we can quickly see the breakdown of our data. It is important to look out for missing or improbable values. Probably the easiest way to identify pecularities in the data is to plot it. githubURL &lt;- &quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/ch7_data_v2.Rdata&quot; load(url(githubURL)) summary(data) ## siteid tempF numDays tempC ## Cheme01 : 1 Min. :37.58 Min. : 363 Min. : 3.10 ## Clark01 : 1 1st Qu.:65.29 1st Qu.:1831 1st Qu.:18.50 ## DEVA01 : 1 Median :69.03 Median :2918 Median :20.57 ## DEVA02 : 1 Mean :67.88 Mean :2465 Mean :19.93 ## DEVA03 : 1 3rd Qu.:74.70 3rd Qu.:3397 3rd Qu.:23.72 ## Jawbone01: 1 Max. :83.78 Max. :4159 Max. :28.77 ## (Other) :62 ## elev slope aspect twi ## Min. : -80.02 Min. : 0.435 Min. : 7.433 Min. : 8.519 ## 1st Qu.: 703.60 1st Qu.: 3.221 1st Qu.: 39.861 1st Qu.: 9.502 ## Median : 946.79 Median : 5.667 Median :113.600 Median :12.947 ## Mean :1083.04 Mean :14.184 Mean :129.911 Mean :13.019 ## 3rd Qu.:1489.26 3rd Qu.:24.142 3rd Qu.:187.696 3rd Qu.:15.744 ## Max. :3038.61 Max. :54.319 Max. :342.366 Max. :21.226 ## NA&#39;s :2 ## solar solarcv tc_1 tc_2 ## Min. :1370 Min. :18.36 Min. : 50.31 Min. : 25.03 ## 1st Qu.:2031 1st Qu.:31.00 1st Qu.: 93.36 1st Qu.: 48.61 ## Median :2101 Median :33.00 Median :125.56 Median : 57.87 ## Mean :2079 Mean :33.82 Mean :122.49 Mean : 58.36 ## 3rd Qu.:2163 3rd Qu.:34.21 3rd Qu.:152.32 3rd Qu.: 66.18 ## Max. :2654 Max. :58.81 Max. :197.26 Max. :108.98 ## ## tc_3 precip temp northness ## Min. : 2.067 Min. : 3.164 Min. :10.39 Min. : 2.547 ## 1st Qu.: 32.682 1st Qu.: 6.581 1st Qu.:21.12 1st Qu.: 31.923 ## Median : 50.433 Median : 7.469 Median :24.05 Median : 91.407 ## Mean : 53.769 Mean :10.367 Mean :23.61 Mean : 90.070 ## 3rd Qu.: 75.994 3rd Qu.:10.422 3rd Qu.:26.61 3rd Qu.:146.644 ## Max. :124.071 Max. :45.046 Max. :31.83 Max. :172.567 ## You may recall from discussion of EDA that QQ plots are a visual way to inspect the normality of a variable. If the variable is normally distributed, the points (e.g. soil observations) should line up along the straight line. # QQ plot library(ggplot2) ggplot(data, aes(sample = tempC)) + geom_qq() + geom_qq_line() By examining the correlations between some of the predictors we can also determine wheter they are collinear (e.g. &gt; 0.6). This is common for similar variables such as landsat bands, terrain derivatives, and climatic variables. Variables that are colinear are redundant and contain no additional information. In additino, collinearity will make it difficult to estimate our regression coefficients. vars &lt;- c(&quot;tempC&quot;, &quot;elev&quot;, &quot;temp&quot;, &quot;precip&quot;, &quot;tc_2&quot;, &quot;tc_1&quot;, &quot;tc_3&quot;) GGally::ggpairs(data[, vars]) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 vars &lt;- c(&quot;tempC&quot;, &quot;slope&quot;, &quot;twi&quot;, &quot;northness&quot;, &quot;solar&quot;, &quot;solarcv&quot;) GGally::ggpairs(data[, vars]) ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 2 rows containing missing values ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 2 rows containing missing values ## Warning: Removed 2 rows containing missing values (geom_point). ## Warning: Removed 2 rows containing missing values (geom_point). ## Warning: Removed 2 rows containing non-finite values (stat_density). ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 2 rows containing missing values ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 2 rows containing missing values ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 2 rows containing missing values ## Warning: Removed 2 rows containing missing values (geom_point). ## Warning: Removed 2 rows containing missing values (geom_point). ## Warning: Removed 2 rows containing missing values (geom_point). The correlation matrices and scatter plots above show that that MAST has moderate correlations with some of the variables, particularly the elevation and the climatic variables. Examining the density plots on the diagonal axis of the scatterplots we can also see that some variables are skewed. 4.5.1 Compare Samples vs Population Since our data was not randomly sampled, we had better check the distribution of our samples vs the population. We can accomplish this by overlaying the sample distribution of predictor variables vs a large random sample. geodata_df &lt;- as.data.frame(geodata_s) geodata_df &lt;- rbind( data.frame(source = &quot;sample&quot;, data[names(geodata_df)]), data.frame(source = &quot;population&quot;, geodata_df) ) geodata_w &lt;- reshape::melt( geodata_df, id.vars = &quot;source&quot;, measures.vars = vars ) ggplot(geodata_w, aes(x = value, fill = source)) + geom_density(alpha = 0.5) + facet_wrap(~ variable, scales = &quot;free&quot;) + ggtitle(&quot;Evaluation of Sample Representativeness&quot;) ## Warning: Removed 6466 rows containing non-finite values (stat_density). The overlap between our sample and the population appear satistifactory. 4.6 Linear modeling R has several functions for fitting linear models. The most common is arguably the lm() function from the stats R package, which is loaded by default. The lm() function is also extended thru the use of several additional packages such as the car and caret R packages. Another noteworthy R package for linear modeling is rms, which offers the ols() function for linear modeling. The rms R package (Harrell et al., 2015) offers an almost comprehesive alternative to `lm() and its accessory function. It is difficult to objectively functions say which approach is better. Therefore methods both methods will be demonstrated. Look for comments (i.e. #) below referring to rms, stats, caret or visreg. # stats fit_lm &lt;- lm(tempC ~ elev + aspect + twi + solar + solarcv + tc_1 + tc_2 + tc_3 + precip + temp, data = data, weights = data$numDays) # rms library(rms) ## Warning: package &#39;rms&#39; was built under R version 4.0.5 ## Loading required package: Hmisc ## Warning: package &#39;Hmisc&#39; was built under R version 4.0.4 ## Loading required package: lattice ## Loading required package: survival ## Warning: package &#39;survival&#39; was built under R version 4.0.5 ## Loading required package: Formula ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units ## Loading required package: SparseM ## Warning: package &#39;SparseM&#39; was built under R version 4.0.4 ## ## Attaching package: &#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve dd &lt;- datadist(data) options(datadist = &quot;dd&quot;) fit_ols &lt;- ols(tempC ~ elev + aspect + twi + solar + solarcv + tc_1 + tc_2 + tc_3 + precip + temp, data = data, x = TRUE, y = TRUE, weights = data$numDays) 4.6.1 Diagnostics 4.6.1.1 Residual plots Once we have a model we need to assess residuals for linearity, normality, and homoscedastivity (or constant variance). Oddly this is one area were the rms R package does not offer convient functions for plotting residuals, therefore well simply access the results of lm(). par(mfcol = c(2, 2)) plot(fit_lm) termplot(fit_lm, partial.resid = TRUE, col.res = &quot;black&quot;, pch = 16) 4.6.1.2 Multicolinearity As we mentioned earlier multicolinearity should be avoided. To assess a model for multicolinearity we can compute the variance inflation factor (VIF). Its square root indicates the amount of increase in the predictor coefficients standard error. A value greater than 3 indicates a doubling the standard error. Rules of thumb vary, but a square root of vif greater than 2 or 3 indicates an unacceptable value. # vif() function from the rms or car packages sqrt(car::vif(fit_lm)) ## elev aspect twi solar solarcv tc_1 tc_2 tc_3 ## 9.282520 1.179764 1.879222 6.744169 6.695338 4.081507 2.891362 3.619210 ## precip temp ## 2.041644 8.270426 # or sqrt(rms::vif(fit_ols)) &gt; 3 ## elev aspect twi solar solarcv tc_1 tc_2 tc_3 precip temp ## TRUE FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE The values above indicate we have several colinear variables in the model, which you might have noticed already from the scatter plot matrix. 4.6.2 Variable selection &amp; model validation Modeling is an iterative process that cycles between fitting and evaluating alternative models. Compared to tree and forest models, linear and generalized models typically require more scrunity from the user. Automated model selection procedures are available, but should not be taken at face value because they may result in complex and unstable models. This is in part due to correlation amongst the predictive variables that can confuse the model. Also, the order in which the variables are included or excluded from the model effects the significance of the other variables, and thus several weak predictors might mask the effect of one strong predictor. Regardless of the approach used, variable selection is probably the most controversial aspect of linear modeling. Both the rms and caret packages offer methods for variable selection and cross-validation. In this instance the rms approach is a bit more convinent, with the one line call to validate(). # Set seed for reproducibility set.seed(42) # rms ## stepwise selection and validation step_rms &lt;- validate(fit_ols, method = &quot;crossvalidation&quot;, B = 10, bw = TRUE) ## ## Backwards Step-down - Original Model ## ## Deleted Chi-Sq d.f. P Residual d.f. P AIC R2 ## solar 0.00 1 0.9627 0.00 1 0.9627 -2.00 34736 ## aspect 0.15 1 0.6944 0.16 2 0.9247 -3.84 34736 ## tc_3 0.70 1 0.4032 0.86 3 0.8362 -5.14 34735 ## precip 0.34 1 0.5609 1.19 4 0.8792 -6.81 34735 ## twi 0.84 1 0.3586 2.04 5 0.8441 -7.96 34733 ## temp 3.25 1 0.0716 5.28 6 0.5082 -6.72 34728 ## ## Approximate Estimates after Deleting Factors ## ## Coef S.E. Wald Z P ## Intercept 43.887389 2.1856221 20.080 0.000e+00 ## elev -0.006998 0.0003973 -17.613 0.000e+00 ## solarcv -0.190672 0.0211880 -8.999 0.000e+00 ## tc_1 -0.036248 0.0078482 -4.619 3.863e-06 ## tc_2 -0.093525 0.0245610 -3.808 1.402e-04 ## ## Factors in Final Model ## ## [1] elev solarcv tc_1 tc_2 The results for validate() above and below show which variables were retained and deleted. Below we can see a dot matrix of which variables were retained in during the 10 iterations of the cross validation. In addition, below we can see the difference between the training and test accuracy and error metrics. Remember that it is the test accuracy we should pay attention too. ## test accuracy and error step_rms ## index.orig training test optimism index.corrected n ## R-square 0.9480 0.9480 0.6819 0.2661 0.6819 10 ## MSE 1.5629 1.5504 2.7899 -1.2395 2.8024 10 ## g 5.7490 5.7647 5.4549 0.3098 5.4392 10 ## Intercept 0.0000 0.0000 3.1803 -3.1803 3.1803 10 ## Slope 1.0000 1.0000 0.8586 0.1414 0.8586 10 ## ## Factors Retained in Backwards Elimination ## ## elev aspect twi solar solarcv tc_1 tc_2 tc_3 precip temp ## * * * * ## * * * * ## * * * * ## * * * * ## * * * * ## * * * * ## * * ## * * * * ## * * * * ## * * * * ## ## Frequencies of Numbers of Factors Retained ## ## 2 4 ## 1 9 The caret package option for variable selection and validation is a bit more verbose than the rms package. However, the caret package is a more versatile package, with options for over 50 different models, such as other tree-based models. # caret library(caret) ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## cluster ## cross validation parameters train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10, savePredictions = TRUE, returnData = TRUE) ## stepwise selection and validation step_caret &lt;- train(tempC ~ elev + solar + aspect + twi + solar + solarcv + tc_1 + tc_2 + tc_3 + log(precip) + temp^2, data = data, weights = data$numDays, method = &quot;lmStepAIC&quot;, trace = FALSE, trControl = train.control, na.action = na.exclude ) ## test accuracy and error summary(step_caret$resample) ## RMSE Rsquared MAE Resample ## Min. :0.777 Min. :0.7788 Min. :0.6110 Length:10 ## 1st Qu.:1.112 1st Qu.:0.8856 1st Qu.:0.8779 Class :character ## Median :1.542 Median :0.9423 Median :1.2662 Mode :character ## Mean :1.479 Mean :0.9171 Mean :1.1279 ## 3rd Qu.:1.762 3rd Qu.:0.9756 3rd Qu.:1.3318 ## Max. :2.373 Max. :0.9843 Max. :1.5093 The output from caret is somewhat different. Notice it selected a slightly different combination of variables and more optimistic test accuracy and error. # summary summary(step_caret$finalModel) ## ## Call: ## lm(formula = .outcome ~ elev + twi + solarcv + tc_1 + tc_2 + ## `log(precip)` + temp, data = dat, weights = wts) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -132.087 -30.532 -1.176 35.609 103.131 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.298762 8.367072 3.621 0.000618 *** ## elev -0.005103 0.001900 -2.686 0.009420 ** ## twi -0.098647 0.067447 -1.463 0.148976 ## solarcv -0.184135 0.020963 -8.784 3.02e-12 *** ## tc_1 -0.026130 0.009523 -2.744 0.008068 ** ## tc_2 -0.073104 0.025710 -2.843 0.006154 ** ## `log(precip)` 0.604990 0.425929 1.420 0.160844 ## temp 0.378019 0.253919 1.489 0.141974 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 55.22 on 58 degrees of freedom ## Multiple R-squared: 0.9613, Adjusted R-squared: 0.9566 ## F-statistic: 205.7 on 7 and 58 DF, p-value: &lt; 2.2e-16 4.6.3 Final model &amp; accuracy assessment # rms final_ols &lt;- ols(tempC ~ elev + solarcv + tc_1 + tc_2, data = data, weights = data$numDays, x = TRUE, y = TRUE) validate(final_ols, method = &quot;crossvalidation&quot;, B = 10) ## index.orig training test optimism index.corrected n ## R-square 0.9397 0.9410 0.8379 0.1031 0.8367 10 ## MSE 1.7778 1.7181 2.3015 -0.5834 2.3612 10 ## g 5.7652 5.6907 5.8429 -0.1521 5.9174 10 ## Intercept 0.0000 0.0000 0.7950 -0.7950 0.7950 10 ## Slope 1.0000 1.0000 0.9576 0.0424 0.9576 10 # caret final_caret &lt;- train(tempC ~ elev + solarcv + tc_1 + tc_2, data = data, weights = data$numDays, method = &quot;lm&quot;, trControl = train.control, na.action = na.exclude ) final_caret$results ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 1.441123 0.9340218 1.120288 0.3257457 0.03683471 0.2596735 final_lm &lt;- final_caret$finalModel 4.6.4 Model Effects summary(final_lm) ## ## Call: ## lm(formula = .outcome ~ ., data = dat, weights = wts) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -138.572 -28.633 4.663 35.574 117.141 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 43.8704062 2.1744491 20.175 &lt; 2e-16 *** ## elev -0.0070036 0.0003957 -17.700 &lt; 2e-16 *** ## solarcv -0.1919234 0.0211084 -9.092 4.48e-13 *** ## tc_1 -0.0366152 0.0077992 -4.695 1.49e-05 *** ## tc_2 -0.0918754 0.0244223 -3.762 0.000372 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56.91 on 63 degrees of freedom ## Multiple R-squared: 0.9554, Adjusted R-squared: 0.9526 ## F-statistic: 337.8 on 4 and 63 DF, p-value: &lt; 2.2e-16 dd &lt;- datadist(data) options(datadist = &quot;dd&quot;) anova(final_ols) ## Analysis of Variance Response: tempC ## ## Factor d.f. Partial SS MS F P ## elev 1 1014554.05 1014554.054 313.30 &lt;.0001 ## solarcv 1 267708.50 267708.498 82.67 &lt;.0001 ## tc_1 1 71373.80 71373.796 22.04 &lt;.0001 ## tc_2 1 45829.32 45829.318 14.15 4e-04 ## REGRESSION 4 4375387.71 1093846.928 337.78 &lt;.0001 ## ERROR 63 204013.46 3238.309 anova(final_lm) ## Analysis of Variance Table ## ## Response: .outcome ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## elev 1 4059084 4059084 1253.4581 &lt; 2.2e-16 *** ## solarcv 1 244880 244880 75.6199 2.18e-12 *** ## tc_1 1 25593 25593 7.9033 0.0065688 ** ## tc_2 1 45829 45829 14.1522 0.0003721 *** ## Residuals 63 204013 3238 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # rms plot(Predict(final_ols)) plot(Predict(final_ols, elev = NA, solarcv = c(23, 33, 51))) # visreg library(visreg) par(mfrow = c(2, 2)) visreg(final_lm) par(mfrow = c(1, 1)) visreg(final_lm, xvar = &quot;elev&quot;, by = &quot;solarcv&quot;, breaks = c(23, 33, 51), overlay = TRUE) 4.7 Generate spatial predictions # Predict tempC model predfun &lt;- function(model, data) { v &lt;- predict(model, data, se.fit=TRUE) } mast_r &lt;- predict(geodata_r, final_lm, fun = predfun, index = 1:2, progress = &#39;text&#39;) writeRaster(mast_r[[1]], filename = &quot;C:/workspace2/mast.tif&quot;, format = &quot;GTiff&quot;, progress = &quot;text&quot;) writeRaster(mast_r[[2]], filename = &quot;C:/workspace2/mast_se.tif&quot;, format = &quot;GTiff&quot;, progress = &quot;text&quot;) 4.8 Create Map library(raster) mlra &lt;- st_transform(mlra, 5070) # mast mast &lt;- raster(&quot;C:/workspace2/mast.tif&quot;) crs(mast) &lt;- &quot;+init=epsg:5070&quot; plot(mast) plot(mlra[1], col = NA, add = TRUE) # mast standard error mast_se &lt;- raster(&quot;C:/workspace2/mast_se.tif&quot;) crs(mast_se) &lt;- &quot;+init=epsg:5070&quot; plot(mast_se) plot(mlra[1], col = NA, add = TRUE) 4.9 Literature Bai, Y., T.A. Scott, W. Chen, R.C. Graham, L. Wu, A.C. Chang, and L.J. Lund, 2010. Soil Temperature Regimes in the Mojave Desert. Soil Science, 175(8):398-404. Harrell, F.E., 2015. Regression Modeling Strategies: With Applications to Linear Models, Logisitc and Ordinal Regression, and Survival Analysis. Springer, New York. https://link.springer.com/book/10.1007%2F978-3-319-19425-7 Mount, H.R., and R.F. Paetzold, 2002. The temperature regime for selected soils in the United States. United States Department of Agriculture, Natural Resources Con-servation Service, National Soil Survey Center, Lincoln, Nebraska, Soil Survey Investi-gation Report No. 48. Peterson, F.F., 1992. Status of Soil Climate Studies in Nevada. pp1-11. Roecker, S.M. and C.A. Haydu-Houdeshell, 2012. Modeling and Application of Soil Temperature in the Mojave and Lower Colorado Deserts of California. 2012 Western Regional Cooperative Soil Survey Conference. Schmidlin, T.W., F.F. Peterson, and R.O. Gifford, 1983. Soil Temperature Regimes of Nevada. Soil So. Sci. Am. J., 47:977-982. 4.10 Additional reading Faraway, J.J., 2002. Practical Regression and Anova using R. CRC Press, New York. https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf James, G., D. Witten, T. Hastie, and R. Tibshirani, 2014. An Introduction to Statistical Learning: with Applications in R. Springer, New York. http://www-bcf.usc.edu/~gareth/ISL/ Hengl, T. 2009. A Practical Guide to Geostatistical Mapping, 2nd Edt. University of Amsterdam, www.lulu.com, 291 p. ISBN 978-90-9024981-0. http://spatial-analyst.net/book/system/files/Hengl_2009_GEOSTATe2c0w.pdf Webster, R. 1997. Regression and functional relations. European Journal of Soil Science, 48, 557-566. http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2389.1997.tb00222.x/abstract 4.11 References (Linear Models) "],["glm.html", "Chapter 5 Generalized Linear Models 5.1 Introduction 5.2 Logistic Regression 5.3 Examples 5.4 Exercise 5.5 Exploratory analysis (EDA) 5.6 Exercise 1: View the data in ArcGIS 5.7 Constructing the model 5.8 Generate spatial predictions 5.9 Exercise 2: View the prediction in ArcGIS 5.10 Additional reading 5.11 Literature Cited 5.12 References (GLM)", " Chapter 5 Generalized Linear Models 5.1 Introduction Generalized linear models (GLM) as the name implies are a generalization of the linear modeling framework to allow for the modeling of response variables (e.g. soil attributes) with non-normal distributions and heterogeneous variances. Whereas linear models are designed for predicting continuous soil properties such as clay content or soil temperature, GLM can be used to predict the presence/absence of argillic horizons (i.e. logistic regression) or counts of a plant species along a transect (i.e. Poisson regression). These generalizations greatly expand the applicability of the linear modeling framework, while still allowing for a similar fitting procedure and interpretation of the resulting models. In the past in order to handle non-linearity and heterogeneous variances, transformations have been made to the response variable, such as the log(x). However, such transformations complicate the models interpretation because the results refer to the transformed scale (e.g. log(x)). These response transformations are not guaranteed to achieve both normality and constant variance simultaneously. GLM approaches transform the response, but also preserve the scale of the response, and provide separate functions to transform the mean response and variance, known as the link and variance functions respectively. So instead of looking like this: \\(f(y) = \\beta_{0} + \\beta_{1}x + \\varepsilon\\) you get this: \\(g(\\mu)\\) or \\(\\eta = \\beta_{0} + \\beta_{1}x + \\varepsilon\\) with \\(g(\\mu)\\) or \\(\\eta\\) symbolizing the link function. Another alteration of the classical linear model is that with GLM the coefficients are estimated iteratively by maximum likelihood estimation instead of ordinary least squares. This results in the GLM minimizing the deviance, instead of the sum of squares. However, for the Gaussian (i.e. normal) distributions the deviance and sum of squares are equivalent. 5.2 Logistic Regression Logistic regression is a specific type of GLM designed to model data that has a binomial distribution (i.e. presence/absence, yes/no, or proportional data), which in statistical learning parlance is considered a classification problem. For binomial data the logit link transform is generally used. The effect of the logit transform can be seen in the following figure. It creates a sigmoidal curve, which enhances the separation between the two groups. It also has the effect of ensuring that the values range between 0 and 1. When comparing a simple linear model vs a simple logistic model we can see the effect of the logit transform on the relationship between the response and predictor variable. As before it follows a sigmoidal curve and prevents predictions from exceeding 0 and 1. 5.3 Examples Example 1: Probability of Mollisols (Beaudette &amp; OGeen, 2009) Example 2: Probability of Red Clay (Evans &amp; Hartemink, 2014) Example 3: Probability of Ponding (NRCS Unpublished) 5.4 Exercise Now that weve discussed some of the basic background GLM theory well move on to a real exercise, and address any additional theory where it relates to specific steps in the modeling process. The examples selected for this chapter come from Joshua Tree National Park (JTNP)(i.e. CA794) in the Mojave desert. The problem tackled here is a familiar one: Where can I expect to find argillic horizons on fan piedmonts? Argillic horizons within the Mojave are typically found on fan remnants, which are a stable landform that is a remnant of the Pleistocene (Peterson, 1981). Despite the low relief of most fans, fan remnants are uplands in the sense that they generally dont receive run-on or active deposition. With this dataset well encounter some challenges. To start with, fan piedmont landscapes typically have relatively little relief. Since most of our predictors will be derivatives of elevation, that wont leave us with much to work with. Also, our elevation data comes from the USGS National Elevation dataset (NED), which provides considerably less detail than say LiDAR or IFSAR data (Shi et al., 2012). Lastly our pedon dataset like most in NASIS, hasnt received near as much quality control as have the components. So well need to wrangle some of the pedon data before we can analyze it. These are all typical problems encountered in any data analysis and should be good practice. Ideally, it would be more interesting to try and model individual soil series with argillic horizons, but due to some of the challenges previously mentioned it would be difficult with this dataset. However, at the end well look at one simple approach to try and separate individual soil series with argillic horizons. 5.4.1 Load packages To start, as always we need to load some extra packages. This will become a familiar routine every time you start R. Most of the basic functions we need to develop a logistic regression model are contained in base R, but the following contain some useful spatial and data manipulation functions. Believe it or not we will use all of them and more. library(aqp) # specialized soil classes and functions library(soilDB) # NASIS and SDA import functions library(raster) # guess library(sf) # vector data functions library(mapview) # interactive mapping library(ggplot2) # graphing library(tidyr) # data manipulation library(caret) # classification and regression training library(car) # additional regression tools 5.4.2 Read in data Hopefully like all good soil scientists and ecological site specialists you enter your field data into NASIS. Better yet hopefully someone else did it for you! Once data are captured in NASIS it is much easier to import the data into R, extract the pieces you need, manipulate it, model it, etc. If its not entered into NASIS, it may as well not exist. For this exercise well load a cached dataset on GitHub. # pedons &lt;- fetchNASIS() githubURL &lt;- &quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/ch7_data.Rdata&quot; load(url(githubURL)) # Examine the makeup of the data we imported from NASIS str(pedons, max.level = 2) ## Formal class &#39;SoilProfileCollection&#39; [package &quot;aqp&quot;] with 9 slots ## ..@ idcol : chr &quot;peiid&quot; ## ..@ hzidcol : chr &quot;phiid&quot; ## ..@ depthcols : chr [1:2] &quot;hzdept&quot; &quot;hzdepb&quot; ## ..@ metadata :&#39;data.frame&#39;: 1 obs. of 2 variables: ## ..@ horizons :&#39;data.frame&#39;: 5163 obs. of 54 variables: ## ..@ site :&#39;data.frame&#39;: 1220 obs. of 122 variables: ## ..@ sp :Formal class &#39;SpatialPoints&#39; [package &quot;sp&quot;] with 3 slots ## ..@ diagnostic :&#39;data.frame&#39;: 2161 obs. of 4 variables: ## ..@ restrictions:&#39;data.frame&#39;: 0 obs. of 0 variables 5.5 Exploratory analysis (EDA) 5.5.1 Data wrangling Generally before we begin modeling you should spend some time exploring the data. By examining a simple summary we can quickly see the breakdown of how many argillic horizons we have. Unfortunately, odds are good that all the argillic horizons havent been consistently populated in the diagnostic horizon table like they should be. Luckily for us, the desert argillic horizons always pop up in the taxonomic name, so we can use pattern matching to extract it. By doing this we gain an additional 11 pedons with argillic horizons and are able to label the missing values (i.e. NA). At a minimum for modeling purposes we probably need 10 pedons of the target were interested in and a total of 100 observations overall. # Check consistency of argillic horizon population # get the site table s &lt;- site(pedons) ## aqp_df_class metadata entry not found - run aqp::rebuildSPC() to fix # tabulate the number of argillic horizons observed table(s$argillic.horizon, useNA = &quot;ifany&quot;) ## ## FALSE TRUE &lt;NA&gt; ## 790 263 167 # or # summary(s$argillic.horizon) # Extract argillic presence from the taxonomic subgroup s$argillic &lt;- grepl(&quot;arg&quot;, s$taxsubgrp) table(s$argillic, useNA = &quot;ifany&quot;) ## ## FALSE TRUE ## 1022 198 Ideally, if the diagnostic horizon table had been populated consistently we could have used the upper depth to diagnostic feature to filter out argillic horizons that start below 50cm, which may not be representative of good argillic horizons and may therefore have gotten correlated to a Torripsamments anyway. Not only are unrepresentative sites confusing for scientists, theyre equally confusing for models. However, as we saw earlier, some pedons dont appear to be fully populated, so well stick with those pedons that have the argillic specified in their taxonomic subgroup name, since it gives us the biggest sample. d &lt;- diagnostic_hz(pedons) ## aqp_df_class metadata entry not found - run aqp::rebuildSPC() to fix d_sub &lt;- subset(d, featkind == &quot;argillic horizon&quot; &amp; featdept &lt; 50) s$argillic.horizon50 &lt;- ifelse(s$peiid %in% d_sub$peiid, TRUE, FALSE) table(s$argillic.horizon50, useNA = &quot;ifany&quot;) ## ## FALSE TRUE ## 998 222 5.5.2 Geomorphic data Another obvious place to look is at the geomorphic data in the site table. This information is intended to help differentiate where our soil observations exist on the landscape. If populated consistently it could potentially be used in future disaggregation efforts, as demonstrated by Nauman and Thompson (2014). # Landform vs argillic presence # Subset s_sub &lt;- subset(s, argillic == TRUE) # Cross tabulate landform vs argillic horizon presence test &lt;- with(s_sub, table(landform, argillic, useNA = &quot;ifany&quot;) ) # Subset and print landform.string with &gt; 3 observations test[test &gt; 3, ] ## alluvial fans fan aprons ## 6 19 ## fan aprons on fan remnants fan remnants ## 4 72 ## hills hillslopes ## 15 28 ## low hills mountain slopes ## 5 8 ## mountains pediments ## 4 9 ## &lt;NA&gt; ## 7 # generalize the landform.string s$landform_generic &lt;- ifelse(grepl(&quot;fan|terrace|sheet|drainageway|wash&quot;, s$landform), &quot;fan&quot;, &quot;hill&quot;) Examining the above frequency table we can see that argillic horizons occur predominantly on fan remnants as was alluded too earlier. However, they also seem to occur frequently on other landforms - some of which are curious combinations of landforms or redundant terms. # Hillslope position # Subset fan landforms s_sub &lt;- subset(s, landform_generic == &quot;fan&quot;) # Cross tabulate and calculate proportions, the &quot;2&quot; calculates the proportions relative to the column totals with(s_sub, round( prop.table(table(hillslopeprof, argillic, useNA = &quot;ifany&quot;), 2) * 100) ) ## argillic ## hillslopeprof FALSE TRUE ## summit 16 39 ## shoulder 4 2 ## backslope 14 20 ## footslope 2 1 ## toeslope 16 4 ## &lt;NA&gt; 48 34 # Slope shape with(s_sub, round( prop.table(table(paste(shapedown, shapeacross), argillic, useNA = &quot;ifany&quot;), 2) * 100) ) ## argillic ## FALSE TRUE ## concave concave 1 0 ## concave convex 0 0 ## concave linear 4 3 ## convex concave 0 1 ## convex convex 7 7 ## convex linear 6 9 ## linear concave 6 1 ## linear convex 21 32 ## linear linear 44 38 ## linear NA 0 0 ## NA NA 11 10 Looking at the hillslope position of fan landforms we can see a slightly higher proportion of argillic horizons are found on summits, while less are found on toeslopes. Slope shape doesnt seem to provide any useful information for distinguishing argillic horizons. # Subset to just look and fans, and select numeric columns s_sub &lt;- subset(s, landform_generic == &quot;fan&quot;, select = c(argillic, bedrckdepth, slope, elev, surface_total_frags_pct)) # convert s_sub to wide data format s_w &lt;- reshape2::melt(s_sub, id.vars = &quot;argillic&quot;, measure.vars = c(&quot;bedrckdepth&quot;, &quot;slope&quot;, &quot;elev&quot;, &quot;surface_total_frags_pct&quot;)) head(s_w, 2) ## argillic variable value ## 1 FALSE bedrckdepth NA ## 2 FALSE bedrckdepth 11 library(ggplot2) ggplot(s_w, aes(x = argillic, y = value)) + geom_boxplot() + facet_wrap(~ variable, scale = &quot;free&quot;) ## Warning: Removed 680 rows containing non-finite values (stat_boxplot). Looking at boxplots of our numeric variables we can see none of them show much separation between the presense/absense of argillic horizons. 5.5.3 Soil Scientist Bias Next well look at soil scientist bias. The question being: Are some soil scientists more likely to describe argillic horizons than others? Due to the excess number of soil scientist that have worked on CA794, including detailees, weve filtered the names of soil scientist to include just the top 3 mappers and given priority to the most senior soil scientists when they occur together. # Custom function to filter out the top 3 soil scientists s &lt;- within(s, { old = descname descname2 = NA descname2[grepl(&quot;Stephen&quot;, old)] = &quot;Stephen&quot; # least senior descname2[grepl(&quot;Paul&quot;, old)] = &quot;Paul&quot; descname2[grepl(&quot;Peter&quot;, old)] = &quot;Peter&quot; # most senior }) s_sub &lt;- subset(s, landform_generic == &quot;fan&quot;) # By frequency with(s_sub, table(descname2, argillic, useNA = &quot;ifany&quot;)) ## argillic ## descname2 FALSE TRUE ## Paul 77 28 ## Peter 268 29 ## Stephen 66 13 ## &lt;NA&gt; 157 45 # By proportion with(s_sub, round( prop.table(table(descname2, argillic), margin = 1) * 100) ) ## argillic ## descname2 FALSE TRUE ## Paul 73 27 ## Peter 90 10 ## Stephen 84 16 For fan landforms, one of the soil scientists seems more likely than the others to describe argillic horizons. However while this information is suggestive, it is far from definitive in showing a potential bias because it doesnt take into account other factors. Well examine this more closely later. 5.5.4 Plot coordinates Where do our points plot? To start we need to convert them to a spatial object first. Then we can create an interactive we map using mapview. Also, if we wish we can also export the locations as a Shapefile. library(sf) library(dplyr) # create an index to remove sites without coordinates idx &lt;- complete.cases(s$x_sdt, s$y_std) s_sub &lt;- s[idx, ] # convert to sites to a spatial object s_sf &lt;- st_as_sf(s_sub, coords = c(&quot;x_std&quot;, &quot;y_std&quot;), crs = 4326 ) %&gt;% # reproject st_transform(crs = 5070) # Read in soil survey area boundaries ca794 &lt;- read_sf(dsn = &quot;D:/geodata/soils/soilsa_a_nrcs.shp&quot;, layer = &quot;soilsa_a_nrcs&quot;) %&gt;% # subset out Joshua Tree National Park filter(areasymbol == &quot;CA794&quot;) %&gt;% # reproject st_transform(crs = 5070) # Plot library(mapview) mapview(ca794, fill = NA) + mapview(s_sf, zcol = &quot;argillic&quot;) # Write shapefile of pedons write_sf(s_sf, dsn = &quot;C:/workspace2&quot;, &quot;ca794_pedons&quot;, driver = &quot;ESRI Shapefile&quot;, delete_dsn = TRUE) 5.6 Exercise 1: View the data in ArcGIS Examine the interactive map or shapefile in ArcGIS along with our potential predictive variables. Discuss with your group, and report your observations or hypotheses. 5.6.1 Extracting spatial data Prior to any spatial analysis or modeling, you will need to develop a suite of geodata files that can be intersected with your field data locations. This is, in and of itself a difficult task, and should be facilitated by your Regional GIS Specialist. Typically, these geodata files would primarily consist of derivatives from a DEM or satellite imagery. Prior to any prediction it is also necessary to ensure the geodata files have the same projection, extent, and cell size. Once we have the necessary files we can construct a list in R of the file names and paths, read the geodata into R, and then extract the geodata values where they intersect with field data. library(raster) # set file path folder &lt;- &quot;D:/geodata/project_data/R8-VIC/ca794/&quot; # list of file names files &lt;- c( z = &quot;ned30m_8VIC.tif&quot;, # elevation slp = &quot;ned30m_8VIC_slope5.tif&quot;, # slope gradient asp = &quot;ned30m_8VIC_aspect5.tif&quot;, # slope aspect twi = &quot;ned30m_8VIC_wetness.tif&quot;, # topographic wetness index twi_sc = &quot;ned30m_8VIC_wetness_sc.tif&quot;, # transformed twi ch = &quot;ned30m_8VIC_cheight.tif&quot;, # catchment height z2str = &quot;ned30m_8VIC_z2stream.tif&quot;, # height above streams mrrtf = &quot;ned30m_8VIC_mrrtf.tif&quot;, # multiresolution ridgetop flatness index mrvbf = &quot;ned30m_8VIC_mrvbf.tif&quot;, # multiresolution valley bottom flatness index solar = &quot;ned30m_8VIC_solar.tif&quot;, # solar radiation precip = &quot;prism30m_8VIC_ppt_1981_2010_annual_mm.tif&quot;, # annual precipitation precipsum = &quot;prism30m_8VIC_ppt_1981_2010_summer_mm.tif&quot;, # summer precipitation temp = &quot;prism30m_8VIC_tmean_1981_2010_annual_C.tif&quot;, # annual temperature mast = &quot;mast30m_ca794_2013.tif&quot;, # mean annual soil temperature ls = &quot;landsat30m_8VIC_b123457.tif&quot;, # landsat bands pc = &quot;landsat30m_8VIC_pc123456.tif&quot;, # principal components of landsat tc = &quot;landsat30m_8VIC_tc123.tif&quot;, # tasseled cap components of landsat k = &quot;gamma30m_8VIC_namrad_k.tif&quot;, # gamma radiometrics signatures th = &quot;gamma30m_8VIC_namrad_th.tif&quot;, u = &quot;gamma30m_8VIC_namrad_u.tif&quot;, cluster = &quot;cluster152.tif&quot;, # unsupervised classification geo = &quot;sgmc30m_8-VIC_geology.tif&quot; ) # combine the folder directory and file names geodata_f &lt;- paste0(folder, files) names(geodata_f) &lt;- names(files) # Create a raster stack geodata_r &lt;- stack(geodata_f) # Extract the geodata and add to a data frame data &lt;- raster::extract(geodata_r, as(s_sf, &quot;Spatial&quot;), sp = TRUE)@data # Modify some of the geodata variables data &lt;- within(data, { cluster = factor(cluster) geo = factor(geo) twi_sc = abs(twi - 13.8) # 13.8 = twi median gsi = (ls_3 - ls_1) / (ls_3 + ls_2 + ls_1) ndvi = (ls_4 - ls_3) / (ls_4 + ls_3) }) # save(data, ca794, pedons, file = &quot;C:/workspace2/github/ncss-tech/stats_for_soil_survey/data/ch7_data.Rdata&quot;) 5.6.2 Examine spatial data With our spatial data in hand, we can now see whether any of the variables will help us separate the presence/absence of argillic horizons. Because were dealing with a classification problem, well compare the numeric variables using boxplots. What were looking for are variables with the least amount of overlap in their distribution (i.e. the greatest separation in their median values). # Load data # githubURL &lt;- &quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/ch7_data.Rdata&quot; # load(url(githubURL)) load(file = &quot;C:/workspace2/github/ncss-tech/stats_for_soil_survey/data/ch7_data.Rdata&quot;) train &lt;- data # Select argillic horizons with &quot;arg&quot; in the subgroup name and on fans # Argillic horizons that occur on hills and mountains more than likely form by different process, and therefore would require a different model.train$argillic train &lt;- transform( train, argillic = ifelse(grepl(&quot;arg&quot;, taxsubgrp) &amp; train$mrvbf &gt; 0.15, &quot;yes&quot;, &quot;no&quot;), ch_log = log(ch + 1), z2str_log = log(z2str + 1), ch = NULL, z2str = NULL ) idx &lt;- which(names(train) == &quot;z&quot;) train &lt;- train[idx:ncol(train)] data_m &lt;- tidyr::gather(train[- c(31:32)], key = key, value = value, - argillic) data_m &lt;- subset(data_m, !is.na(value)) ggplot(data_m, aes(x = argillic, y = value)) + geom_boxplot() + facet_wrap(~ key, scales = &quot;free&quot;) 5.7 Constructing the model R has several functions for fitting linear models. The most common is arguably the glm() function from the stats R package, which is loaded by default. The glm() function is also extended thru the use of several additional packages such as the car and caret R packages. Another noteworthy R package for logistic regrssion is rms, which offers the lrm() function. The rms R package (Harrell et al., 2015) offers an almost comprehesive alternative to glm() and its accessory function. It is difficult to objectively functions say which approach is better. Therefore methods both methods will be demonstrated. Look for comments (i.e. #) below referring to rms, stats, caret or visreg. # stats fit_glm &lt;- glm(argillic ~ z + slp + twi_sc + ch_log + z2str_log + mrrtf + solar + precip + precipsum + temp + mast + tc_1 + tc_2 + tc_3 + k + th + u + cluster, data = train, family = binomial) # rms library(rms) dd &lt;- datadist(train) options(datadist = &quot;dd&quot;) fit_lrm &lt;- lrm(argillic ~ z + slp + twi_sc + ch_log + z2str_log + mrrtf + solar + precip + precipsum + temp + mast + tc_1 + tc_2 + tc_3 + k + th + u, data = train, x = TRUE, y = TRUE) 5.7.1 Diagnostic 5.7.1.1 Residual plots One unfortunate side effect of side effect of logistic regression is that the default residual plots are not interpretable. However the partial residual plots can be useful for identifying outliers and nonlinear trends. par(mfcol = c(2, 2)) plot(fit_glm) par(mfcol = c(1, 1)) car::residualPlots(fit_glm, fit = FALSE, tests = FALSE) 5.7.1.2 Multicolinearity As we mentioned earlier multicolinearity should be avoided. To assess a model for multicolinearity we can compute the variance inflation factor (VIF). Its square root indicates the amount of increase in the predictor coefficients standard error. A value greater than 3 indicates a doubling the standard error. Rules of thumb vary, but a square root of vif greater than 2 or 3 indicates an unacceptable value. sqrt(vif(fit_glm)) 5.7.2 Variable Selection &amp; model validation Modeling is an iterative process that cycles between fitting and evaluating alternative models. Compared to tree and forest models, linear and generalized models typically require more scrunity from the user. Automated model selection procedures are available, but should not be taken at face value because they may result in complex and unstable models. This is in part due to correlation amongst the predictive variables that can confuse the model. Also, the order in which the variables are included or excluded from the model effects the significance of the other variables, and thus several weak predictors might mask the effect of one strong predictor. Regardless of the approach used, variable selection is probably the most controversial aspect of linear modeling. Both the rms and caret packages offer methods for variable selection and cross-validation. In this instance the rms approach is a bit more convinent and faster, with the one line call to validate(). set.seed(42) # rms ## stepwise selection and validation step_rms &lt;- validate(fit_lrm, method = &quot;crossvalidation&quot;, B = 10, bw = TRUE) The results for validate() above and below show which variables were retained and deleted. Below we can see a dot matrix of which variables were retained in during the 10 iterations of the cross validation. In addition, below we can see the difference between the training and test accuracy and error metrics. Remember that it is the test accuracy we should pay attention too. ## test accuracy and error step_rms The caret package option for variable selection and validation is a bit more verbose than the rms package. However, the caret package is a more versatile package, with options for over 50 different models, such as other tree-based models. # caret library(caret) ## cross validation parameters train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10, classProbs = TRUE, savePredictions = TRUE, summaryFunction = twoClassSummary ) # stepwise selection and validation step_caret &lt;- train(argillic ~ z + slp + twi_sc + ch_log + z2str_log + mrrtf + solar + precip + precipsum + temp + mast + tc_1 + tc_2 + tc_3 + k + th + u, data = train, method=&quot;glmStepAIC&quot;, family = &quot;binomial&quot;, direction =&quot;backward&quot;, trace = FALSE, trControl = train.control, na.action = na.exclude ) # test accuracy &amp; error step_caret$results # summary summary(step_glm &lt;- step_caret$finalModel) 5.7.3 Final model &amp; accuracy Because were dealing with a classification problem, we have to consider both errors of commission (Type I) and omission (Type II), or their corresponding accuracies of sensitivity (producers accuracy) and positive predicted value (users accuracy or precision) respectively. Before we can assess the error, however, we need to select a probability threshold. Sensitivity and specificity examine how well the ground truth or reference data compares to the predictions. Positive and negative predicted values (users accuracy) examine the inverse concept of how well the predictions match the reference data # examine possible thresholds train$predict &lt;- predict(step_glm, train, type = &quot;response&quot;) ggplot(train, aes(x = predict, fill = argillic)) + geom_density(alpha = 0.5) + geom_vline(aes(xintercept = 0.5), lty = &quot;dashed&quot;) + xlab(&quot;probability&quot;) + scale_x_continuous(breaks = seq(0, 1, 0.2)) train$predict &lt;- ifelse(train$predict &gt; 0.3, &#39;yes&#39;, &#39;no&#39;) # Confusion Matrix cm &lt;- table(observed = train$argillic, predicted = train$predict) confusionMatrix(cm, positive = &quot;yes&quot;) # Deviance squared library(modEvA) # Deviance squared Dsquared(step_glm) # Adjusted deviance squared Dsquared(step_glm, adjust = TRUE) # Spatially variable accuracy temp &lt;- train %&gt;% group_by(cluster) %&gt;% dplyr::summarize( TP = sum(predict == &quot;yes&quot; &amp; argillic == &quot;yes&quot;, na.rm = TRUE), FN = sum(predict == &quot;no&quot; &amp; argillic == &quot;yes&quot;, na.rm = TRUE), sensitivity = TP / (TP + FN) ) ggplot(temp, aes(x = cluster, y = sensitivity)) + geom_point() table(train$argillic, train$cluster) Discuss the variability of the predictions across the clusters, perhaps different models need to be constructed in each cluster, some clusters appear to be dominated by specific soil series, these data arent clean enough (nor are the series concepts usually) to model series separately, however, we could use the clusters as an additional model to attempt to separate the series. Do the hyperthermic clusters perform differently. 5.7.4 Model effects # summary summary(step_glm) # Convert the coefficients to an odds scale, who here gambles? exp(coef(step_glm)) # analysis of deviance anova(step_glm) # visreg library(visreg) par(mfrow = c(2, 2)) visreg(step_glm, scale = &quot;response&quot;, ylab = &quot;argillic probability&quot;) View the results in ArcGIS and examine the accuracy at individual points Discuss the effects of data quality, including both NASIS and GIS Discuss how the modeling process isnt an end in itself, but serves to uncover trends, possibly generate additional questions and direct future investigations 5.8 Generate spatial predictions # Custom function to return the predictions and their standard errors library(raster) predfun &lt;- function(model, data) { v &lt;- predict(model, data, type = &quot;response&quot;, se.fit = TRUE) cbind( p = as.vector(v$fit), se = as.vector(v$se.fit) ) } # Generate spatial predictions r &lt;- predict(geodata_r, step_glm, fun = predfun, index = 1:2, progress = &quot;text&quot;) # Export the results writeRaster(r[[1]], &quot;argi.tif&quot;, overwrite = T, progress = &quot;text&quot;) writeRaster(r[[2]], &quot;argi_se.tif&quot;, overwrite = T, progress = &quot;text&quot;) library(raster) # argillic probability plot(raster(&quot;C:/workspace2/argi.tif&quot;)) plot(ca794[1], col = NA, add = TRUE) # argillic standard error plot(raster(&quot;C:/workspace2/argi_se.tif&quot;)) plot(ca794[1], col = NA, add = TRUE) # Download clipped example from Pinto Basin Joshua Tree githubURL &lt;- &quot;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/logistic/argi_pb.zip&quot; download.file(githubURL, destfile = &quot;C:/workspace2/argi_pb.zip&quot;) unzip(zipfile=&quot;C:/workspace2/argi_pb.zip&quot;, exdir=&quot;C:/workspace2&quot;) 5.9 Exercise 2: View the prediction in ArcGIS Examine the raster predictions in ArcGIS and compare them to the Shapefile of that contains the original observations (hint classify the Shapefile symbology using the argillic column) Discuss with your group, and report your observations or hypotheses 5.10 Additional reading Lane, P.W., 2002. Generalized linear models in soil science. European Journal of Soil Science 53, 241- 251. http://onlinelibrary.wiley.com/doi/10.1046/j.1365-2389.2002.00440.x/abstract James, G., D. Witten, T. Hastie, and R. Tibshirani, 2014. An Introduction to Statistical Learning: with Applications in R. Springer, New York. http://www-bcf.usc.edu/~gareth/ISL/ Hengl, T. 2009. A Practical Guide to Geostatistical Mapping, 2nd Edt. University of Amsterdam, www.lulu.com, 291 p. ISBN 978-90-9024981-0. http://spatial-analyst.net/book/system/files/Hengl_2009_GEOSTATe2c0w.pdf 5.11 Literature Cited Beaudette, D. E., &amp; OGeen, A. T, 2009. Quantifying the aspect effect: an application of solar radiation modeling for soil survey. Soil Science Society of America Journal, 73:1345-1352 Gessler, P. E., Moore, I. D., McKenzie, N. J., &amp; Ryan, P. J, 1995. Soil-landscape modelling and spatial prediction of soil attributes. International Journal of Geographical Information Systems, 9:421-432 Gorsevski, P. V., Gessler, P. E., Foltz, R. B., &amp; Elliot, W. J, 2006. Spatial prediction of landslide hazard using logistic regression and ROC analysis. Transactions in GIS, 10:395-415 Evans, D.M. and Hartemink, A.E., 2014. Digital soil mapping of a red clay subsoil covered by loess. Geoderma, 230:296-304. Harrell, F.E., 2015. Regression Modeling Strategies: With Applications to Linear Models, Logisitc and Ordinal Regression, and Survival Analysis. Springer, New York. https://link.springer.com/book/10.1007%2F978-3-319-19425-7 Hosmer Jr, D.W., Lemeshow, S. and Sturdivant, R.X., 2013. Applied logistic regression (Vol. 398). John Wiley &amp; Sons Kempen, B., Brus, D. J., Heuvelink, G., &amp; Stoorvogel, J. J. (2009). Updating the 1: 50,000 Dutch soil map using legacy soil data: A multinomial logistic regression approach. Geoderma, 151:311-326. Nauman, T. W., and J. A. Thompson, 2014. Semi-automated disaggregation of conventional soil maps using knowledge driven data mining and classification trees. Geoderma 213:385-399. http://www.sciencedirect.com/science/article/pii/S0016706113003066 Peterson, F.F., 1981. Landforms of the basin and range province: defined for soil survey. Nevada Agricultural Experiment Station Technical Bulletin 28, University of Nevada - Reno, NV. 52 p. http://jornada.nmsu.edu/files/Peterson_LandformsBasinRangeProvince.pdf Shi, X., L. Girod, R. Long, R. DeKett, J. Philippe, and T. Burke, 2012. A comparison of LiDAR-based DEMs and USGS-sourced DEMs in terrain analysis for knowledge-based digital soil mapping. Geoderma 170:217-226. http://www.sciencedirect.com/science/article/pii/S0016706111003387 5.12 References (GLM) "],["tree-based-models.html", "Chapter 6 Tree-based Models 6.1 Introduction 6.2 Exploratory Data Analysis 6.3 Classification and Regression Trees (CART) 6.4 Random Forest 6.5 Prediction using Tree-based Models 6.6 Summary 6.7 Additional Reading 6.8 References (Tree-based Models)", " Chapter 6 Tree-based Models Statistics for pedologists course banner image 6.1 Introduction Tree-based models are a supervised machine learning method commonly used in soil survey and ecology for exploratory data analysis and prediction due to their simplistic nonparametric design. Instead of fitting a model to the data, tree-based models recursively partition the data into increasingly homogenous groups based on values that minimize a loss function (such as Sum of Squared Errors (SSE) for regression or Gini Index for classification) (McBratney et al.,2013). The two most common packages for generating tree-based models in R are rpart and randomForest. The rpart package creates a regression or classification tree based on binary splits that maximize homogeneity and minimize impurity. The output is a single decision tree that can be further pruned or trimmed back using the cross-validation error statistic to reduce over-fitting. The randomForest package is similar to rpart, but is double random in that each node is split using a random subset of predictors AND observations at each node and this process is repeated hundreds of times (as specified by the user). Unlike rpart, random forests do not produce a graphical decision tree since the predictions are averaged across hundreds or thousands of trees. Instead, random forests produce a variable importance plot and a tabular statistical summary. 6.2 Exploratory Data Analysis The data that we will be working with in this chapter were collected in support of a MLRA 127 soil survey update project to tabularly and spatially update SSRUGO map units for spodic properties in the Monongahela National Forest. Soils that were historically covered by Eastern Hemlock and Red Spruce exhibit spodic morphology on shale, siltstone, and sandstone bedrocks at elevations typically &gt;3,200 ft in West Virginia (Nauman et al., 2015). The landscape and vegetative communities were greatly altered by fire and logging in the early 1900s, complicating the identification of spodic morphology. It is of particular interest to the project leader and the U.S. Forest Service that spatial maps be developed to depict the location of spodic morphology and folistic epipedons in the Monongahela National Forest. Folistic epipedons provide habitat for the Northern Flying Squirrel, just recently removed from the endangered species list. 6.2.1 Getting Data Into R and Exporting to Shapefile Before we dive in to model building, lets first import and plot the dataset in R. library(lattice) #graphing library(sp) #spatial data library(maps) #maps library(rgdal) #spatial import library(corrplot) #graphical display of correlation matrix file &lt;-&#39;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/logistic/wv_transect_editedforR.csv&#39; download.file(file, destfile = &quot;soildata.csv&quot;) soildata &lt;- read.csv(&quot;soildata.csv&quot;, header=TRUE, sep=&quot;,&quot;) coordinates(soildata) &lt;- ~ x + y #set the coordinates; converting dataframe to a spatial object proj4string(soildata) &lt;- CRS(&quot;+init=EPSG:4269&quot;) #set the projection; https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf map(&quot;county&quot;, &quot;west virginia&quot;) points(soildata) #plot points #convert soildata into a shapefile writeOGR(soildata, dsn = &quot;C:/workspace&quot;, &quot;soildata&quot;, driver = &quot;ESRI Shapefile&quot;) Conveniently, environmental covariate values were previously extracted for all of the observations in the soildata dataset. How would you extract raster data to points in R? (Hint) 6.2.2 Examining Data in R #since we converted the soildata dataframe to a spatial object to export as a shapefile, we will need to convert it back to a dataframe to plot and further examine the data in R #re-importing the data and overwriting the soildata object is just one way to achieve this file &lt;-&#39;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/logistic/wv_transect_editedforR.csv&#39; download.file(file, destfile = &quot;soildata.csv&quot;) soildata &lt;- read.csv(&quot;soildata.csv&quot;, header=TRUE, sep=&quot;,&quot;) View(soildata) #view the data str(soildata) #examine the internal data structure ## &#39;data.frame&#39;: 250 obs. of 50 variables: ## $ x : num -79.7 -79.7 -79.7 -79.8 -79.7 ... ## $ y : num 38.6 38.6 38.6 38.6 38.6 ... ## $ Overtype : chr &quot;Hardwood&quot; &quot;Hardwood&quot; &quot;Hardwood&quot; &quot;Hardwood&quot; ... ## $ Underconifer: chr &quot;n&quot; &quot;y&quot; &quot;n&quot; &quot;y&quot; ... ## $ Oi : num 3 4 3 4 3 2 2 2 4 3 ... ## $ Oe : num 0 0 2 4 4 6 4 2 2 4 ... ## $ Oa : num 0 3 0 0 8 0 0 0 7 4 ... ## $ Otot : num 3 7 5 8 15 8 6 4 13 7 ... ## $ epipedon : chr &quot;ochric&quot; &quot;ochric&quot; &quot;ochric&quot; &quot;ochric&quot; ... ## $ spodint : num 0 0 0 0 1 1 1 0.5 1 0 ... ## $ subgroup : chr &quot;typic&quot; &quot;typic&quot; &quot;typic&quot; &quot;typic&quot; ... ## $ order : chr &quot;ultisol&quot; &quot;inceptisol&quot; &quot;ultisol&quot; &quot;inceptisol&quot; ... ## $ ps : chr &quot;fl&quot; &quot;ls&quot; &quot;fl&quot; &quot;cl&quot; ... ## $ drainage : chr &quot;wd&quot; &quot;wd&quot; &quot;wd&quot; &quot;wd&quot; ... ## $ series : chr &quot;Carrollton&quot; &quot;Mandy&quot; &quot;Carrollton&quot; &quot;Mandy&quot; ... ## $ taxon : chr &quot;series&quot; &quot;series&quot; &quot;series&quot; &quot;taxadjunct&quot; ... ## $ slope : int 45 54 39 25 38 30 38 36 16 36 ... ## $ surfacetex : chr &quot;l&quot; &quot;l&quot; &quot;cnl&quot; &quot;l&quot; ... ## $ stoniness : chr &quot;stx&quot; &quot;stv&quot; &quot;stx&quot; &quot;stv&quot; ... ## $ depthclass : chr &quot;md&quot; &quot;md&quot; &quot;md&quot; &quot;md&quot; ... ## $ bedrockdepth: int 200 200 200 200 200 200 200 200 200 200 ... ## $ depth_cm : int 67 69 52 95 93 81 86 90 75 92 ... ## $ hillslope : chr &quot;backslope&quot; &quot;backslope&quot; &quot;backslope&quot; &quot;backslope&quot; ... ## $ tipmound : int 0 0 0 0 0 0 0 0 0 0 ... ## $ rainfall : int 51 51 51 51 51 51 51 51 51 51 ... ## $ geology : chr &quot;Dch&quot; &quot;Dhs&quot; &quot;Dhs&quot; &quot;Dhs&quot; ... ## $ aachn : num 27.46 81.7 2.59 43.36 70.75 ... ## $ dem10m : num 1123 1239 1155 1240 1211 ... ## $ downslpgra : num 25.9 25.1 78 37 26.1 ... ## $ eastness : num -0.53 0.972 1 0.9 -0.474 ... ## $ greenrefl : num 0.1035 0.0425 0.0351 0.0776 0.0721 ... ## $ landsatb1 : num 0.0726 0.0742 0.0742 0.0775 0.0694 ... ## $ landsatb2 : num 0.062 0.0583 0.0547 0.0583 0.0528 ... ## $ landsatb3 : num 0.0335 0.0351 0.0301 0.0419 0.0318 ... ## $ landsatb7 : num 0.0494 0.0516 0.043 0.0451 0.0302 ... ## $ maxc100 : num -0.0845 -0.0963 -0.0848 0.2386 0.1368 ... ## $ maxent : num 20.26 30.21 27.44 7.88 73.85 ... ## $ minc100 : num -0.1925 -0.1457 -0.8324 0.069 0.0998 ... ## $ mirref : num 0.1608 0.0827 0.1109 0.1477 0.0762 ... ## $ ndvi : num 0.866 0.822 0.842 0.8 0.77 ... ## $ northeastn : num -0.974 0.854 0.728 0.944 -0.958 ... ## $ northness : num -0.848 0.237 0.03 0.435 -0.88 ... ## $ northwestn : num -0.225 -0.52 -0.686 -0.329 -0.287 ... ## $ planc100 : num -0.1177 -0.1077 -0.2173 0.1321 0.0476 ... ## $ proc100 : num -0.231 -0.178 -0.735 0.221 0.161 ... ## $ protection : num 0.237 0.154 0.222 0.114 0.138 ... ## $ relpos11 : num 0.352 0.507 0.233 0.544 0.585 ... ## $ slp50 : num 42.4 42.3 26.3 24.5 36.3 ... ## $ solar : int 1481510 1280610 1348160 1322630 1532160 1536340 1488440 1155820 1440500 1474730 ... ## $ tanc75 : num 0.0511 0.0279 0.0553 -0.0187 -0.0346 ... The example dataset, soildata, consists of 250 observations and 58 variables that were collected in the field or derived from geospatial data to identify spodic soil properties in West Virginia. Of particular interest is determining best splits for spodic intensity (relative spodicity index). As you can see in the data structure, R interpreted the spodint field as numeric. Since spodint is an index, it will need to be changed to a factor and then to an ordered factor. The same will need to be done for tipmound (a tip and mound microtopography index). set.seed(250) soildata$spodint &lt;- as.factor(soildata$spodint) soildata$spodint &lt;- ordered(soildata$spodint) soildata$tipmound &lt;- as.factor(soildata$tipmound) soildata$tipmound &lt;- ordered(soildata$tipmound) Next, lets explore the tabular data: boxplot(solar~spodint, data=soildata, xlab=&quot;spodic intensity&quot;, ylab=&quot;solar&quot;) #does solar radiation affect spodic intensity? boxplot(northwestn~spodint, data=soildata, xlab=&quot;spodic intensity&quot;, ylab=&quot;northwestness&quot;) #how about aspect? densityplot(~ Otot|order, data=soildata) #distribution of O horizon thickness among soil orders numeric &lt;- data.frame(soildata[, c(8, 25, 27:50)]) #combine numeric columns into a new data frame names(numeric) ## [1] &quot;Otot&quot; &quot;rainfall&quot; &quot;aachn&quot; &quot;dem10m&quot; &quot;downslpgra&quot; ## [6] &quot;eastness&quot; &quot;greenrefl&quot; &quot;landsatb1&quot; &quot;landsatb2&quot; &quot;landsatb3&quot; ## [11] &quot;landsatb7&quot; &quot;maxc100&quot; &quot;maxent&quot; &quot;minc100&quot; &quot;mirref&quot; ## [16] &quot;ndvi&quot; &quot;northeastn&quot; &quot;northness&quot; &quot;northwestn&quot; &quot;planc100&quot; ## [21] &quot;proc100&quot; &quot;protection&quot; &quot;relpos11&quot; &quot;slp50&quot; &quot;solar&quot; ## [26] &quot;tanc75&quot; cormatrix &lt;- cor(numeric) #calculate correlation matrix corrplot(cormatrix, method = &quot;circle&quot;) #plot correlation matrix 6.2.3 Exercise 1 Examine the soildata shapefile and environmental covariate data in ArcGIS. Come up with a theory of which possible covariates might be useful for predicting spodic morphology and folistic epipedons. Also, think of the different possible ways to model these features given the soildata dataset. 6.3 Classification and Regression Trees (CART) The basic form for all CART models is (y ~ x), where y is the dependent variable to be predicted from x, a set of independent variables. If the dependent variable (y) is numeric, the resulting tree will be a regression tree. Conversely, if the dependent variable (y) is categorical, the resulting tree will be a classification tree. The rpart package allows all data types to be used as independent variables, regardless of whether the model is a classification or regression tree. The rpart algorithm ignores missing values when determining the quality of a split and uses surrogate splits to determine if observation(s) with missing data is best split left or right. If an observation is missing all surrogate splits, then the observation(s) is sent to the child node with the largest relative frequency (Feelders, 1999). Assuming that the rpart and randomForest packages are already installed on your machine, simply load the packages using the library() function. library(rpart) #CART models library(randomForest) #random forest library(rpart.plot) #rpart plot graphics library(caret) #confusion matrix If you wanted to create a classification tree for spodint using all of the variables, you would simply type: rpart(spodint ~ ., data=soildata). Since our goal is to generate a spatial prediction model, we only want to use the variables for which we have spatial coverageour environmental covariate rasters. spodintmodel &lt;- rpart(spodint ~ rainfall + geology + aachn + dem10m + downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + landsatb3 +landsatb7 + maxc100 + maxent + minc100 + mirref + ndvi+ northeastn + northness + northwestn + planc100 + proc100 + protection + relpos11 + slp50 + solar + tanc75, data = soildata, method = &quot;class&quot;) spodintmodel ## n= 250 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 250 162 0 (0.35 0.084 0.26 0.024 0.28) ## 2) northwestn&lt; 0.55138 148 80 0 (0.46 0.081 0.29 0.027 0.14) ## 4) maxc100&lt; 0.10341 57 20 0 (0.65 0.088 0.14 0.018 0.11) * ## 5) maxc100&gt;=0.10341 91 56 1 (0.34 0.077 0.38 0.033 0.16) ## 10) solar&lt; 1504355 83 52 0 (0.37 0.084 0.33 0.036 0.18) ## 20) minc100&lt; -0.197089 9 1 0 (0.89 0 0 0 0.11) * ## 21) minc100&gt;=-0.197089 74 47 1 (0.31 0.095 0.36 0.041 0.19) ## 42) maxent&lt; 12.9024 23 10 0 (0.57 0.13 0.26 0.043 0) ## 84) solar&lt; 1460005 16 5 0 (0.69 0.19 0.12 0 0) * ## 85) solar&gt;=1460005 7 3 1 (0.29 0 0.57 0.14 0) * ## 43) maxent&gt;=12.9024 51 30 1 (0.2 0.078 0.41 0.039 0.27) ## 86) greenrefl&gt;=0.01846795 40 20 1 (0.22 0.075 0.5 0.025 0.17) ## 172) dem10m&gt;=1012.96 33 14 1 (0.27 0.061 0.58 0 0.091) ## 344) minc100&lt; 0.06651935 24 7 1 (0.21 0.083 0.71 0 0) * ## 345) minc100&gt;=0.06651935 9 5 0 (0.44 0 0.22 0 0.33) * ## 173) dem10m&lt; 1012.96 7 3 2 (0 0.14 0.14 0.14 0.57) * ## 87) greenrefl&lt; 0.01846795 11 4 2 (0.091 0.091 0.091 0.091 0.64) * ## 11) solar&gt;=1504355 8 0 1 (0 0 1 0 0) * ## 3) northwestn&gt;=0.55138 102 53 2 (0.2 0.088 0.22 0.02 0.48) ## 6) protection&lt; 0.0988242 25 15 1 (0.2 0.16 0.4 0.04 0.2) ## 12) proc100&lt; 0.1502945 14 10 0 (0.29 0.29 0.14 0 0.29) * ## 13) proc100&gt;=0.1502945 11 3 1 (0.091 0 0.73 0.091 0.091) * ## 7) protection&gt;=0.0988242 77 33 2 (0.19 0.065 0.16 0.013 0.57) ## 14) maxent&lt; 9.5806 30 19 0 (0.37 0.1 0.17 0.033 0.33) ## 28) rainfall&lt; 52 9 2 0 (0.78 0 0 0 0.22) * ## 29) rainfall&gt;=52 21 13 2 (0.19 0.14 0.24 0.048 0.38) ## 58) landsatb7&gt;=0.0398039 12 7 1 (0.17 0.25 0.42 0 0.17) * ## 59) landsatb7&lt; 0.0398039 9 3 2 (0.22 0 0 0.11 0.67) * ## 15) maxent&gt;=9.5806 47 13 2 (0.085 0.043 0.15 0 0.72) * plot(spodintmodel) text(spodintmodel, cex=0.8) #cex is text size #if you are having trouble viewing the text in the plot window, click zoom to open a bigger window #you may also need to adjust the plot margins or text size; for this example, try: par(mar=c(3,6,3,6)) plot(spodintmodel) text(spodintmodel, cex=0.6) For more plot customization, use the rpart.plot package. rpart.plot(spodintmodel, extra=3) #extra=3 displays the misclassification rate at the node, expressed as the number of incorrect classifications divided by the total observations in the node; there are many options under the extra setting for classification models rpart.plot(spodintmodel, extra=103) #adding 100 to the extra setting displays the percentage observations in the node prp(spodintmodel,type=1,extra=1,branch=1) #prp is another function in the rpart.plot package that has numerous plot customization options Notice that the terminal nodes display the different spodic intensity classes, ranging from 0 to 2. Can you think of another way that we could model spodic expression? Could we treat spodic intesity (an ordered factor) as numeric, ranging from 0 to 1 to develop a regression tree? Does this make sense? what would a mean of 1 tell you about the observations in the terminal node? What if we considered everything with a spodic intensity of &lt;= 0.5 to be non-spodic and everything &gt;0.5 to be spodic? A binary probability approach to predicting spodic morphology, similar to Nauman et al., 2015. index &lt;- c(0, 0.5, 1, 1.5, 2) #index for lookup table values &lt;- c(&quot;nonspodic&quot;, &quot;nonspodic&quot;, &quot;spodic&quot;, &quot;spodic&quot;, &quot;spodic&quot;) #assigning corresponding categories to look up values soildata$newcolumn &lt;- values[match(soildata$spodint, index)] #match spodint to index and assign values soildata$newcolumn &lt;- as.factor(soildata$newcolumn) #convert new column from character to factor spodintmodel2 &lt;- rpart(newcolumn ~ rainfall + geology + aachn + dem10m + downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + landsatb3 +landsatb7 + maxc100 + maxent + minc100 + mirref + ndvi+ northeastn + northness + northwestn + planc100 + proc100 + protection + relpos11 + slp50 + solar + tanc75, data = soildata, method = &quot;class&quot;) spodintmodel2 ## n= 250 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 250 109 spodic (0.4360000 0.5640000) ## 2) eastness&gt;=-0.1275925 100 36 nonspodic (0.6400000 0.3600000) ## 4) tanc75&gt;=-0.0268239 64 16 nonspodic (0.7500000 0.2500000) * ## 5) tanc75&lt; -0.0268239 36 16 spodic (0.4444444 0.5555556) ## 10) northeastn&gt;=0.7433605 13 3 nonspodic (0.7692308 0.2307692) * ## 11) northeastn&lt; 0.7433605 23 6 spodic (0.2608696 0.7391304) * ## 3) eastness&lt; -0.1275925 150 45 spodic (0.3000000 0.7000000) ## 6) minc100&lt; -0.1576455 47 23 spodic (0.4893617 0.5106383) ## 12) dem10m&gt;=1054.305 15 2 nonspodic (0.8666667 0.1333333) * ## 13) dem10m&lt; 1054.305 32 10 spodic (0.3125000 0.6875000) ## 26) protection&gt;=0.178847 13 5 nonspodic (0.6153846 0.3846154) * ## 27) protection&lt; 0.178847 19 2 spodic (0.1052632 0.8947368) * ## 7) minc100&gt;=-0.1576455 103 22 spodic (0.2135922 0.7864078) ## 14) landsatb7&gt;=0.0440802 37 15 spodic (0.4054054 0.5945946) ## 28) planc100&lt; -0.00902005 8 1 nonspodic (0.8750000 0.1250000) * ## 29) planc100&gt;=-0.00902005 29 8 spodic (0.2758621 0.7241379) * ## 15) landsatb7&lt; 0.0440802 66 7 spodic (0.1060606 0.8939394) * plot(spodintmodel2) text(spodintmodel2, cex=0.8) Notice that several of the splits changed. Which model performed better? One way to compare the two models is to use the function printcp(): printcp(spodintmodel) ## ## Classification tree: ## rpart(formula = spodint ~ rainfall + geology + aachn + dem10m + ## downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + ## landsatb3 + landsatb7 + maxc100 + maxent + minc100 + mirref + ## ndvi + northeastn + northness + northwestn + planc100 + proc100 + ## protection + relpos11 + slp50 + solar + tanc75, data = soildata, ## method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] dem10m greenrefl landsatb7 maxc100 maxent minc100 ## [7] northwestn proc100 protection rainfall solar ## ## Root node error: 162/250 = 0.648 ## ## n= 250 ## ## CP nsplit rel error xerror xstd ## 1 0.179012 0 1.00000 1.00000 0.046614 ## 2 0.030864 1 0.82099 0.90123 0.048107 ## 3 0.029321 2 0.79012 0.91358 0.047967 ## 4 0.018519 7 0.63580 0.95679 0.047374 ## 5 0.015432 8 0.61728 0.94444 0.047561 ## 6 0.012346 11 0.56790 0.94444 0.047561 ## 7 0.010000 14 0.53086 0.95679 0.047374 printcp(spodintmodel2) ## ## Classification tree: ## rpart(formula = newcolumn ~ rainfall + geology + aachn + dem10m + ## downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + ## landsatb3 + landsatb7 + maxc100 + maxent + minc100 + mirref + ## ndvi + northeastn + northness + northwestn + planc100 + proc100 + ## protection + relpos11 + slp50 + solar + tanc75, data = soildata, ## method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] dem10m eastness landsatb7 minc100 northeastn planc100 protection ## [8] tanc75 ## ## Root node error: 109/250 = 0.436 ## ## n= 250 ## ## CP nsplit rel error xerror xstd ## 1 0.256881 0 1.00000 1.00000 0.071933 ## 2 0.050459 1 0.74312 0.91743 0.071064 ## 3 0.027523 5 0.54128 0.98165 0.071773 ## 4 0.010000 8 0.45872 0.90826 0.070943 The printcp() funtion generates a cost complexity parameter table that provides the complexity parameter value (CP), relative model error (1 - relative error = ~variance explained), error estimated from a 10-fold cross validation (xerror), and the standard error of the xerror (xstd). The CP values control the size of the tree; the greater the CP value, the fewer the number of splits in the tree. To determine the optimal CP value, rpart automatically performs a 10-fold cross validation. The optimal size of the tree is generally the row in the CP table that minimizes all error with the fewest branches. Another way to determine the optimal tree size is to use the plotcp() function. This will plot the xerror versus cp value and tree size. plotcp(spodintmodel) plotcp(spodintmodel2) The optimal CP value is 0.029321 for spodintmodel and 0.050459 for spodintmodel2. Since both spodic intensity models overfit that data, they will need to be pruned using the prune() function. pruned &lt;- prune(spodintmodel, cp=0.029321) printcp(pruned) ## ## Classification tree: ## rpart(formula = spodint ~ rainfall + geology + aachn + dem10m + ## downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + ## landsatb3 + landsatb7 + maxc100 + maxent + minc100 + mirref + ## ndvi + northeastn + northness + northwestn + planc100 + proc100 + ## protection + relpos11 + slp50 + solar + tanc75, data = soildata, ## method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] northwestn protection ## ## Root node error: 162/250 = 0.648 ## ## n= 250 ## ## CP nsplit rel error xerror xstd ## 1 0.179012 0 1.00000 1.00000 0.046614 ## 2 0.030864 1 0.82099 0.90123 0.048107 ## 3 0.029321 2 0.79012 0.91358 0.047967 rpart.plot(pruned, extra=3) pruned2 &lt;- prune(spodintmodel2, cp=0.050459) printcp(pruned2) ## ## Classification tree: ## rpart(formula = newcolumn ~ rainfall + geology + aachn + dem10m + ## downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + ## landsatb3 + landsatb7 + maxc100 + maxent + minc100 + mirref + ## ndvi + northeastn + northness + northwestn + planc100 + proc100 + ## protection + relpos11 + slp50 + solar + tanc75, data = soildata, ## method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] eastness ## ## Root node error: 109/250 = 0.436 ## ## n= 250 ## ## CP nsplit rel error xerror xstd ## 1 0.256881 0 1.00000 1.00000 0.071933 ## 2 0.050459 1 0.74312 0.91743 0.071064 rpart.plot(pruned2, extra=3) The misclassification rate (in cross-validation) for the spodintmodel was 57% (root node error * xerror * 100) which dropped to 38% in the spodintmodel2. Why did the performance of these models differ significantly? Lets compute an internal validation using a confusion matrix to further examine differences in these models. In order to do this, we will need to split our data into a training and test set. ## splits 70% of the data selected randomly into training set and the remaining 30% sample into test set datasplit &lt;- sort(sample(nrow(soildata), nrow(soildata)*.7)) train &lt;- soildata[datasplit,] test &lt;- soildata[-datasplit,] spodintmodel &lt;- rpart(spodint ~ rainfall + geology + aachn + dem10m + downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + landsatb3 +landsatb7 + maxc100 + maxent + minc100 + mirref + ndvi+ northeastn + northness + northwestn + planc100 + proc100 + protection + relpos11 + slp50 + solar + tanc75, data = train, method = &quot;class&quot;) printcp(spodintmodel) ## ## Classification tree: ## rpart(formula = spodint ~ rainfall + geology + aachn + dem10m + ## downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + ## landsatb3 + landsatb7 + maxc100 + maxent + minc100 + mirref + ## ndvi + northeastn + northness + northwestn + planc100 + proc100 + ## protection + relpos11 + slp50 + solar + tanc75, data = train, ## method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] dem10m downslpgra maxc100 maxent northwestn planc100 proc100 ## [8] solar ## ## Root node error: 118/175 = 0.67429 ## ## n= 175 ## ## CP nsplit rel error xerror xstd ## 1 0.186441 0 1.00000 1.00000 0.052538 ## 2 0.059322 1 0.81356 0.87288 0.055168 ## 3 0.050847 2 0.75424 0.96610 0.053422 ## 4 0.038136 3 0.70339 0.91525 0.054494 ## 5 0.025424 5 0.62712 0.85593 0.055383 ## 6 0.021186 6 0.60169 0.84746 0.055479 ## 7 0.010000 9 0.53390 0.84746 0.055479 pruned &lt;- prune(spodintmodel, cp=0.070175) pred &lt;- predict(pruned, newdata=test, type=&quot;class&quot;) #predicting class test data using the pruned model confusionMatrix(pred, test$spodint) #computes confusion matrix and summary statistics ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 0.5 1 1.5 2 ## 0 22 2 10 1 7 ## 0.5 0 0 0 0 0 ## 1 0 0 0 0 0 ## 1.5 0 0 0 0 0 ## 2 9 5 6 0 13 ## ## Overall Statistics ## ## Accuracy : 0.4667 ## 95% CI : (0.3505, 0.5855) ## No Information Rate : 0.4133 ## P-Value [Acc &gt; NIR] : 0.2053 ## ## Kappa : 0.181 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 0 Class: 0.5 Class: 1 Class: 1.5 Class: 2 ## Sensitivity 0.7097 0.00000 0.0000 0.00000 0.6500 ## Specificity 0.5455 1.00000 1.0000 1.00000 0.6364 ## Pos Pred Value 0.5238 NaN NaN NaN 0.3939 ## Neg Pred Value 0.7273 0.90667 0.7867 0.98667 0.8333 ## Prevalence 0.4133 0.09333 0.2133 0.01333 0.2667 ## Detection Rate 0.2933 0.00000 0.0000 0.00000 0.1733 ## Detection Prevalence 0.5600 0.00000 0.0000 0.00000 0.4400 ## Balanced Accuracy 0.6276 0.50000 0.5000 0.50000 0.6432 #sensitivity=producer&#39;s accuracy and specificity=user&#39;s accuracy spodintmodel2 &lt;- rpart(newcolumn ~ rainfall + geology + aachn + dem10m + downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + landsatb3 +landsatb7 + maxc100 + maxent + minc100 + mirref + ndvi+ northeastn + northness + northwestn + planc100 + proc100 + protection + relpos11 + slp50 + solar + tanc75, data = train, method = &quot;class&quot;) printcp(spodintmodel2) ## ## Classification tree: ## rpart(formula = newcolumn ~ rainfall + geology + aachn + dem10m + ## downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + ## landsatb3 + landsatb7 + maxc100 + maxent + minc100 + mirref + ## ndvi + northeastn + northness + northwestn + planc100 + proc100 + ## protection + relpos11 + slp50 + solar + tanc75, data = train, ## method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] downslpgra eastness landsatb2 maxent northeastn rainfall slp50 ## ## Root node error: 71/175 = 0.40571 ## ## n= 175 ## ## CP nsplit rel error xerror xstd ## 1 0.225352 0 1.00000 1.00000 0.091489 ## 2 0.070423 1 0.77465 0.91549 0.090028 ## 3 0.056338 2 0.70423 1.02817 0.091872 ## 4 0.042254 3 0.64789 1.02817 0.091872 ## 5 0.028169 4 0.60563 1.02817 0.091872 ## 6 0.014085 6 0.54930 1.08451 0.092487 ## 7 0.010000 7 0.53521 1.08451 0.092487 pruned2 &lt;- prune(spodintmodel2, cp=0.050459) pred2 &lt;- predict(pruned2, newdata=test, type=&quot;class&quot;) #predicting class of test data using the pruned model confusionMatrix(pred2, test$newcolumn) #computes confusion matrix and summary statistics ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonspodic spodic ## nonspodic 22 16 ## spodic 16 21 ## ## Accuracy : 0.5733 ## 95% CI : (0.4538, 0.6869) ## No Information Rate : 0.5067 ## P-Value [Acc &gt; NIR] : 0.1493 ## ## Kappa : 0.1465 ## ## Mcnemar&#39;s Test P-Value : 1.0000 ## ## Sensitivity : 0.5789 ## Specificity : 0.5676 ## Pos Pred Value : 0.5789 ## Neg Pred Value : 0.5676 ## Prevalence : 0.5067 ## Detection Rate : 0.2933 ## Detection Prevalence : 0.5067 ## Balanced Accuracy : 0.5733 ## ## &#39;Positive&#39; Class : nonspodic ## The accuracy of the spodintmodel using split sample internal validation was 39% (61% misclassification error). The model incorrectly classified all spodic intensity ratings of 0.5 and 1.5. The accuracy of the spodintmodel2 was 63% (37% misclassification error). The model was able to predict spodic better than nonspodic. Notice that both of the misclassification errors increased slightly using the split sample validation versus the deafult internal 10-fold cross-validation used in by the rpart package. It is not uncommon to see slight differences in overall model performance between validations. In this case, it confirms that the first model is relatively 40% accurate and the second model is relatively 63% accurate. As a side note: The default 10-fold internal cross-validation in rpart divides the data into 10 subsets, using 9 sets as learning samples to create trees, and 1 set as test samples to calculate error rates. This process is repeated for all possible combinations of learning and test samples (a total of 10 times), and error rates are averaged to estimate the error rate for the full data set. 6.3.1 Exercise 2: rpart The examples above dealt with classification trees which resulted in categorical terminal nodes determined by majority votes. In a regression tree model, terminal nodes reflect the mean of the observations in that node. Using the soildata dataset, construct a rpart regression tree model to predict total O horizon thickness. Prune the model if necessary and answer the following questions: ** 1) Was the majority of the variance in total O horizon thickness captured with the rpart model?** ** 2) What were the most important variables in the model?** ** 3) How could the model be improved?** 6.4 Random Forest The randomForest algorithm fits hundreds to thousands of CART models to random subsets of input data and combines the trees for prediction. Similarly to rpart, randomForest allows all data types to be used as independent variables, regardless of whether the model is a classification or regression tree. Unlike rpart, the randomForest algorithm does not straight forwardly handle missing values with surrogate splits. There is a function called rfImpute() that uses a proximity matrix from the randomForest to populate missing values with either the weighted average of the non-missing observations (weighted by the proximities) for continuous predictors or the category with the largest average proximity for categorical predictors. Going back to the soildata dataset, lets generate a random forest regression model for total O horizon thickness and compare it to the one we just generated in rpart. Just like rpart, randomForest has the same basic model function: (y ~ x). rf &lt;- randomForest(Otot ~ rainfall + geology + aachn + dem10m + downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + landsatb3 +landsatb7 + maxc100 + maxent + minc100 + mirref + ndvi+ northeastn + northness + northwestn + planc100 + proc100 + protection + relpos11 + slp50 + solar + tanc75, data = soildata, importance=TRUE, ntree=1000, mtry=10) #importance=TRUE will allow the generation of a variable importance plot rf #statistical summary ## ## Call: ## randomForest(formula = Otot ~ rainfall + geology + aachn + dem10m + downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + landsatb3 + landsatb7 + maxc100 + maxent + minc100 + mirref + ndvi + northeastn + northness + northwestn + planc100 + proc100 + protection + relpos11 + slp50 + solar + tanc75, data = soildata, importance = TRUE, ntree = 1000, mtry = 10) ## Type of random forest: regression ## Number of trees: 1000 ## No. of variables tried at each split: 10 ## ## Mean of squared residuals: 28.09843 ## % Var explained: 21.86 plot(rf) #out of bag (OOB) error rate versus number of trees; this will help us tune the ntree parameter The rf model, generated using the default number of trees and number of variables tried at each split, explained approximately 23% of the variance and produced a mean square error (sum of squared residuals divided by n) of 28 cm2. If you were to run this same model again, the % variance explained and MSE would change slightly due to the random subsetting and averaging in the randomForest algorithm. How does this compare with the rpart model? Recall that the soildata dataset had one Histosol observation: hist(soildata$Otot) Lets remove that observation to see how it impacted our model. file &lt;- &#39;https://raw.githubusercontent.com/ncss-tech/stats_for_soil_survey/master/data/logistic/wv_transect_editedforR.csv&#39; download.file(file, destfile = &quot;soildata.csv&quot;) soildata &lt;- read.csv(&quot;soildata.csv&quot;, header=TRUE, sep=&quot;,&quot;) soildata2 &lt;- droplevels(subset(soildata, order!=&quot;histosol&quot;)) #remove Histosol observation rf2 &lt;- randomForest(Otot ~ rainfall + geology + aachn + dem10m + downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + landsatb3 +landsatb7 + maxc100 + maxent + minc100 + mirref + ndvi+ northeastn + northness + northwestn + planc100 + proc100 + protection + relpos11 + slp50 + solar + tanc75, data = soildata2, importance=TRUE, ntree=1000, mtry=9) #importance=TRUE will allow the generation of a variable importance plot rf2 # statistical summary ## ## Call: ## randomForest(formula = Otot ~ rainfall + geology + aachn + dem10m + downslpgra + eastness + greenrefl + landsatb1 + landsatb2 + landsatb3 + landsatb7 + maxc100 + maxent + minc100 + mirref + ndvi + northeastn + northness + northwestn + planc100 + proc100 + protection + relpos11 + slp50 + solar + tanc75, data = soildata2, importance = TRUE, ntree = 1000, mtry = 9) ## Type of random forest: regression ## Number of trees: 1000 ## No. of variables tried at each split: 9 ## ## Mean of squared residuals: 24.24666 ## % Var explained: 22.73 Did removing the outlier Histosol observation improve the model? The defaults for the number of trees (ntree) and number of variables tried at each split (mtry) may need to be adjusted in the randomForest command to explain more variance in the data and to reduce model over-fitting. For most datasets, manually tweaking these parameters and examining the statistical summary is often sufficient. The tuneRF() function can be used to determine the optimal mtry value, but some users have claimed that this algorithm leads to bias. Feel free to manually tweak the ntree and mtry settings to see how they effect the overall model performance. Another way to assess the rf model is to look at the variable importance plot. varImpPlot(rf2) imp &lt;- as.data.frame(sort(importance(rf2)[,1],decreasing = TRUE),optional = T) names(imp) &lt;- &quot;% Inc MSE&quot; imp # sorted tabular summary ## % Inc MSE ## landsatb7 22.2881522 ## maxent 18.0479569 ## protection 10.7843422 ## northwestn 8.2679676 ## solar 7.6946079 ## rainfall 7.2362535 ## downslpgra 6.0682438 ## slp50 5.5789709 ## minc100 5.2974000 ## aachn 5.0169593 ## dem10m 4.6986309 ## eastness 4.5805129 ## maxc100 4.1549669 ## relpos11 4.1451131 ## northeastn 4.1421563 ## northness 4.1022384 ## planc100 4.0799308 ## proc100 3.8260886 ## greenrefl 3.3736472 ## geology 2.7520274 ## landsatb2 2.6020824 ## landsatb3 1.6680639 ## landsatb1 1.6473007 ## tanc75 0.9004840 ## ndvi 0.8694725 ## mirref 0.4696301 For each tree, each predictor in the OOB sample is randomly permuted and passed through the tree to obtain an error rate (mean square error (MSE) for regression and Gini index for classification). The error rate from the unpermuted OOB is then subtracted from the error rate of the permuted OOB data, and averaged across all trees. When this value is large, it implies that a variable is highly correlated to the dependent variable and is needed in the model. In a regression tree analysis, randomForest uses %IncMSE and IncNodePurity to rank variable importance. %IncMSE is simply the average increase in squared residuals of the test set when variables are randomly permuted (little importance = little change in model when variable is removed or added) and IncNodePurity is the increase in homogeneity in the data partitions. In a classification tree analysis, randomForest uses MeanDecreaseAccuracy and MeanDecreaseGini. For MeanDecreaseAccuracy, the more the accuracy of the model decreases due to the addition of a single variable, the more important the variable is deemed. MeanDecreaseGini is a measure of how each variable contributes to the homogeneity of the nodes and leaves. In the rf2 model, it is apparent that landsatb7 is the most important variable used in the model, followed by maxent, protection index, northwestness, solar, and rainfall. 6.4.1 Exercise 3: randomForest Using the soildata dataset, construct a randomForest model to predict the probability of a folistic epipedon. Be sure to tweak the ntree and mtry parameters to capture the most variability. Use the following code to combine ochric and umbric into a new category called nonfolistic. soildata$epipedon2 &lt;- soildata$epipedon levels(soildata$epipedon2)[levels(soildata$epipedon2)==&quot;ochric&quot;] &lt;- &quot;nonfolistic&quot; levels(soildata$epipedon2)[levels(soildata$epipedon2)==&quot;umbric&quot;] &lt;- &quot;nonfolistic&quot; ** 1) What is the out-of-bag error rate?** ** 2) Compare this model with the total O horizon thickness regression model. Which would be better for spatial interpolation?** ** 3) How could you improve this model?** 6.5 Prediction using Tree-based Models As with any modeling technique, tree-based models can be used for prediction and can be spatially interpolated using environmental covariates. In order to interpolate a model, R requires that all raster images have a common datum, common cell resolution, are coregistered, and are preferably .img files. The function stack() combines all of the rasters into a raster stack. The predict() function is then used in the form of: predict(rasterstack, fittedmodel, type=\"\"). Follow along through the example below to interpolate the rpart total O horizon thickness model: library(raster) rasters &lt;- stack(list.files(getwd(),pattern=&quot;img$&quot;,full.names=FALSE)) #combines rasters with a .img file extension stored in the working directory rasters model &lt;- randomForest(Otot~landsatb7+maxent+protection+northwestn+solar, data=soildata2) predict(rasters,model,progress=&quot;window&quot;,overwrite=TRUE,filename=&quot;rfpredict.img&quot;) #type not specified=vector of predicted values, &quot;response&quot; for predicted class, &quot;prob&quot; for probabilities, or &quot;vote&quot; for matrix of vote counts (one column for each class and one row for each new input); either in raw counts or in fractions (if norm.votes=TRUE) #options for predicting rpart model: type= &quot;vector&quot; for mean response at the node, &quot;prob&quot; for matrix of class probabilities, or &quot;class&quot; for a factor of classifications based on the responses The output raster rfpredict.img can be added and viewed in ArcMap. R GUI image You can also view the interpolated model in R: rfpredict &lt;- raster(&quot;rfpredict.img&quot;) plot(rfpredict, xlab=&quot;Easting (m)&quot;, ylab=&quot;Northing (m)&quot;, main=&quot;Total O Horizon Thickness (cm)&quot;) 6.6 Summary Tree-based models are intuitive, quick to run, nonparametric, and are often ideal for exploratory data analysis and prediction. Both rpart and randomForest produce graphical and tabular outputs to aid interpretation. Both packages also perform internal validataion (rpart=10-fold cross validation; randomForest=OOB error estimates) to assess model performance. Tree-based models do require pruning and/or tweaking of model parameters to reduce over-fitting and are unstable in that removing observations (especially outliers) or independent predictors can greatly alter the tree structure. In general, tree-based models are robust against multicollinearity and low n, high p datasets (low sample size and many predictors). 6.7 Additional Reading Gareth, J., D. Witten, T. Hastie, and R. Tibshirani, 2014. An Introduction to Statistical Learning: with Applications in R. Springer, New York. http://www-bcf.usc.edu/~gareth/ISL/ Rad, M.R.P., N. Toomanian, F. Khormali, C.W. Brungard, C.B. Komaki, and P. Bogaert. 2014. Updating soil survey maps using random forest and conditioned Latin hypercube sampling in the loess derived soils of northern Iran. Geoderma. 232-234: 97-106. 6.8 References (Tree-based Models) ## Attaching package: &#39;igraph&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union ## Loading required package: lattice ## Warning: package &#39;wesanderson&#39; was built under R version 4.0.5 ## This is aqp 1.30 ## ## Attaching package: &#39;aqp&#39; ## The following objects are masked from &#39;package:plyr&#39;: ## ## mutate, summarize ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## Warning: package &#39;kableExtra&#39; was built under R version 4.0.4 "],["acc-unc-categorical-data.html", "A Accuracy and Uncertainty for Categorical Predictions A.1 Status Quo A.2 Theses A.3 Soap Box Time A.4 Concept Demonstration via Simulated Data A.5 Accuracy A.6 Uncertainty A.7 Review A.8 Example Implementation A.9 Resources A.10 References (Class Accuracy / Uncertainty)", " A Accuracy and Uncertainty for Categorical Predictions This document is an abbreviated attempt at addressing some of the issues related to accuracy and uncertainty that I have brought up over discussion of raster (soil class) mapping standards. As such, the following is a combination of soap box moments, demonstrations of methods, todo items for my future self, and references. Honestly, before going any further be sure to read the recent paper by Rossiter, Zeng, and Zhang (2017). This article does a fine job of summarizing the major differences between classification and prediction. Most of the modeling frameworks we will be using or evaluating generate predictions in the form of probabilities (prediction). At some point the stack of probabilities will be converted into a single map depicting the most likely class at each pixel (classification). The iterative assessment of model performance (e.g. accuracy and uncertainty) should happen as part of the prediction phase via metrics such as the Brier score and Shannon entropy. An evaluation of the final classification is helpful for communicating accuracy to a wider audience (e.g. percent correctly classified) but should not be the primary means of optimizing model performance. A.1 Status Quo The Digital Soil Mapping chapter (5) from the latest Soil Survey Manual describes two commonly used metrics for the description of accuracy and uncertainty: overall accuracy / percent correctly classified (PCC) and the confusion index (CI) of Burrough, van Gaans, and Hootsmans (1997). These methods are widely used and implementation is simple. Given the complex nature of class mapping results (e.g. stack of class probabilities) and inherent (likely quantifiable) similarity of soil classes, I think that we should explore options for a more robust suite of accuracy and uncertainty metrics. Furthermore, it is my opinion that any evaluation of probabilistic predictions should be integrated over all classes. A.2 Theses The \\(\\tau\\) statistic of (Rossiter, Zeng, and Zhang 2017) is a more reliable and nuanced representation of accuracy vs. PCC. The \\(\\tau\\) statistic can be upgraded with additional knowledge given the availability of 1) prior understanding of class proportions, and/or, 2) meaningful parameterization of pair-wise class distances. There can be consensus on formulation of approximate pair-wise distances, within a given modeling domain. Pair-wise distances may not necessarily be the same across modeling domains or projects. Brier scores are option for an even more nuanced representation of accuracy as they integrate all predicted probabilities. The confusion index of Burrough, van Gaans, and Hootsmans (1997) is an unstable metric when the number of predicted classes is large and when the most likely classes are associated with low probabilities. Shannon entropy (log base 2) is a more reliable representation of uncertainty than the confusion index, especially when the number of possible classes varies by project. The importance of a universally reliable representation of uncertainty is even more important when several methods are used concurrently. There should be a way to integrate pair-wise distances into the Shannon entropy (or related method) and Brier scores; maybe we will discover those here. A.3 Soap Box Time Our current QC/QA process is based on many forms of evaluation, accumulates some degree of subjectivity and relies heavily on qualitative forms of information (field experience, institutional knowledge, etc.). On the opposite side of the spectrum, the validation of raster mapping is often claimed to be free of subjective interference and entirely quantitative. Those are good things that we should always strive for, however, the simplicity of calculating a percent correctly classified can interfere with a more nuanced evaluation of accuracy. As I mentioned on the phone (and implicitly volunteered for) a validation score might be more meaningful than any single validation metrics. One such score might include: agreement between predicted probabilities and observed class (e.g. Brier scores) agreement between the most likely class and observed class, accounting for class similarities (e.g. weighted \\(\\tau\\)) distribution of class-wise Shannon entropy values calibration vs. predicted vs. validation proportion of classes some kind of metric that integrates spatial connectivity of predictions / observations, for example: cross-tabulate calibration / prediction / validation classes with geomorphon classes I strongly believe that we need a robust suite of metrics primarily for internal discussion and evaluation of raster mapping products; even more so when complex modeling frameworks such as randomForest or neural nets are used. Accuracy and uncertainty metrics are primarily vehicles for understanding, re-calibrating (as needed), and communicating statistical models as part of the development and QA/QC process. A.4 Concept Demonstration via Simulated Data Consider a supervised classification that generates predictions for 5 possible soil classes. Suites of predicted probabilities fall into 3 general cases: Case 1: classes D and E are nearly tied for the most likely class, but their respective probabilities are generally &lt; 0.5 Case 2: class E is almost always the most likely class, but classes B, C, and D are tied for second place Case 3: class E is always the most likely class, all other classes have probabilities &lt; 0.2 # examples of three cases print(p.1) Figure A.1: Probability distributions of predictions. Even though these are simulated data, the three cases above demonstrate common modeling scenarios where classification uncertainty ranges from very low (Case 3) in some areas to quite high (Case 1) in others. These three cases could easily be associated with real situations: Case 1: predictions for soil classes represent a hillslope complex that isnt quite disentangled by the model Case 2: predictions for soil classes represent limited success in partitioning between a single water shedding (E) vs. multiple water collecting positions (A-D) Case 3: predictions for soil classes represent a successful partitioning between Holocene age deposits (E) vs. older alluvial terraces (A-D) A.5 Accuracy A.5.1 Confusion Matrix / Area Under ROC See Chapter 9. Review some of the commentary on the use of only the confusion matrix and AUROC for rating predictions in the medical field. The confusion matrix and associated statistics are a useful starting point but not the most effective means for comparing performance. A.5.2 Brier Scores Brier scores (Brier 1950, @Harrell2001) quantify agreement between observed classes and predicted probabilities: \\[ B = \\frac{1}{n} \\sum_{i=1}^{n}{ ( p_{i} - y_{i} )^{2} } \\] where \\(B\\) is an index of agreement between predicted probabilities, \\(\\mathbf{p}\\), and class labels, \\(\\mathbf{y}\\). Larger values suggest less agreement between probabilities and observed class labels. Follow-up: https://en.wikipedia.org/wiki/Brier_score https://stats.stackexchange.com/questions/112250/understanding-the-rank-probability-score http://empslocal.ex.ac.uk/people/staff/dbs202/publications/2008/stephenson-brier.pdf http://iopscience.iop.org/article/10.1088/1748-9326/7/4/044019 What about a weighted version of this score, based on a re-statement of the distance matrix? A.5.3 Tau and Weighted Tau (class-similarity) (Rossiter, Zeng, and Zhang 2017) implemented in aqp::tauw(). This paper contains some discussion on a weighted version of Shannon Entropy using the subset of similarities between predicted classes and the actual class. A.5.3.1 Commentary from DGR Prior class probabilities. Commentary from DGR: + That depends on the mapping method. In LDA we can set the priors, then wed use these in tau. But for an automatic DSM procedure the priors are all equal (Foodys modified kappa). If judging a manual mapper, the priors can be their overall probabilities for an area. E.g., in one county we have a pretty good idea that it is half Vertisols, so the mapper is prejudiced (in the good sense) about this. Class similarity The weighting is quite tricky since obviously it can be used to manipulate results. I really like the error loss method if there is some numerical value put on each difference  as I did with the NC site index. In CA you have the Storie index, you could use that difference for mis-mappings of series. Numerical taxonomy measures could also be used but youd need to agree on which properties to use. If the purpose of the map is e.g. to estimate C stocks, then the difference between the mean C stocks between classes from NASIS might be used. Coming up with a transparent and accepted weighting can be tricky. A.6 Uncertainty A.6.1 Shanon Entropy \\[ H = -\\sum_{i=1}^{n}{p_{i} * log_{2}(p_{i})} \\] where \\(H\\) is an index of uncertainty associated with predicted probabilities, \\(\\mathbf{p}\\), of encountering classes \\(i\\) through \\(n\\). Smaller values imply less entropy (more information). Given equal class probabilities, H will increas as the number of classes increases. Kempen et al. (2009) described a normalized version of Shannon entropy that is constrained to values between 0 and 1: \\[ H = -\\sum_{i=1}^{n}{p_{i} * log_{n}(p_{i})} \\] where \\(H\\) is an index of uncertainty associated with predicted probabilities, \\(\\mathbf{p}\\), of encountering classes \\(i\\) through \\(n\\). This representation may be conveniently contained within the range of \\([0,1]\\), however, it cannot be used to compare uncertainty from models using different numbers of classes. It is my recommendation that the \\(log_{2}\\) version of Shannon H be used as our primary metric of uncertainty for predictive soil class mapping. Shannon entropy does not take into account similarity among classes. A.7 Review # examples of three cases print(p.1) pp &lt;- ldply(s, performance) names(pp)[1] &lt;- &#39;example&#39; kable_styling(kable(pp, row.names = FALSE, digits = 2, format=&#39;html&#39;), full_width = FALSE) example brier.score tau.equal tau.actual PCC Case 1 0.73 0.20 0.14 0.36 Case 2 0.71 0.28 0.23 0.43 Case 3 0.33 0.76 0.43 0.80 ex &lt;- ldply(s, extractExample, n=1) names(ex)[1] &lt;- &#39;example&#39; ex$CI &lt;- NULL ex$actual &lt;- NULL add_header_above(kable_styling(kable(ex, row.names = FALSE, digits = 2, format=&#39;html&#39;), full_width = FALSE), header=c(&quot; &quot; = 1, &quot;Class Probabilities&quot; = 5, &quot;Uncertainty&quot; = 1)) Class Probabilities Uncertainty example A B C D E Shannon.H Case 1 0.10 0.19 0.12 0.25 0.33 2.20 Case 2 0.03 0.19 0.19 0.28 0.32 2.09 Case 3 0.03 0.04 0.06 0.04 0.83 0.98 A.8 Example Implementation The aqp package has an implementation of Shannon entropy and Brier score; there are many other implementations but these are convenient for soil survey work. Consider the following table of predicted probabilities (classes A,B,C,D,E) and observed class (actual). library(aqp) # example data d &lt;- structure(list(A = c(0.0897243494322252, 0.0537087411977284, 0.0643087579284512, 0.0582791533521884, 0.0655491726966812, 0.0878056947034425, 0.0550727743006022, 0.10724015754623, 0.0332599961787985, 0.0555131608754956 ), B = c(0.191110141078936, 0.187244044389649, 0.119214057525671, 0.198461646003737, 0.161851348940294, 0.172157251906694, 0.113611770097243, 0.178697159594029, 0.194607795787689, 0.188977055949146), C = c(0.121941735763077, 0.0770539012535731, 0.0977753159795662, 0.0774293724263895, 0.072198187957068, 0.0366921003115242, 0.151033286139089, 0.0974443429098862, 0.124876574685048, 0.0864142563046045), D = c(0.351108807309283, 0.322120077305279, 0.440632731639948, 0.401063395801608, 0.312647702445919, 0.304193047630158, 0.270239142407351, 0.258895264130713, 0.422747316475851, 0.252724366285052 ), E = c(0.246114966416479, 0.359873235853771, 0.278069136926363, 0.264766432416077, 0.387753587960038, 0.399151905448182, 0.410043027055715, 0.357723075819142, 0.224508316872614, 0.416371160585702), id = c(&quot;1&quot;, &quot;10&quot;, &quot;100&quot;, &quot;1000&quot;, &quot;101&quot;, &quot;102&quot;, &quot;103&quot;, &quot;104&quot;, &quot;105&quot;, &quot;106&quot; ), actual = c(&quot;D&quot;, &quot;B&quot;, &quot;D&quot;, &quot;E&quot;, &quot;D&quot;, &quot;D&quot;, &quot;E&quot;, &quot;E&quot;, &quot;D&quot;, &quot;E&quot; )), .Names = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;id&quot;, &quot;actual&quot;), row.names = c(NA, 10L), class = &quot;data.frame&quot;) # check it out # predictions, and actual, observed class head(d) ## A B C D E id actual ## 1 0.08972435 0.1911101 0.12194174 0.3511088 0.2461150 1 D ## 2 0.05370874 0.1872440 0.07705390 0.3221201 0.3598732 10 B ## 3 0.06430876 0.1192141 0.09777532 0.4406327 0.2780691 100 D ## 4 0.05827915 0.1984616 0.07742937 0.4010634 0.2647664 1000 E ## 5 0.06554917 0.1618513 0.07219819 0.3126477 0.3877536 101 D ## 6 0.08780569 0.1721573 0.03669210 0.3041930 0.3991519 102 D Brier scores (accuracy) are computed over all predictions and associated observed classes. # compute Brier score from all predictions brierScore(d, classLabels = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;), actual = &#39;actual&#39;) ## [1] 0.5833992 Shannon entropy (uncertainty) is computed from each vector of predicted probabilities. # shannon entropy for first row, could be a single pixel or obs. point shannonEntropy(d[1, c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;)]) ## [1] 2.166525 # compute shannon entropy for all rows apply(d[, c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;)], 1, shannonEntropy) ## 1 2 3 4 5 6 7 8 ## 2.166525 2.021157 1.982791 2.024063 2.011094 1.971243 2.036219 2.151995 ## 9 10 ## 2.006615 2.018874 A.9 Resources A.9.1 Evaluating Accuracy of Categorical / Probabilistic Predictions Commentary on the use of confusion matrix and AUROC What is the AUC / AUROC? Commentary on C-index / AUROC AUROC Scoring Rules Frank Harrels Website + links Classification vs. Prediction A.9.2 Sampling and Stability of Estimates Gridded Data: Sample vs. Population Demonstration of sampling density vs. stability of median A.10 References (Class Accuracy / Uncertainty) References "],["commentary-selecting-a-modeling-framework.html", "B Commentary: Selecting a Modeling Framework B.1 Soil Temperature Regime Modeling in CA792 B.2 References (Modeling Frameworks)", " B Commentary: Selecting a Modeling Framework The choice of modeling framework has a significant impact on the quantity of data required, flexibility to account for non-linearities and interactions, potential for over-fitting, model performance, and degree to which the final model can be interpreted. This document and related commentary provides a nice background on the interplay between model generality, interpretation, and performance. TODO: package up these data to be used as an example. B.1 Soil Temperature Regime Modeling in CA792 The following figure goes along with a bit of conversation I had (below) with some of my colleagues, on the topic of modeling soil temperature regime using a variety of frameworks. The strong bioclimatic gradient with MLRAs 17, 18, 22A, 22B made is possible to use a modeling framework that generated reasonable predictions and resulted in an interpretable model. Essentially, each framework (MLR, regression trees, randomForest, etc.) is useful for different tasks but thinking about the most appropriate framework ahead of time is time well spent. Predictions arent the same as science or understanding. Sometimes we need the former, sometimes we need the latter and sometimes we need both. The x-axis is elevation, in most cases the dominant driver of soil temperature and soil temperature regime in this area. The y-axis is conditional probability of several STR. The top panel represents the smooth surface fit by multinomial logistic regression. These smooth surfaces lend to testable interpretations such as the transition between STR per 1,000 of elevation gain follows XXX. This is far more useful when the model includes other factors such as annual beam radiance and the effect of cold air drainages. Absolute accuracy is sacrificed for a general (e.g. continuous over predictors) representation of the system that can support inference. Another example, at elevation XXX, what is the average effect of moving from a south-facing slope to a north-facing slope?. The second panel down represents the hard thresholds generated by an algorithm from the tree-based classification framework (e.g. recursive partitioning trees via rpart). Accuracy is about the same as the MLR approach and the hard breaks can be interpreted as reasonable cut points or thresholds that may have links to physical processes. Note that the cut points identified by this framework are very close to the 50% probability cross-over points in the MLR panel. The result is a (potentially) pragmatic partitioning of reality that can support decisions but not inference (e.g. rate of change in Pr(STR) vs. 1000 elevation gain). The third panel down represents the nearly-exact (over?) fitting of STR probabilities generated by the random forest framework. This approach builds thousands of classification trees and (roughly) averages them together. The results are incredible (unbelievable?) within-training-set accuracy (99% here) at the expense of an interpretable model. That isnt always a problem: sometimes predictions are all that we have time for. That said, this framework requires a 100x larger training sample (vs. MLR) and an independent validation data set before it can be trusted on new data. B.2 References (Modeling Frameworks) "]]
